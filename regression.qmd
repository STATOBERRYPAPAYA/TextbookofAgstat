# Regression analysis {#sec-regression}

Regression analysis is one of the most important tools in statistics, used to understand and quantify the relationships between variables. In essence, regression helps us answer questions such as, "How does a change in one factor (like fertilizer usage) affect another factor (like crop yield)?" It provides a mathematical framework to explore these relationships based on observed data.

Regression analysis involves two types of variables:

-   **Dependent variable** (*y*): This is the main outcome or the variable you are trying to predict or explain. For example, crop yield might be the dependent variable in an agricultural study. We usually denote dependent variable as *y.* The dependent variable is also known by various names such as the target variable, response variable, outcome variable, predicted variable, explained variable, **regressand**, *y*-variable, criterion variable, or output variable.

-   **Independent variables** (*x*)*:* These are the factors that are thought to influence or predict changes in the dependent variable, such as the amount of water, fertilizer, or sunlight received by plants. The independent variable is also known as the predictor variable, explanatory variable, input variable, **regressor**, feature, covariate, *x*-variable, or control variable.

**Why Use Regression Analysis?**

Regression analysis is particularly useful because it allows you to:

1.  **Quantify relationships**: It measures how strongly one or more independent variables are associated with the dependent variable.

2.  **Predict outcomes**: Once the relationship is understood, regression can be used to predict the dependent variable for new values of the independent variables.

3.  **Identify key factors**: It can highlight which variables have the most significant impact on the dependent variable, guiding decision-making.

4.  **Control for multiple factors:** By including several independent variables, regression helps isolate the effect of each variable while controlling for others.

**Types of regression**

There are different types of regression techniques, depending on the nature of the data and the relationship between variables:

-   **Simple linear regression**: Examines the relationship between one independent variable and one dependent variable, assuming a straight-line relationship.

-   **Multiple linear regression**: It is an extension of simple linear regression, allowing for the analysis of the relationship between one dependent variable (*y*) and multiple independent variables (more than one *x*). It is used when the outcome (dependent variable) is influenced by more than one factor (independent variables).

-   **Nonlinear regression:** Deals with situations where the relationship between variables is not a straight line.

-   **Logistic regression:** Used when the dependent variable is categorical, such as predicting whether a plant will survive (yes or no) based on environmental factors.

**Practical applications**

Regression analysis has a wide range of applications across fields:

-   In **agriculture**, it can be used to study the effect of irrigation, soil nutrients, and weather on crop yield.

-   In **economics**, it helps analyze the impact of income, education, and employment on consumer behavior.

-   In **medicine**, regression can predict health outcomes based on patient characteristics.

By the end of this chapter, you will learn how to perform regression analysis, interpret its results, and understand its assumptions and limitations. This will enable you to use regression as a powerful tool for making informed decisions and predictions.

Correlation versus regression

| **Item** | **Correlation** | **Regression** |
|----|----|----|
| **Definition** | Measures the strength and direction of the relationship between two variables. | Models the relationship between a dependent variable and one or more independent variables. |
| **Objective** | To quantify the degree of association between variables. | To predict the value of the dependent variable based on the independent variable(s). |
| **Causation** | Does not imply causation; it only measures association. Causation means changes in one variable cause changes in another. | Can imply causation if assumptions are met and the model is well-specified. |
| **Equation** | No equation is derived. | Derives an equation: $y=\beta_0 + \beta_1x + \epsilon$. |
| **Variables involved** | Considers two variables at a time. | Can involve one or multiple independent variables to predict a dependent variable. |
| **Symmetry** | Correlation between (x) and (*y*) is the same as (*y*) and (*x*). | Regression is directional: it models (*y*) as dependent on (*x*), not vice versa. |
| **Range** | The correlation coefficient (*r*) ranges from -1 to 1. | Regression coefficients ($\beta_0, \beta_1$) ranges from $-\infty$ to $+\infty$. |
| **Units** | Unit less measure. | Dependent on the units of the variables involved. |
| **Purpose** | To understand the strength of the relationship. | To predict outcomes or explain variability in the dependent variable. |

## Simple linear regression

Regression can be simply defined as a technique of fitting best line or line of best fit to estimate value of one variable on the basis of another variable. Now what is a best line? or line of best fit?. To understand this concept better, consider the data presented in @tbl-corrdata, which shows the average daily soil moisture content and the corresponding monetary yield from crops in Example 8.2 of @sec-scatterdiag. This example helps visualize how the relationship between two variablesâ€”soil moisture content (independent variable) and crop yield (dependent variable)

We can use regression analysis to answer the following questions

What will be the crop yield in rupees when soil moisture content is maintained at 20%?

What is the functional form of relationship between soil moisture content and monetary crop yield?

```{r r1, echo=FALSE,fig.cap='Scatter diagram of data in Example 8.1',out.width="50%", fig.align='center'}
knitr::include_graphics(rep("images/r2.jpeg"))
```

We can draw a line to denote the functional relationship between temperature and sales

```{r r2, echo=FALSE,fig.cap='lines drawn to show functional relationship',out.width="70%", fig.align='center'}
knitr::include_graphics(rep("images/r99.png"))
```

You can see that as shown above we can draw any number of lines, so which is the best fit line?

We can say the **best fit line** is the line that passes through all points such that distance of each point to the line is minimum. Using regression technique we could easily draw such a line. Before proceeding you should know the concept of error and residuals.

## Error and residual

An error is the difference between the observed value and the true value (true value is the unobserved population mean of the population from which sample observations are taken). A residual is the difference between the observed value and the predicted value (by the model **fitted line**). Error cannot be measured but residual can be; so residual is considered as an estimate of error.

```{r r3, echo=FALSE,fig.cap='Error depicted in best fit line ',out.width="70%", fig.align='center'}
knitr::include_graphics(rep("images/r91.png"))
```

The distance of each observation (*e*~i~) from the fitted line can be considered as the residual (error). Best fit line can be obtained by minimizing this distance. This can be achieved using the mathematical technique "**principle of least squares**".

## Straight lines

A straight line is the simplest figure in geometry.

Mathematical equation of a straight line Y= ***a*** + ***b***X.

Two important features of a line **slope** and **intercept**. ***a*** is the Y-intercept, the intercept of a line is the y-value of the point where it crosses the y-axis. ***b*** is the **slope of a line,** which is a number that measures its "steepness". It is the change in Y for a unit change in X along the line. In regression ***b*** is called as regression coefficient.

Intercept (***a***)

```{r r4, echo=FALSE,fig.cap='Intercept of a line ',out.width="50%", fig.align='center'}
knitr::include_graphics(rep("images/r92.png"))
```

Slope (***b***)

```{r r5, echo=FALSE,fig.cap='Slope of a line ',out.width="50%", fig.align='center'}
knitr::include_graphics(rep("images/r93.png"))
```

***a*** and ***b*** can be considered as a finger print of a line; with these values we can easily identify the line.

So now our problem is simple, to find a line of best, estimate ***a*** & ***b***, such that error *e*~i~ of each observation is minimized. For that we use the **method of least squares**.

## Method of least squares

On considering the error term *e*~i~; equation of a straight line is

*y*~i~=*a*+*bx*~i~+*e~i~*;

Where *e~i~* is the *i*^th^ error term corresponding to *y*~i~, *i* =1,2,...,*n*

Line of best fit can be obtained by estimating ***a*** and ***b*** by minimizing error sum 'Î£*e~i~'*. By theorem Î£*e~i~ =*0; so ***a*** and ***b*** are estimated by minimising Î£*e~i~* ^2^

## Principle of least squares

The statistical method used to determine a line of best fit by minimizing theÂ sum of squares of the error term Î£*e~i~* ^2^

*y~i~*=*a*+*bx*~i~+*e~i~*;

*e~i~ =* *y*~i~ -- (*a*+*bx*~i~)

*e~i~*^2^ = {*y*~i~ -- (*a*+*bx*~i~)}^2^

**Î£***e~i~*^2^ = **Î£** {*y*~i~ -- (*a*+*bx*~i~)}^2^

**Î£***e~i~*^2^ is called as error sum of squares. As we are minimizing error sum of squares, hence the name principle of least squares.

We want to minimize, **E = Î£***e*~i~^2^ = **Î£** {*y*~i~ -- (*a*+*bx*~i~)}^2^

*i*.*e*. we need to find ***a*** and ***b*** such that **E** is minimum

**E** can be minimized by taking derivative with respect to ***a*** and ***b*** and equating to zero. On doing so we will get two equations, these equations are termed as **normal equations** and solving those normal equations will give the formula for ***a*** and ***b***.

We are not discussing calculation part here. After taking derivatives we will get two equations (Normal equations) as below:

$$\sum_{i = 1}^{n}{y_{i} = n\mathbf{a} + \mathbf{b}\sum_{i = 1}^{n}x_{i}}$$

$$\sum_{i = 1}^{n}{y_{i}x_{i} = \mathbf{a}\sum_{i = 1}^{n}x_{i} + \mathbf{b}\sum_{i = 1}^{n}x_{i}^{2}}$$

On solving the above equations we will get

Regression coefficient, $$b=\frac{\sum_{i = 1}^{n}{y_{i}x_{i} - \frac{\sum_{i = 1}^{n}{y_{i}\sum_{i = 1}^{n}x_{i}}}{n}}}{\sum_{i = 1}^{n}x_{i}^{2} - \frac{\left( \sum_{i = 1}^{n}x_{i} \right)^{2}}{n}}$$

$$=\frac{cov(x,y)}{var(x)}$$

$$\mathbf{b =}\frac{\mathbf{cov(x,y)}}{\mathbf{var(x)}}
$$

$$\mathbf{a = }\overline{\mathbf{y}}\mathbf{- b}\overline{\mathbf{x}}$$

where $\overline{y}$ = mean of *y*; $\overline{x}$ = mean of *x*

## Two lines of regression

There are two lines of regression- that of *y* on *x* and *x* on *y*.

[Regression of *y* on *x*]{.underline}

Consider the two variables *x* and *y*, if you are considering *y* as dependent variable and *x* as independent variable then your equation is:

***y*** **= *a* + *bx***

This is used to predict the unknown value of variable *y* when value of variable *x* is known. Usually ***b*** here is denoted as ***b***~yx~

$$\mathbf{b}_{\mathbf{\text{yx}}}\mathbf{=}\frac{\mathbf{cov(x,y)}}{\mathbf{var(x)}}$$

Consider Example 8.1; considering ice cream sales as dependent variable and temperature as independent variable

```{r r6, echo=FALSE,fig.cap='Scatter diagram of data in Example 8.1',out.width="50%", fig.align='center'}
knitr::include_graphics(rep("images/r7.jpeg"))
```

[Regression of *x* on *y*]{.underline}

Consider the two variables *x* and *y*, if you are considering *x* as dependent variable and *y* as independent variable then your equation is:

***x*****= *c* + *my;*** where ***c*** is the intercept and ***m*** is the slope

This is used to predict the unknown value of variable *x* when value of variable *y* is known. Usually ***b*** here is denoted as ***b***~xy~

$$\mathbf{b}_{\mathbf{\text{xy}}}\mathbf{=}\frac{\mathbf{cov(x,y)}}{\mathbf{var(y)}}$$

Consider Example 8.1; considering temperature as dependent variable and ice cream sales as independent variable

```{r r7, echo=FALSE,fig.cap='Scatter diagram of data in Example 8.1',out.width="50%", fig.align='center'}
knitr::include_graphics(rep("images/r8.jpeg"))
```

You can see both the regression were different. It depends on the experimenter to choose dependent and independent variable. In the above example it is evident that considering temperature as dependent variable is meaningless, *i*.*e*. what is the usefulness in predicting temperature based on ice cream sales?. So the selection of dependent and independent variable is entirely the discretion of experimenter based on the objective of his study.

## Assumptions of Regression

If *y* is the dependent variable and *x* is the independent variable then

1.  The *x*'s are non-random or fixed constants.

2.  At each fixed value of *x* the corresponding values of *y* have a normal distribution about a mean.

3.  For any given *x*, the variance of *y* is same.

4.  The values of *y* observed at different levels of *x* are completely independent.

## Properties of Regression coefficients

1.  The correlation coefficient between *x* and *y* is the geometric mean of the two regression coefficients ***b***~yx~ and ***b***~xy~

$$r = \sqrt{b_{\text{yx}}b_{\text{xy}}}$$

2.  Regression coefficients are independent of change of origin but not of scale.

3.  If one regression coefficient is greater than unity, then the other must be less than unity but not vice versa. *i*.*e*. both the regression coefficients can be less than unity but both cannot be greater than unity, *i*.*e*. if ***b***~yx~ \>1 then ***b***~xy~ \<1 and if ***b***~xy~ \>1, then ***b***~yx~ \<1.

4.  Also if one regression coefficient is positive the other must be positive (in this case the correlation coefficient is positive) and if one regression coefficient is negative the other must be negative (in this case the correlation coefficient is negative).

5.  The range of regression coefficients is $- \infty$ to $+ \infty$.

6.  If the variables ( X ) and ( Y ) are independent, the regression coefficients are zero. This is referred to as the independence property of regression coefficients.

## Uses of Regression

**Prediction**: The regression analysis is useful in predicting the value of one variable from the given value of another variable. Such predictions are useful when it is very difficult or expensive to measure the dependent variable, Y.

**Identify the strength of relationship**: The regression might be used to identify the strength of the effect that the independent variable(s) have on a dependent variable. Like the strength of relationship between dose and effect, sales and marketing spending, or age and income.

**Forecast effects or impact of changes**: That is, the regression analysis helps us to understand how much the dependent variable changes with a change in one or more independent variables. A typical question is, "how much additional sales income do I get for each additional 1000 spent on marketing.

**Predicts trends and future values**: The regression analysis can be used to predict trend and future values, like "what will be the price of gold in 6 months?"

## Properties of Regression lines

1.  Regression lines minimize the sum of squared deviations of observed values from the predicted values, ensuring the best possible fit.

2.  The regression lines intersect at the mean values of ð‘‹andð‘Œ *i.e.,* at ($\overline{X}$,$\overline{Y}$)

3.  The slopes of the regression lines are related to the correlation coefficient r. If r=0, the lines are perpendicular, indicating no linear relationship.

## Example problem

Now consider the example 8.1 and answer the questions

| Temperature (Â°C) | Ice Cream Sales (in \$) |
|:-----------------|:------------------------|
| 14.2             | 215                     |
| 16.4             | 325                     |
| 11.9             | 185                     |
| 15.2             | 332                     |
| 18.5             | 406                     |
| 22.1             | 522                     |
| 19.4             | 412                     |
| 25.1             | 614                     |
| 23.4             | 544                     |
| 18.1             | 421                     |
| 22.6             | 445                     |
| 17.2             | 408                     |

1.  What is the functional form of relationship between Temperature and Ice cream sales?

2.  What will be the Ice cream sales when temperature is 20^o^ Celsius?

**Solution**

1.  Fit a model considering Ice cream sales as dependent variable (*y*) and temperature as independent variable (*x*). Fitting a model means estimating ***b*** and ***a*** using equation.

2.  After fitting the model put 20 in the *x* value you will get the predicted *y* value

Model: *y* = ***a***+***b**x*

```{r ex85, echo=FALSE,warning=FALSE,results='asis'}
library(knitr)
library(kableExtra)
dt<-read.csv("csv/Book12.csv")
colnames(dt) <- c("Sl No.", "Temperature (x)", "Sales (y)", "$(x_{i}-\\overline{x})$","$(y_{i}-\\overline{y})$ ", "$(x_{i}-\\overline{x})(y_{i}-\\overline{y})$", "$(x_{i}-\\overline{x})^2$")
kable(dt,escape = FALSE) %>% kable_classic(full_width = F, html_font = "Cambria")%>%
kable_styling(position="center",bootstrap_options = c("striped"))%>%
  row_spec(0, bold = TRUE, align = "c")
```

*n* =12

$$mean,\overline{x} = \ \frac{224.1}{12} = 18.675$$

$$mean,\overline{y} = \ \frac{4829}{12} = 402.416$$

*Cov* (*x*,*y*) = $$\frac{1}{n}\sum_{i = 1}^{n}{\left( x_{i} - \overline{x} \right)\left( y_{i} - \overline{y} \right)}$$

$$\sum_{i = 1}^{12}{\left( x_{i} - \overline{x} \right)\left( y_{i} - \overline{y} \right)} = 5325.03$$

$$Cov (x,y) = \frac{5325.03}{12} = 443.752$$

$$variance\ of\ x,\ var\left( x \right) = \ \frac{1}{n}\sum_{i = 1}^{n}\left( x_{i} - \overline{x} \right)^{2} = \frac{176.983}{12} = 14.7485$$

$$\mathbf{b =}\frac{\mathbf{cov(x,y)}}{\mathbf{var(x)}}$$

$$\mathbf{b =}\frac{443.752}{14.7485}\mathbf{=}30.088$$

$$\mathbf{a =}\overline{\mathbf{y}}\mathbf{- b}\overline{\mathbf{x}}$$

$$\mathbf{a =}402.416 - 30.088\left( 18.675 \right) = \  - 159.477$$

So our model is

$$y = \  - 159.477 + 30.088x$$

$$Ice\ cream\ sales = \  - 159.477 + 30.088(Temperature)$$

Ice cream sales when temperature is 20^o^ Celsius

$$x = 20$$

$y = \  - 159.477 + 30.088(20)$ = 442.283

So the predicted ice cream sales at 20^o^ Celsius is **442.283**

. Â \
Â 

## Partial and multiple correlation {#sec-partialmulti}

## 

::: {#hello8 .greeting .message style="color: #c9c6c5;"}
<center>**"Statistics is the art of never having to say you're wrong":-Robert P. Abelson**</center>
:::
