# Regression analysis {#sec-regression}

Regression analysis is one of the most important tools in statistics, used to understand and quantify the relationships between variables. In essence, regression helps us answer questions such as, "How does a change in one factor (like fertilizer usage) affect another factor (like crop yield)?" It provides a mathematical framework to explore these relationships based on observed data.

Regression analysis involves two types of variables:

-   **Dependent variable** (*y*): This is the main outcome or the variable you are trying to predict or explain. For example, crop yield might be the dependent variable in an agricultural study. We usually denote dependent variable as *y.* The dependent variable is also known by various names such as the target variable, response variable, outcome variable, predicted variable, explained variable, **regressand**, *y*-variable, criterion variable, or output variable.

-   **Independent variables** (*x*)*:* These are the factors that are thought to influence or predict changes in the dependent variable, such as the amount of water, fertilizer, or sunlight received by plants. The independent variable is also known as the predictor variable, explanatory variable, input variable, **regressor**, feature, covariate, *x*-variable, or control variable.

**Why Use Regression Analysis?**

Regression analysis is particularly useful because it allows you to:

1.  **Quantify relationships**: It measures how strongly one or more independent variables are associated with the dependent variable.

2.  **Predict outcomes**: Once the relationship is understood, regression can be used to predict the dependent variable for new values of the independent variables.

3.  **Identify key factors**: It can highlight which variables have the most significant impact on the dependent variable, guiding decision-making.

4.  **Control for multiple factors:** By including several independent variables, regression helps isolate the effect of each variable while controlling for others.

**Types of regression**

There are different types of regression techniques, depending on the nature of the data and the relationship between variables:

-   **Simple linear regression**: Examines the relationship between one independent variable and one dependent variable, assuming a straight-line relationship.

-   **Multiple linear regression**: It is an extension of simple linear regression, allowing for the analysis of the relationship between one dependent variable (*y*) and multiple independent variables (more than one *x*). It is used when the outcome (dependent variable) is influenced by more than one factor (independent variables).

-   **Nonlinear regression:** Deals with situations where the relationship between variables is not a straight line.

-   **Logistic regression:** Used when the dependent variable is categorical, such as predicting whether a plant will survive (yes or no) based on environmental factors.

**Practical applications**

Regression analysis has a wide range of applications across fields:

-   In **agriculture**, it can be used to study the effect of irrigation, soil nutrients, and weather on crop yield.

-   In **economics**, it helps analyze the impact of income, education, and employment on consumer behavior.

-   In **medicine**, regression can predict health outcomes based on patient characteristics.

By the end of this chapter, you will learn how to perform regression analysis, interpret its results, and understand its assumptions and limitations. This will enable you to use regression as a powerful tool for making informed decisions and predictions.

Correlation versus regression

| **Item** | **Correlation** | **Regression** |
|------------------------|------------------------|------------------------|
| **Definition** | Measures the strength and direction of the relationship between two variables. | Models the relationship between a dependent variable and one or more independent variables. |
| **Objective** | To quantify the degree of association between variables. | To predict the value of the dependent variable based on the independent variable(s). |
| **Causation** | Does not imply causation; it only measures association. Causation means changes in one variable cause changes in another. | Can imply causation if assumptions are met and the model is well-specified. |
| **Equation** | No equation is derived. | Derives an equation: $y=\beta_0 + \beta_1x + \epsilon$. |
| **Variables involved** | Considers two variables at a time. | Can involve one or multiple independent variables to predict a dependent variable. |
| **Symmetry** | Correlation between (x) and (*y*) is the same as (*y*) and (*x*). | Regression is directional: it models (*y*) as dependent on (*x*), not vice versa. |
| **Range** | The correlation coefficient (*r*) ranges from -1 to 1. | Regression coefficients ($\beta_0, \beta_1$) ranges from $-\infty$ to $+\infty$. |
| **Units** | Unit less measure. | Dependent on the units of the variables involved. |
| **Purpose** | To understand the strength of the relationship. | To predict outcomes or explain variability in the dependent variable. |

## Simple linear regression

Regression can be simply defined as a technique of fitting best line or line of best fit to estimate value of one variable on the basis of another variable. Now what is a best line? or line of best fit?. To understand this concept better, consider the data presented in @tbl-corrdata, which shows the average daily soil moisture content and the corresponding monetary yield from crops in Example 8.2 of @sec-scatterdiag. This example helps visualize how the relationship between two variables—soil moisture content (independent variable) and crop yield (dependent variable).\
We can use regression analysis to answer the following questions. What will be the crop yield in rupees when soil moisture content is maintained at 20%?. What is the functional form of relationship between soil moisture content and monetary crop yield?.

Refer to the scatter diagram of @tbl-corrdata in @fig-scattercorrdata. To represent the relationship between soil moisture content and monetary crop yield, we might attempt to draw a line, as illustrated in @fig-funcrel. However, as shown in @fig-funcrel, it’s possible to draw numerous lines through the data points. This raises the question: *which line is the best fit*?  

::: {#fig-funcrel layout-ncol="2"}
![](images/r99.1.png){#fig-funcrel1 width="200px" height="150px"}  
![](images/r99.2.png){#fig-funcrel2 width="200px" height="150px"}  
![](images/r99.3.png){#fig-funcrel3 width="200px" height="150px"}  

lines drawn to show functional relationship between soil moisture and yield
:::  


The best fit line is defined as the one that minimizes the distances between the observed data points and the line itself. These distances, which represent how far each data point is from the line, are minimized collectively using a specific criterion. The regression technique provides a systematic approach to determine and draw this best fit line. Before diving further into regression, it is essential to understand the concepts of error and residuals, which play a critical role in determining the best fit line. The entire topic of regression is on *how to draw a best fit line*?.

### Error and residual

In regression analysis, an error represents the difference between an observed value (a data point) and the true regression line, which reflects the actual relationship between the dependent and independent variables in the population. Since the true regression line is based on the entire population and is usually unknown, the error is a theoretical concept that cannot be directly calculated.

A residual is the difference between an observed value and the value predicted by a regression line based on sample data. Specifically, for a given data point, the residual is calculated as the observed value minus the predicted value from the regression line. Residuals are measurable because they are derived from the observed data and the regression line that is obtained using the sample. Unlike errors, which are theoretical and represent deviations from the true underlying model, residuals provide a practical estimate of these deviations, allowing us to assess the goodness of fit and identify any patterns or discrepancies in the model.

In essence, a residual serves as an estimate of the error. While errors represent the deviation from the actual true value, residuals reflect the discrepancy between observed data and the fitted model. From @fig-residual you can see the residual $e_i$ of an *i*^th observation in a fitted regression line.

![Residual and a best fit line](images/r91.png){#fig-residual fig-align="center" width="70%" style="text-align:center;"}

The distance of *i*^th observation ($e_i$) from the fitted line can be considered as the residual (error). Best fit line can be obtained by minimizing this distance. This can be achieved using the mathematical technique "**principle of least squares**" discussed in @sec-leastsquares. Before going to identify a best fit line on should know the concept of a straight line.

### Straight lines

A straight line is the simplest figure in geometry. Mathematical equation of a straight line is\
$$Y = \alpha + \beta X$$ {#eq-straightline}

Two important features of a line **slope** ($\alpha$) and **intercept**($\beta$). $\alpha$ is the Y-intercept, the intercept of a line is the y-value of the point where it crosses the y-axis. $\beta$ is the **slope of a line,** which is a number that measures its "steepness". It is the change in Y for a unit change in X along the line. In regression $\beta$ is called as regression coefficient.

Intercept and slope {.underline}  
::: {#fig-interceptandslope layout-ncol="2"}
![Intercept of a straight line](images/r92.png){#fig-intercept width="200px" height="150px"}  
![Slope of a straight line](images/r93.png){#fig-intercept width="250px" height="150px"}  

Intercept and slope
:::  

$\alpha$ and $\beta$ can be considered as a finger print of a line; with these values we can easily identify the line. So now our problem is simple, to find a line of best, estimate $\alpha$ and $\beta$, such that error *e*~i~ of each observation is minimized. For that we use the *method of least squares*.

### Method of least squares {#sec-leastsquares}

On considering the error term $e_i$; equation of a straight line is

$$y_i = \alpha +\beta x_i + e_i$$ {#eq-simpleregression}

Where $e_i$ is the *i*^th^ error term corresponding to $y_i$, *i* =1,2,...,*n*

Line of best fit can be obtained by estimating $\alpha$ and $\beta$ by minimizing error sum $\sum_{i = 1}^{n}{e_i}$. By theorem $\sum_{i = 1}^{n}{e_i} = 0$; so $\alpha$ and $\beta$ are estimated by minimising $\sum_{i = 1}^{n}{e_i}^2$. The term $\sum_{i = 1}^{n}e_i^2$ is called as error sum of squares. As we are minmizing the sum of the squares of error term the process is known by the name `principle of least squares`.  

**Principle of least squares** {.underline}  

`Principle of least squares` is the statistical method used to determine a line of best fit by minimizing the sum of squares of the error term *i*.*e* minimizing $\sum_{i = 1}^{n}{e_i}^2$.  

Consider @eq-simpleregression

$$y_i = \alpha +\beta x_i + e_i$$  

$$e_i = y_i -(\alpha +\beta x_i)$$ {#eq-leastsq1} 

$$e_i^2 = [y_i -(\alpha +\beta x_i)]^2$$ {#eq-leastsq2} 

$$\sum_{i = 1}^{n}e_i^2 = \sum_{i = 1}^{n}[y_i -(\alpha +\beta x_i)]^2$$ {#eq-leastsq3}  

we want to minimize @eq-leastsq3 and estimate $\alpha$ and $\beta$. $\sum_{i = 1}^{n}e_i^2$ can be minimized by taking derivative with respect to $\alpha$  and $\beta$ and equating to zero. On doing so we will get two equations, these equations are termed as *normal equations* and solving those normal equations will give the formulas for estimating $\alpha$  and $\beta$.  

Differentiating $E=\sum_{i = 1}^{n}e_i^2$ with respect to $\alpha$ and wquating to 0.

$$\frac{\partial E}{\partial \alpha} = \sum_{i=1}^n 2 \left[ y_i - (\alpha + \beta x_i) \right](-1)$$

$$\frac{\partial E}{\partial \alpha} = -2 \sum_{i=1}^n \left[ y_i - \alpha - \beta x_i \right]$$

Equating this derivative to 0:

$$\sum_{i=1}^n \left[ y_i - \alpha - \beta x_i \right] = 0$$

Expand the summation:

$$\sum_{i=1}^n y_i - n\alpha - \beta \sum_{i=1}^n x_i = 0$$

Rearrange:

$$\sum_{i=1}^n y_i =n\alpha + \beta \sum_{i=1}^n x_i$$

Differentiating $E=\sum_{i = 1}^{n}e_i^2$ with respect to $\beta$:

$$\frac{\partial S}{\partial \beta} = \sum_{i=1}^n 2 \left[ y_i - (\alpha + \beta x_i) \right](-x_i)$$

Simplify:

$$\frac{\partial S}{\partial \beta} = -2 \sum_{i=1}^n x_i \left[ y_i - \alpha - \beta x_i \right]$$

Set this derivative to 0:

$$\sum_{i=1}^n x_i \left[ y_i - \alpha - \beta x_i \right] = 0$$

Expand the summation:

$$\sum_{i=1}^n x_i y_i - \alpha \sum_{i=1}^n x_i - \beta \sum_{i=1}^n x_i^2 = 0$$

Rearrange:

$$\sum_{i=1}^n x_i y_i = \alpha \sum_{i=1}^n x_i + \beta \sum_{i=1}^n x_i^2$$

$$\sum_{i = 1}^{n}{y_{i}x_{i} = \mathbf{a}\sum_{i = 1}^{n}x_{i} + \mathbf{b}\sum_{i = 1}^{n}x_{i}^{2}}$$

On solving the above equations we will get

Regression coefficient, $$b=\frac{\sum_{i = 1}^{n}{y_{i}x_{i} - \frac{\sum_{i = 1}^{n}{y_{i}\sum_{i = 1}^{n}x_{i}}}{n}}}{\sum_{i = 1}^{n}x_{i}^{2} - \frac{\left( \sum_{i = 1}^{n}x_{i} \right)^{2}}{n}}$$

$$=\frac{cov(x,y)}{var(x)}$$

$$\mathbf{b =}\frac{\mathbf{cov(x,y)}}{\mathbf{var(x)}}
$$

$$\mathbf{a = }\overline{\mathbf{y}}\mathbf{- b}\overline{\mathbf{x}}$$

where $\overline{y}$ = mean of *y*; $\overline{x}$ = mean of *x*

## Two lines of regression

There are two lines of regression- that of *y* on *x* and *x* on *y*.

[Regression of *y* on *x*]{.underline}

Consider the two variables *x* and *y*, if you are considering *y* as dependent variable and *x* as independent variable then your equation is:

***y*** **= *a* + *bx***

This is used to predict the unknown value of variable *y* when value of variable *x* is known. Usually ***b*** here is denoted as ***b***~yx~

$$\mathbf{b}_{\mathbf{\text{yx}}}\mathbf{=}\frac{\mathbf{cov(x,y)}}{\mathbf{var(x)}}$$

Consider Example 8.1; considering ice cream sales as dependent variable and temperature as independent variable

```{r r6, echo=FALSE,fig.cap='Scatter diagram of data in Example 8.1',out.width="50%", fig.align='center'}
knitr::include_graphics(rep("images/r7.jpeg"))
```

[Regression of *x* on *y*]{.underline}

Consider the two variables *x* and *y*, if you are considering *x* as dependent variable and *y* as independent variable then your equation is:

***x*****= *c* + *my;*** where ***c*** is the intercept and ***m*** is the slope

This is used to predict the unknown value of variable *x* when value of variable *y* is known. Usually ***b*** here is denoted as ***b***~xy~

$$\mathbf{b}_{\mathbf{\text{xy}}}\mathbf{=}\frac{\mathbf{cov(x,y)}}{\mathbf{var(y)}}$$

Consider Example 8.1; considering temperature as dependent variable and ice cream sales as independent variable

```{r r7, echo=FALSE,fig.cap='Scatter diagram of data in Example 8.1',out.width="50%", fig.align='center'}
knitr::include_graphics(rep("images/r8.jpeg"))
```

You can see both the regression were different. It depends on the experimenter to choose dependent and independent variable. In the above example it is evident that considering temperature as dependent variable is meaningless, *i*.*e*. what is the usefulness in predicting temperature based on ice cream sales?. So the selection of dependent and independent variable is entirely the discretion of experimenter based on the objective of his study.

## Assumptions of Regression

If *y* is the dependent variable and *x* is the independent variable then

1.  The *x*'s are non-random or fixed constants.

2.  At each fixed value of *x* the corresponding values of *y* have a normal distribution about a mean.

3.  For any given *x*, the variance of *y* is same.

4.  The values of *y* observed at different levels of *x* are completely independent.

## Properties of Regression coefficients

1.  The correlation coefficient between *x* and *y* is the geometric mean of the two regression coefficients ***b***~yx~ and ***b***~xy~

$$r = \sqrt{b_{\text{yx}}b_{\text{xy}}}$$

2.  Regression coefficients are independent of change of origin but not of scale.

3.  If one regression coefficient is greater than unity, then the other must be less than unity but not vice versa. *i*.*e*. both the regression coefficients can be less than unity but both cannot be greater than unity, *i*.*e*. if ***b***~yx~ \>1 then ***b***~xy~ \<1 and if ***b***~xy~ \>1, then ***b***~yx~ \<1.

4.  Also if one regression coefficient is positive the other must be positive (in this case the correlation coefficient is positive) and if one regression coefficient is negative the other must be negative (in this case the correlation coefficient is negative).

5.  The range of regression coefficients is $- \infty$ to $+ \infty$.

6.  If the variables ( X ) and ( Y ) are independent, the regression coefficients are zero. This is referred to as the independence property of regression coefficients.

## Uses of Regression

**Prediction**: The regression analysis is useful in predicting the value of one variable from the given value of another variable. Such predictions are useful when it is very difficult or expensive to measure the dependent variable, Y.

**Identify the strength of relationship**: The regression might be used to identify the strength of the effect that the independent variable(s) have on a dependent variable. Like the strength of relationship between dose and effect, sales and marketing spending, or age and income.

**Forecast effects or impact of changes**: That is, the regression analysis helps us to understand how much the dependent variable changes with a change in one or more independent variables. A typical question is, "how much additional sales income do I get for each additional 1000 spent on marketing.

**Predicts trends and future values**: The regression analysis can be used to predict trend and future values, like "what will be the price of gold in 6 months?"

## Properties of Regression lines

1.  Regression lines minimize the sum of squared deviations of observed values from the predicted values, ensuring the best possible fit.

2.  The regression lines intersect at the mean values of 𝑋and𝑌 *i.e.,* at ($\overline{X}$,$\overline{Y}$)

3.  The slopes of the regression lines are related to the correlation coefficient r. If r=0, the lines are perpendicular, indicating no linear relationship.

## Example problem

Now consider the example 8.1 and answer the questions

| Temperature (°C) | Ice Cream Sales (in \$) |
|:-----------------|:------------------------|
| 14.2             | 215                     |
| 16.4             | 325                     |
| 11.9             | 185                     |
| 15.2             | 332                     |
| 18.5             | 406                     |
| 22.1             | 522                     |
| 19.4             | 412                     |
| 25.1             | 614                     |
| 23.4             | 544                     |
| 18.1             | 421                     |
| 22.6             | 445                     |
| 17.2             | 408                     |

1.  What is the functional form of relationship between Temperature and Ice cream sales?

2.  What will be the Ice cream sales when temperature is 20^o^ Celsius?

**Solution**

1.  Fit a model considering Ice cream sales as dependent variable (*y*) and temperature as independent variable (*x*). Fitting a model means estimating ***b*** and ***a*** using equation.

2.  After fitting the model put 20 in the *x* value you will get the predicted *y* value

Model: *y* = ***a***+***b**x*

```{r ex85, echo=FALSE,warning=FALSE,results='asis'}
library(knitr)
library(kableExtra)
dt<-read.csv("csv/Book12.csv")
colnames(dt) <- c("Sl No.", "Temperature (x)", "Sales (y)", "$(x_{i}-\\overline{x})$","$(y_{i}-\\overline{y})$ ", "$(x_{i}-\\overline{x})(y_{i}-\\overline{y})$", "$(x_{i}-\\overline{x})^2$")
kable(dt,escape = FALSE) %>% kable_classic(full_width = F, html_font = "Cambria")%>%
kable_styling(position="center",bootstrap_options = c("striped"))%>%
  row_spec(0, bold = TRUE, align = "c")
```

*n* =12

$$mean,\overline{x} = \ \frac{224.1}{12} = 18.675$$

$$mean,\overline{y} = \ \frac{4829}{12} = 402.416$$

*Cov* (*x*,*y*) = $$\frac{1}{n}\sum_{i = 1}^{n}{\left( x_{i} - \overline{x} \right)\left( y_{i} - \overline{y} \right)}$$

$$\sum_{i = 1}^{12}{\left( x_{i} - \overline{x} \right)\left( y_{i} - \overline{y} \right)} = 5325.03$$

$$Cov (x,y) = \frac{5325.03}{12} = 443.752$$

$$variance\ of\ x,\ var\left( x \right) = \ \frac{1}{n}\sum_{i = 1}^{n}\left( x_{i} - \overline{x} \right)^{2} = \frac{176.983}{12} = 14.7485$$

$$\mathbf{b =}\frac{\mathbf{cov(x,y)}}{\mathbf{var(x)}}$$

$$\mathbf{b =}\frac{443.752}{14.7485}\mathbf{=}30.088$$

$$\mathbf{a =}\overline{\mathbf{y}}\mathbf{- b}\overline{\mathbf{x}}$$

$$\mathbf{a =}402.416 - 30.088\left( 18.675 \right) = \  - 159.477$$

So our model is

$$y = \  - 159.477 + 30.088x$$

$$Ice\ cream\ sales = \  - 159.477 + 30.088(Temperature)$$

Ice cream sales when temperature is 20^o^ Celsius

$$x = 20$$

$y = \  - 159.477 + 30.088(20)$ = 442.283

So the predicted ice cream sales at 20^o^ Celsius is **442.283**

.  \
 

## Partial and multiple correlation {#sec-partialmulti}

## 

::: {#hello8 .greeting .message style="color: #c9c6c5;"}
<center>**"Statistics is the art of never having to say you're wrong":-Robert P. Abelson**</center>
:::
