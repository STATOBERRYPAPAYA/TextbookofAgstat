[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "TEXTBOOK OF AGRICULTURAL STATISTICS",
    "section": "",
    "text": "Welcome\nWelcome to the Textbook of Agricultural Statistics – a comprehensive resource thoughtfully crafted by the Department of Agricultural Statistics at the College of Agriculture, Vellayani, Kerala Agricultural University. This book was created to meet the need for a clear, accessible, and practical guide to statistics tailored for agricultural research.\nStatistics is an essential tool in agriculture, enabling researchers and practitioners to uncover patterns, validate results, and make data-driven decisions. However, the subject is often perceived as complex and challenging to master. This textbook is designed to change that perception, offering straightforward explanations and practical guidance to make statistical concepts approachable and applicable.\nAs John Tukey once said, “The greatest value of a picture is when it forces us to notice what we never expected to see.” We have embraced this philosophy by incorporating clear examples, practical applications, and data visualizations to illustrate concepts and deepen understanding.\nWhile this book is primarily written with undergraduate students in mind, its simplicity and focus on real-world applications make it a valuable resource for a wide audience, including postgraduate students, researchers, and anyone seeking to build a strong foundation in statistics and experimental design.\nYou will find clear explanations, practical examples, and step-by-step instructions throughout the chapters, all tailored to the unique needs of agricultural studies. Our goal is to ensure that learners at all levels can confidently apply statistical methods to their work and research.\nWhether you are new to statistics or looking to revisit the basics with a fresh perspective, we hope this book serves as a supportive companion in your journey to understanding and applying statistical tools effectively in agriculture.\n\nAcknowledgements\nThis book is the result of collaborative efforts among dedicated teachers and statisticians, but the majority of the reviewing, editing, and refinement has been inspired and shaped by the students.\nFor two years, this book was made available online on the MeLON (Module for eLearning and Online Notes) platform of the College of Agriculture, Vellayani. During this time, students provided valuable feedback, pointed out areas for improvement, and offered insights that greatly enhanced the quality and clarity of the content. We are sincerely grateful to all the students whose suggestions and input played a key role in the development of this textbook.\nWe would also like to thank the College of Agriculture, Vellayani, for fostering an environment that encourages learning, growth, and the sharing of ideas.\n\n\nNote from the Publisher\nThis textbook is published by PAPAYA, the publication division of Statoberry LLP, which is committed to providing high-quality educational resources for agricultural research. The online version of this book is available for free, in line with our dedication to open access and knowledge sharing. Visit us at PAPAYA.\n\n\nCopyright Information\n© 2024 Statoberry LLP. All rights reserved.\nOnline version of this book is licensed under a\n\nCreative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.\nUnder this license:\n- Attribution: You must give appropriate credit, provide a link to the license, and indicate if changes were made.\n- Non Commercial: You may not use the material for commercial purposes.\n- No Derivatives: If you remix, transform, or build upon the material, you may not distribute the modified material.\nFor commercial use or distribution of print copies, prior written permission from Statoberry LLP is required.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "Preface",
    "section": "",
    "text": "For a long time, I have dreamed of writing a book that truly serves the needs of undergraduate students in agriculture—a book that demystifies statistics and makes it accessible and practical for their studies and research. Statistics, while being an essential tool in agricultural sciences, is often presented in ways that make it seem more complicated than it actually is. Textbooks in this field tend to delve into intricate details that go far beyond what most agricultural students require, leaving them overwhelmed and disconnected from the subject’s practical relevance.\nThis book is my humble attempt to change that. It has been written with undergraduate students in mind, focusing on the basics of statistics and their direct applications in agricultural research. Each chapter is designed to simplify complex concepts, making them clear, relatable, and easy to understand. While the primary audience is undergraduate students, this book can also serve as a helpful resource for anyone looking to brush up on the fundamentals of statistics.\nThe journey of writing this book has been greatly enriched by the feedback and insights of the students at the College of Agriculture, Vellayani. For two years, an earlier version of this book was made available on MeLON (Module for eLearning and Online Notes), our college’s online platform. The students, with their thoughtful suggestions and sharp observations, have helped refine the content and shape it into what it is today. Their enthusiasm and curiosity have been a constant source of inspiration throughout this process.\nI would also like to express my heartfelt gratitude to my co-authors, Dr. Manju Mary Paul, Dr. Adarsh V. S., and Mohammed Hisham M., whose expertise, commitment, and contributions have been invaluable in bringing this book to life. Their collaboration and dedication have greatly enhanced the quality and depth of this work.\nA special thanks to Jithin Chandran, Gaatha Prasad, Anjana Biwas T., and Varsha H. for their valuable suggestions and minor corrections. They were postgraduate students in Agricultural Statistics at the time of writing this book, and their support has significantly increased the quality of this work.\nI hope this textbook becomes a guiding light for students and researchers alike, helping them build a solid foundation in statistics while inspiring confidence in their ability to use these tools effectively. If this book makes statistics less intimidating and more approachable for even one reader, I will consider my efforts worthwhile.\nWith deep gratitude to my students, colleagues, and everyone who supported this work, I present this book as a tool to empower the next generation of agricultural scientists and researchers.\nDr. Pratheesh P. Gopinath\nHead\nDepartment of Agricultural Statistics\nCollege of Agriculture, Vellayani\n2 December 2024",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Basics of statistics",
    "section": "",
    "text": "1.1 The word “statistics”\nStatistics is the science of understanding, analyzing, and interpreting data. It plays a crucial role in making informed decisions across various fields, from agriculture to medicine, economics to environmental studies. This chapter serves as an entry point into the fascinating world of statistics, introducing you to its basic concepts and practical applications.\nWe begin by exploring the origins and definitions of statistics, emphasizing its relationship with mathematics and its distinct role in solving real-world problems. From there, we focus on the importance of data—the raw material of statistics—examining its types and how it is collected, organized, and analyzed.\nThe chapter also covers essential concepts such as population and sample, variables and constants, and the different types of variables. These concepts form the building blocks for understanding how statistical studies are designed and conducted.\nFinally, we introduce frequency distributions—an indispensable tool for summarizing and interpreting data. Topics such as construction of frequency distributions, grouped and cumulative frequency distributions, and relative frequency will help you make sense of data and uncover underlying patterns.\nBy the end of this chapter, you will have a comprehensive understanding of the core principles of statistics, setting the stage for deeper exploration and advanced applications in later chapters. The concepts presented here are largely based on the works of (Goon and Dasgupta 1983) and (Gupta and Kapoor 1997)\nThe term statistics originates from the Neo-Latin word statisticum collegium, meaning “council of state,” and the Italian word statista, meaning “statesman” or “politician.” The German term Statistik emerged in the early 18th century and initially referred to the “collection and classification of data,” particularly data used by governments and administrative bodies. This usage was introduced by the German scholar Gottfried Achenwall in 1749, who is often credited as the founder of modern statistics.\nIn 1791, Sir John Sinclair introduced the term Statistik into English through his publication of the “Statistical Account of Scotland”(Ball 2004), a comprehensive 21-volume work. This marked the beginning of the use of the term statistics in English to describe the systematic collection and analysis of data. Later, in 1845, Francis G.P. Neison an actuary1 to the Medical Invalid and General Life Office published Contributions to Vital Statistics, the first book to include the word “statistics” in its title, focusing on actuarial and demographic data. These developments laid the foundation for statistics as a discipline, evolving from statecraft to a broader scientific approach to data analysis and interpretation.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Basics of statistics</span>"
    ]
  },
  {
    "objectID": "intro.html#the-word-statistics",
    "href": "intro.html#the-word-statistics",
    "title": "1  Basics of statistics",
    "section": "",
    "text": "Figure 1.1: Statistical Account of Scotland by Sir John Sinclair (1791)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Basics of statistics</span>"
    ]
  },
  {
    "objectID": "intro.html#statistics-and-mathematics",
    "href": "intro.html#statistics-and-mathematics",
    "title": "1  Basics of statistics",
    "section": "1.2 Statistics and mathematics",
    "text": "1.2 Statistics and mathematics\nMathematics and statistics, while closely related, serve distinct purposes and operate on fundamentally different principles. Mathematics can be thought of as a well-organized library, where everything follows strict rules and logical paths. Once a theorem is proven in mathematics, it remains universally true, leaving little room for ambiguity or change. It is a deductive science, relying on precise axioms and logical reasoning to arrive at exact and unchanging results.\nStatistics, however, operates in a different realm. It deals with real-world data, which is often messy, unpredictable, and influenced by numerous uncontrolled factors. Statistics is more like an open field, where methods and approaches must adapt to the variability of data. Unlike the certainty of mathematics, statistics uses inductive reasoning to analyze data, account for randomness, and make decisions or predictions under uncertainty. This flexibility is essential because real-world phenomena, especially in fields like biology, are rarely as neat and predictable as mathematical constructs.\nIn biological sciences, we study complex systems such as plants, animals, and ecosystems, where exact outcomes are rarely achievable. These systems are influenced by a multitude of factors, many of which cannot be precisely measured or controlled. This is where the concept of the error term becomes important. The error term represents the difference between observed and predicted values in a statistical model, accounting for the inherent variability and uncertainty in biological phenomena.\nStatisticians embrace this uncertainty, developing mathematical models that approximate reality as closely as possible. Unlike mathematicians, whose focus is on achieving perfect precision, statisticians aim to draw meaningful insights from imperfect and variable data. In the study of biological systems, the goal is not to eliminate uncertainty but to understand patterns, relationships, and trends within the data.\nThus, while mathematics seeks absolute certainty, statistics accepts variability and uncertainty as fundamental characteristics of the real world. By acknowledging and incorporating these uncertainties, statisticians provide valuable tools to study and explain complex biological phenomena, making statistics an indispensable discipline for understanding the complexities of nature.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Basics of statistics</span>"
    ]
  },
  {
    "objectID": "intro.html#definition-of-statistics",
    "href": "intro.html#definition-of-statistics",
    "title": "1  Basics of statistics",
    "section": "1.3 Definition of statistics",
    "text": "1.3 Definition of statistics\nStatistics is the science which deals with the\n\nCollection of data\nOrganization of data or classification of data\nPresentation of data\nAnalysis of data\nInterpretation of data\n\n\n\n\n\n\n\nJust for Fun\n\n\n\nLet’s give a definition to statistics using the words themselves:\nStrengthening Technological Advancement Through Implementing Systematic Techniques in Contemporary Sciences\n\n\nTwo main branches of statistics are:\nDescriptive statistics, which deals with summarizing data from a sample using indexes such as the mean or standard deviation etc.\nInferential statistics, use a random sample of data taken from a population to describe and make inferences about the population parameters.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Basics of statistics</span>"
    ]
  },
  {
    "objectID": "intro.html#data",
    "href": "intro.html#data",
    "title": "1  Basics of statistics",
    "section": "1.4 Data",
    "text": "1.4 Data\nData can be defined as individual pieces of factual information that are recorded and used to draw meaningful insights through the science of statistics. Think of data as the building blocks that form the foundation for understanding the world around us. It’s the raw material from which we extract patterns, trends, and conclusions that help us make better decisions.\nIn today’s fast-paced world, data is more important than ever. From predicting weather patterns to optimizing business strategies, data is at the heart of nearly every advancement. Without data, we’re left with guesswork—making it impossible to understand complex systems or make informed decisions.\nHere are some examples of data in action:\n\nNumber of farmers in a village: Understanding this helps policymakers make decisions about agricultural development and rural economics.\nRainfall over a period of time: This data is crucial for predicting crop yields, planning irrigation, and managing water resources.\nArea under paddy crop in a state: This informs agricultural policies, resource allocation, and even global food supply chains.\n\nAs you can see, data isn’t just a collection of numbers; it’s the key to solving real-world problems and shaping the future. In the hands of skilled statisticians, data has the power to unlock insights that can improve lives, drive innovation, and guide decisions at every level.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Basics of statistics</span>"
    ]
  },
  {
    "objectID": "intro.html#scope-and-limits",
    "href": "intro.html#scope-and-limits",
    "title": "1  Basics of statistics",
    "section": "1.5 Scope and limits",
    "text": "1.5 Scope and limits\nFunctions of statistics: Statistics plays a crucial role in simplifying complex data, transforming it into clear and meaningful information. It supports decision-making by presenting facts in an organized manner, aids in the formulation of effective policies, facilitates comparisons, and assists in making forecasts. By applying appropriate statistical methods, researchers can draw valid conclusions from experiments.\nApplications of statistics: Statistics has become an integral part of almost every field of human activity. It is indispensable in areas such as administration, business, economics, research, banking, insurance, and more. Its ability to quantify and analyze data makes it an essential tool across industries.\nCommon limitations of statistics: Statistical methods are applicable only when there is variability in the data being studied. Statistics focuses on the analysis of groups or aggregates, rather than individual data points. The results derived from statistical analysis are often approximate and subject to uncertainty. Statistics is sometimes misapplied or misinterpreted, leading to erroneous conclusions.\nAs statisticians, we believe that the power of statistics knows no bounds. It’s a tool that, when applied correctly, can unlock insights from any dataset. While the limitations listed above are commonly found in textbooks and curricula across SAUs (State Agricultural Universities), I believe these are more about guiding students on the appropriate use of statistics rather than presenting true constraints. With the right methodology and approach, statistics can be applied in any situation to derive valuable insights and support sound decision-making.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Basics of statistics</span>"
    ]
  },
  {
    "objectID": "intro.html#population-and-sample",
    "href": "intro.html#population-and-sample",
    "title": "1  Basics of statistics",
    "section": "1.6 Population and sample",
    "text": "1.6 Population and sample\nConsider the following example. Suppose we wish to study the height of all students in a college. It will take us a long time to measure the height of all students of the college, so we may select 20 of the students and measure their height (in cm). Suppose we obtain the measurements like this :\n149, 156, 148, 161, 159, 143, 158, 152, 164, 171, 157, 152, 163,\n158, 151, 147, 157, 146, 153, 159.\nIn this study, we are interested in the height of all students in the college. The set of height of all students in the college is called the population of this study. The set of 20 height, H = {149, 156, 148, …, 153, 159}, is a sample from this population.\nPopulation\nIn statistics, a population refers to the entire collection of elements, individuals, or objects that possess a particular characteristic and are the subject of a statistical study. It encompasses all possible observations or measurements that could be included in the analysis. For example, a population could be all the students in a university, all the trees in a forest, or all the farms in a region. The population provides the complete set of data from which conclusions can be drawn.\nSample\nA sample is a subset of a population selected for the purpose of conducting a statistical analysis. It represents a smaller group drawn from the population, ideally chosen to reflect its characteristics. Samples are used to estimate population parameters when it is impractical or impossible to collect data from the entire population. The key to a good sample is that it should be representative of the population to allow valid inferences to be made.\nPopulation parameter\nA population parameter is a numerical characteristic or value that describes an aspect of an entire population. It is a fixed (constant), often unknown value that represents the true measurement of a specific attribute for every member of the population. Common population parameters include the population mean, population variance, and population proportion. Since it is usually impractical or impossible to measure the entire population, parameters are often estimated using sample data.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Basics of statistics</span>"
    ]
  },
  {
    "objectID": "intro.html#variables-and-constants",
    "href": "intro.html#variables-and-constants",
    "title": "1  Basics of statistics",
    "section": "1.7 Variables and constants",
    "text": "1.7 Variables and constants\nVariables\nA variable is a characteristic or attribute that can take different values for different individuals, at different times, or in different locations. In other words, variables are subject to change. Examples of variables include:\n\nThe number of fruits on a branch, the number of plots in a field, or the number of schools in a country.\nPlant height, crop yield, panicle length, or temperature.\n\nVariables can be classified into two broad categories: quantitative variables, which are measured on a numerical scale (such as height or yield), and qualitative (or categorical) variables, which describe categories or characteristics (such as plant species or color).\nConstants\nA constant refers to a value that does not change under any circumstances. Unlike variables, constants retain the same value throughout the study. Examples of constants include:\n\nMathematical values such as pi (\\(\\pi\\)), which is the ratio of the circumference of a circle to its diameter (\\(\\pi\\) = 3.14159…), and e, the base of the natural logarithms (e = 2.71828).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Basics of statistics</span>"
    ]
  },
  {
    "objectID": "intro.html#types-of-variables",
    "href": "intro.html#types-of-variables",
    "title": "1  Basics of statistics",
    "section": "1.8 Types of variables",
    "text": "1.8 Types of variables\nQuantitative variables\nA quantitative variable is one that can be expressed in numerical terms and takes values that are measurable. Examples of quantitative variables include the number of fruits on a branch, the number of plots in a field, the number of schools in a country, plant height, crop yield, panicle length, and temperature. Quantitative variables can be further classified into two categories: discrete and continuous.\nDiscrete variables\nDiscrete variables are variables that can only take a finite or countable number of distinct values. They are often whole numbers and can be counted. For instance, the number of fruits on a branch, the number of plots in a field, or the number of schools in a country are all discrete variables.\nSince discrete variables represent countable quantities, they can only take specific, separate values, such as 0, 1, 2, etc. For example, the number of daily hospital admissions is a discrete variable because it can only take whole number values like 0, 1, or 2, but not fractional values like 1.8 or 3.96.\nContinuous variables\nContinuous variables, on the other hand, are variables that can take any value within a given range or interval and can be measured. These variables do not have distinct gaps or interruptions in their possible values. For example, plant height, yield, temperature, and panicle length are continuous variables because they can be measured to a high degree of precision, such as 5.5 cm, 5.8 cm, or any value within a relevant range. Continuous variables can assume an infinite number of possible values within a given range, making them different from discrete variables.\nCategorical variables\nA categorical variable is a type of variable where the data is divided into distinct categories that do not have a numerical value. For example, marital status (single, married, widowed), employment status (employed, unemployed), or religious affiliation (Protestant, Catholic, Jewish, Muslim, others) are examples of categorical variables. These variables are often referred to as qualitative variables, as they describe qualities or characteristics rather than measurable quantities.\nUnlike quantitative variables, categorical variables cannot be measured or counted in the traditional sense. Instead, they classify data into specific groups or categories.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Basics of statistics</span>"
    ]
  },
  {
    "objectID": "intro.html#measurement-scales",
    "href": "intro.html#measurement-scales",
    "title": "1  Basics of statistics",
    "section": "1.9 Measurement scales",
    "text": "1.9 Measurement scales\nVariables can be classified into four distinct levels of measurement scales each representing a different way of organizing and interpreting data. These four levels are nominal, ordinal, interval, and ratio.\nNominal scale\nThe nominal scale is the most basic level of measurement and applies to categorical (qualitative) variables. Data measured on the nominal scale consist of categories that are distinct but have no inherent order or ranking. The categories are simply used for labeling or naming objects or groups. For example, gender (male, female), blood group (A, B, AB, O), and marital status (single, married, divorced) are all nominal variables. In the nominal scale, arithmetic operations, such as addition or subtraction, cannot be performed on the data.\nOrdinal scale\nThe ordinal scale also applies to qualitative data, but with an important distinction: the data on the ordinal scale are ordered. This means that the categories have a specific rank or order, but the differences between the categories are not necessarily uniform or meaningful. For example, the grades given in a class (excellent, good, fair, poor) are ordinal, where “excellent” is ranked higher than “good,” and “good” is ranked higher than “fair,” and so on. However, the difference between “excellent,” and “good” is not numerically defined, making the exact magnitude of the difference unclear. Ordinal data allows us to say that one value is greater or lesser than another, but it does not allow for the measurement of exact differences.\nInterval scale\nThe interval scale is used for quantitative (numerical) data, and it provides more information than the nominal or ordinal scales. On the interval scale, the data points are ordered, and the differences between them are meaningful and measurable. However, the interval scale does not have a true zero point. This means that while we can measure the difference between values, we cannot make statements about ratios between them. An example of an interval scale is temperature measured in Celsius or Fahrenheit. For instance, if the temperature in two cities is 20°C and 30°C, we can say that the temperature in the second city is 10°C higher. However, we cannot say that the second city is “twice as hot” as the first city, because the zero point (0°C) does not represent the absence of temperature.\nRatio scale\nThe ratio scale is the highest level of measurement and applies to quantitative data. It shares the properties of the interval scale—ordered data with measurable differences between values—but it also has a meaningful zero point. This true zero point represents the total absence of the quantity being measured. With the ratio scale, not only can we measure differences between values, but we can also compute meaningful ratios. For example, weight is measured on the ratio scale. A weight of 60 kg is twice as much as a weight of 30 kg, and a weight of 0 kg indicates the complete absence of weight. Similarly, temperature measured on the Kelvin scale is an example of a ratio scale, where 0 Kelvin represents absolute zero, the complete absence of heat.\nIn summary, the key distinctions between these measurement scales are:\n\nNominal: Categories without any order.\nOrdinal: Ordered categories without consistent differences.\nInterval: Ordered data with meaningful differences, but no true zero.\nRatio: Ordered data with meaningful differences and a true zero point, allowing for meaningful ratios.\n\n\n\n\n\n\n\nFigure 1.2: Classification of variables",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Basics of statistics</span>"
    ]
  },
  {
    "objectID": "intro.html#collection-of-data",
    "href": "intro.html#collection-of-data",
    "title": "1  Basics of statistics",
    "section": "1.10 Collection of data",
    "text": "1.10 Collection of data\nThe process of collecting data is the foundational step in any statistical investigation or research study. Data can be gathered for an entire population or for a sample drawn from it. Typically, data collection is performed on a sample basis, especially when studying large populations. Collecting data is a challenging task, requiring skill and precision. The person responsible for gathering the data, known as the enumerator or investigator, must be well-trained to ensure the accuracy and reliability of the data collected. The individuals or groups providing the information are referred to as the respondents.\n\n1.10.1 Types of data\nData collection can be categorized into two main types based on the source from which the data is derived:\n\nPrimary Data\nSecondary Data\n\nPrimary data\nPrimary data refer to first-hand, original data that are collected directly by the researcher or an organization for a specific purpose. These data have not been processed or analyzed previously and are considered the most authentic form of data. Primary data are typically gathered through surveys, interviews, experiments, or observations, and they represent a direct reflection of the phenomena being studied.\nExample: Population census data collected by the government are considered primary data. These are collected directly from individuals by government authorities for the purpose of census enumeration and demographic analysis.\nSecondary data\nSecondary data refer to data that have already been collected, processed, and published by other organizations or researchers for a different purpose. These data may have undergone some degree of analysis or treatment before being made available for new studies. Secondary data are often more convenient to use, as they are readily accessible, but they may not always perfectly suit the specific needs of the researcher.\nExample: An economic survey of a country, such as reports from the Bureau of Statistics or other governmental agencies, is an example of secondary data. These data were originally collected for purposes such as policy analysis or economic planning, and now can be used for additional research.\nThe distinction between primary and secondary data lies primarily in their origin and the process of collection. Primary data are first-hand, original data collected directly from a single source by the researcher for a specific purpose. These data are considered pure as they have not undergone any prior statistical treatment. In contrast, secondary data are obtained from existing sources or agencies and have been previously collected and processed for different purposes. They are not considered pure as they have undergone some form of statistical treatment. While primary data are original and collected for the first time, secondary data are pre-existing and gathered from other sources. Both types of data have their respective advantages and limitations, and the choice between them depends on the research objectives, availability of resources, and the nature of the study.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Basics of statistics</span>"
    ]
  },
  {
    "objectID": "intro.html#collecting-primary-data",
    "href": "intro.html#collecting-primary-data",
    "title": "1  Basics of statistics",
    "section": "1.11 Collecting primary data",
    "text": "1.11 Collecting primary data\nPrimary data can be collected using various methods, depending on the research requirements and resources available:\nPersonal investigation\nIn this method, the researcher directly conducts the survey and collects the data themselves. This approach often results in highly accurate and reliable data. It is best suited for small-scale research projects where direct involvement of the researcher is feasible.\nThrough investigation\nIn this method, trained investigators are employed to collect data. These investigators engage with individuals, asking questions and filling out questionnaires based on the responses. This method is widely used by organizations for larger data collection efforts.\nCollection through questionnaire\nResearchers distribute questionnaires to local representatives or agents who collect data based on their own experience and observations. While this method is relatively quick, it typically provides only a rough estimate of the information.\nThrough the phone\nData is gathered by contacting individuals via telephone/mobile phone. This method is fast and allows for accurate information to be collected efficiently, making it suitable for studies that require a broad reach but still need reliable data.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Basics of statistics</span>"
    ]
  },
  {
    "objectID": "intro.html#collecting-secondary-data",
    "href": "intro.html#collecting-secondary-data",
    "title": "1  Basics of statistics",
    "section": "1.12 Collecting secondary data",
    "text": "1.12 Collecting secondary data\nSecondary data are collected through various established channels:\nOfficial\nOfficial sources include publications from government bodies such as the Statistical Division, Ministry of Finance, Federal Bureaus of Statistics, and various ministries (e.g., Agriculture, Food, Industry, Labor). These sources provide comprehensive and authoritative data.\nSemi-Official\nSemi-official sources include publications from institutions like the State Bank, Railway Board, Central Cotton Committee, and Boards of Economic Enquiry. It also encompasses reports from trade associations, chambers of commerce, technical journals, trade publications, and research organizations such as universities and other academic institutions. These sources provide valuable data, though they may not be as universally authoritative as official sources.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Basics of statistics</span>"
    ]
  },
  {
    "objectID": "intro.html#frequency-distribution",
    "href": "intro.html#frequency-distribution",
    "title": "1  Basics of statistics",
    "section": "1.13 Frequency distribution",
    "text": "1.13 Frequency distribution\nThe data presented below shows the number of fruits per branch in a mango tree selected from a particular plot. The data, presented in this form in which it was collected, is called raw data.\n0, 1, 0, 5, 2, 3, 2, 3, 1, 5,\n5, 2, 3, 4, 4, 5, 4, 0, 5, 4,\n2, 4, 4, 4, 1\nIt can be seen that, the minimum and the maximum numbers of fruits per branch are 0 and 5, respectively. Apart from these numbers, it is impossible, without further careful study, to extract any exact information from the data. But by breaking down the data into the form below\n\n\n\nFrequency distribution table\n\n\nNow certain features of the data become apparent. For instance, it can easily be seen that, most of the branches selected have four fruits because number of branches having 4 fruits is 7. This information cannot easily be obtained from the raw data. The above table is called a frequency table or a frequency distribution. It is so called because it gives the frequency or number of times each observation occurs. Thus, by finding the frequency of each observation, a more intelligible picture is obtained.\n\n1.13.1 Construction\nIn this section, we will discuss the process of constructing a frequency distribution. Follow the steps below. This method helps to clearly visualize the frequency of each observation, ensuring that the total frequency adds up to the total number of observations.\n\nList all values of the variable in ascending order of magnitude.\nForm a tally column, that is, for each value in the data, record a stroke in the tally column next to that value. In the tally, each fifth stroke is made across the first four. This makes it easy to count the entries and enter the frequency of each observation.\nCheck that the frequencies sum to the total number of observations.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Basics of statistics</span>"
    ]
  },
  {
    "objectID": "intro.html#grouped-frequency-distribution",
    "href": "intro.html#grouped-frequency-distribution",
    "title": "1  Basics of statistics",
    "section": "1.14 Grouped frequency distribution",
    "text": "1.14 Grouped frequency distribution\nData below gives the plant height of 20 paddy varieties, measured to the nearest centimeters.\n109, 107, 129, 122, 118, 110, 124, 146, 138, 121,\n115, 132, 131, 139, 142, 134, 143, 144, 127, 116\nIt can be seen that the minimum and the maximum plant height are 107 cm and 144 cm, respectively. A frequency distribution giving every plant height between 107 cm and 144 cm would be very long and would not be very informative. The problem is to overcome by grouping the data into classes.\nIf we choose the classes\n100 – 109\n110 – 119\n120 – 129\n130 – 139\n140 – 149\nwe obtain the frequency distribution given below:\n\n\n\nGrouped Frequency distribution table\n\n\nAbove table gives the frequency of each group or class; it is therefore called a grouped frequency table or a grouped frequency distribution. Using this grouped frequency distribution, it is easier to obtain information about the data than using the raw data. For instance, it can be seen that 14 of the 20 paddy varieties have plant height between 110 cm and 139 cm (both inclusive). This information cannot easily be obtained from the raw data.\nIt should be noted that, even though above table is concise, some information is lost. For example, the grouped frequency distribution does not give us the exact plant height of the paddy varieties. Thus the individual plant height of the paddy varieties are lost in our effort to obtain an overall picture.\n\n1.14.1 Terminologies\nClass limits\nThe intervals into which the observations are put are called class intervals. The end points of the class intervals are called class limits. For example, the class interval 100 – 109, has lower class limit 100 and upper class limit 109.\nContinuous classes\nContinuous classes are intervals where the class limits represent a continuous range of values, with no gaps between the intervals.\nExample: If the class intervals are 10 − 20, 20−30, 30−40, and so on, there are no gaps between them, and all values within these ranges are included seamlessly.\nDiscontinuous classes\nDiscontinuous classes are intervals where gaps exist between the class limits. In such cases, class boundaries are used to close the gaps and ensure continuity.\nExample:\nIf the class intervals are 10 - 19, 20 - 29, 30 - 39, and so on, there is a gap between the end of one interval and the start of the next. The actual range of each interval is defined using class boundaries, which is explained below.\nClass boundaries\nThe raw data in the above example were recorded to the nearest centimeters. Thus, a plant height of 109.5cm would have been recorded as 110cm, a plant height of 119.4 cm would have been recorded as 119cm, while a plant height of 119.5 cm would have been recorded as 120 cm. It can therefore be seen that, the class interval 110 – 119, consists of measurements greater than or equal to 109.5 cm and less than 119.5 cm. The numbers 109.5 and 119.5 are called the lower and upper boundaries of the class interval 110 – 120. The class boundaries of the other class intervals are given below:\n\n\n\nClass boundary and class limits\n\n\nNote:\nNotice that the lower class boundary of the ith class interval is the mean of the lower class limit of the class interval and the upper class limit of the (i-1)th class interval (i = 2, 3, 4, …). For example, in the table above the lower class boundaries of the second and the fourth class intervals are (110 + 109) /2 = 109.5 and (130 + 129)/2 = 129.5 respectively.\nIt can also be seen that the upper class boundary of the ith class interval is the mean of the upper class limit of the class interval and the lower class limit of the (i+1)th class interval (i = 1, 2, 3, …). Thus, in the above table the upper class boundary of the fourth class interval is (139 + 140)/2 = 139.5.\n\n\n\n\n\n\nNote\n\n\n\nFor continuous classes, class limits and boundaries are the same because there are no gaps between intervals. However, for discontinuous classes, boundaries are important as they close gaps and ensure every value belongs to one class.\n\n\nClass mark\nThe mid-point of a class interval is called the class mark or class mid-point of the class interval. It is the average of the upper and lower class limits of the class interval. It is also the average of the upper and lower class boundaries of the class interval. For example, in the table, the class mark of the third class interval was found as follows: class mark = (120+129)/2 = (119.5 + 129.5)/2 = 124.5.\nClass width\nFor continuous classes:\nThe class width is the difference between the upper and lower class limits of a class interval. Since the class limits and boundaries are the same for continuous classes, the width can also be determined by subtracting two consecutive lower or upper class limits.\nFor discontinuous classes:\nThe class width is the difference between the upper and lower class boundaries of a class interval. For discontinuous classes, class boundaries are used to account for gaps, and the width can also be determined by subtracting two consecutive lower or upper class boundaries.\nNote: \nIn the grouped frequency table above with discontinuous classes, the width of the second class interval is calculated as |110 - 119| = 9. It can be observed that the width is the same for all classes. This result can also be obtained by taking the numerical difference between the lower class boundaries of the second and third class intervals.\n\n\n1.14.2 Construction\nStep 1. Decide how many classes you wish to use\nStep 2. Determine the class width\nStep 3. Set up the individual class limits\nStep 4. Tally the items into the classes\nStep 5. Count the number of items in each class\nConsider the example where an agricultural student measured the lengths of leaves on an oak tree (to the nearest cm). Measurements on 38 leaves are as follows\n9, 16, 13, 7, 8, 4, 18, 10, 17, 18,\n9, 12, 5, 9, 9, 16, 1, 8, 17, 1, 10, 5, 9, 11, 15, 6, 14, 9, 1, 12,\n5, 16, 4, 16, 8, 15, 14, 17\nStep 1. Decide how many classes you wish to use\nH.A. Sturges provides a formula for determining the approximation number of classes. \\[\\mathbf{k = 1 + 3.322}.\\mathbf{\\log}\\mathbf{N}\\] Number of classes should be greater than calculated k.\nIn our example N=38, so k= (1+3.322)×log(38) = (1+3.322)×1.5797 = 6.24 = approx 7\nSo the approximated number of classes should be not less than 6.24 i.e.\\(\\ k^{'}\\) =7\nStep 2. Determine the class width\nGenerally, the class width should be the same size for all classes. C= | max − min|/ k. Class width \\(C^{'}\\)should be greater than calculated C. For this example, C = | 18− 1|/6.24 = 2.72, so approximately class width \\(C^{'} =\\) 3 (Note that k used here is the calculated value using Sturges formula not the approximated).\nStep 3. To set up the individual class limits, we need to find the lower limit only\n\\[L = min - \\frac{C^{'} \\times k^{'} - (max - min)}{2}\\]\nwhere, C and k here are final approximated class width and number of classes respectively in our example, \\(L = 1 - \\frac{(3 \\times 7) - (18 - 1)}{2}\\) = 1-2 = -1; since there is no negative values in data = 0. Final frequency table will be as shown in Table 1.1\n\n\n\nTable 1.1: Frequency distribution table\n\n\n\n\n\nClass\nFrequency\n\n\n\n\n0-3\n3\n\n\n3-6\n5\n\n\n6-9\n5\n\n\n9-12\n9\n\n\n12-15\n5\n\n\n15-18\n9\n\n\n18-21\n2\n\n\n\n\n\n\n\n\nEven though the student only measured in whole numbers, the data is continuous, so “4 cm” means the actual value could have been anywhere from 3.5 cm to 4.5 cm.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Basics of statistics</span>"
    ]
  },
  {
    "objectID": "intro.html#cumulative-frequency",
    "href": "intro.html#cumulative-frequency",
    "title": "1  Basics of statistics",
    "section": "1.15 Cumulative frequency",
    "text": "1.15 Cumulative frequency\nIn many situations, we are not interested in the number of observations in a given class interval, but in the number of observations which are less than (or greater than) a specified value. For example, in the above table, it can be seen that 3 leaves have length less than 3.5 cm and 9 leaves (i.e. 3 + 6) have length less than 6.5 cm. These frequencies are called cumulative frequencies. A table of such cumulative frequencies is called a cumulative frequency table or cumulative frequency distribution.\nCumulative frequency is defined as a running total of frequencies. Cumulative frequency can also defined as the sum of all previous frequencies up to the current point. Notice that the last cumulative frequency is equal to the sum of all the frequencies. Two types of cumulative frequencies are Less than Cumulative Frequency (LCF) and Greater than Cumulative Frequency(GCF). LCF is the number of values less than a specified value. GCF is the number of observations greater than a specified value.\nThe specified value for LCF in the case of grouped frequency distribution will be upper limits and for GCF will be the lower limits of the classes. LCF’s are obtained by adding frequencies in the successive classes and GCF are obtained by subtracting the successive class frequencies from the total frequency. See calculated LCF and GCF in Table 1.2 below.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Basics of statistics</span>"
    ]
  },
  {
    "objectID": "intro.html#relative-frequency",
    "href": "intro.html#relative-frequency",
    "title": "1  Basics of statistics",
    "section": "1.16 Relative frequency",
    "text": "1.16 Relative frequency\nIt is sometimes useful to know the proportion, rather than the number, of values falling within a particular class interval. We obtain this information by dividing the frequency of the particular class interval by the total number of observations. Relative frequency of a class is the frequency of class divided by total observations. Relative frequencies all add up to 1. See relative frequency calculated in Table 1.2 .\n\n\n\nTable 1.2: LCF,GCF and Relative frequency\n\n\n\n\n\nClass\nFrequency\nA\nB\nC\n\n\n\n\n0.5 - 3.5\n3\n3\n38\n0.079\n\n\n3.5 - 6.5\n6\n9\n35\n0.158\n\n\n6.5 - 9.5\n10\n19\n29\n0.263\n\n\n9.5 - 12.5\n5\n24\n19\n0.132\n\n\n12.5 - 15.5\n5\n29\n14\n0.132\n\n\n15.5 - 18.5\n9\n38\n9\n0.237\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHistorical Insights\n\n\n\n“The power of statistics: A wartime story”\nDuring World War II, military engineers faced a daunting challenge: how to protect their planes and crews better during missions. When the planes returned from combat, they were covered in bullet holes. Naturally, the engineers assumed they should add armor to the most damaged areas to make the planes more resilient. It seemed like common sense—but then came a surprising twist. Abraham Wald, a brilliant statistician, was called in to analyze the problem. He noticed something others had missed. The planes being studied were the ones that survived the missions. The missing data—the planes that never made it back, held the key to solving the puzzle. Wald reasoned that the areas with no damage on the returning planes were likely the most critical: hits in these areas would have brought the planes down, leaving no data behind.\nWald’s insight transformed how decisions were made, leading to smarter strategies that saved countless lives. This story is a powerful reminder of the importance of statistics not just in analyzing what’s visible but in understanding what’s missing. It’s a perfect example of how data, combined with critical thinking, can uncover hidden truths and drive impactful decisions. This revolutionary application of statistics eventually led to the birth of operations research, a field that continues to solve complex problems in diverse areas today.\n\n\n\n\n\n\n\n\nQuotes to Inspire\n\n\n\n“Data is the sword of the 21st century, those who wield it well, the Samurai.”\n- Jonathan Rosenberg, former Google SVP\n\n\n\n\n\n\n\nBall, Philip. 2004. Critical Mass. Farrar, Straus; Giroux.\n\n\nGoon, Gupta, A. M., and B Dasgupta. 1983. Fundamentals of Statistics. Vol. I. TheWorld Press.\n\n\nGupta, S. C., and V. K. Kapoor. 1997. Fundamentals of Mathematical Statistics. Sulthan Chand Publications, New Delhi.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Basics of statistics</span>"
    ]
  },
  {
    "objectID": "intro.html#footnotes",
    "href": "intro.html#footnotes",
    "title": "1  Basics of statistics",
    "section": "",
    "text": "actuary: A person who compiles and analyses statistics and uses them to calculate insurance risks and premiums.↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Basics of statistics</span>"
    ]
  },
  {
    "objectID": "agstatindia.html",
    "href": "agstatindia.html",
    "title": "2  Statistics on agriculture",
    "section": "",
    "text": "2.1 Compiling crop statistics\nThe agricultural sector accounts for approximately 18% of India’s GDP and employs nearly half of its workforce. Reliable and timely information is vital for planners and policymakers to develop effective agricultural policies and make informed decisions on procurement, storage, public distribution, imports, exports, and other related matters. As such, the collection and management of agricultural statistics hold significant importance.\nThis chapter provides an overview of the system for collecting agricultural statistics in India. While agriculture is a state subject, agricultural statistics fall under the concurrent list, resulting in a decentralised system. State Governments, through their State Agricultural Statistics Authorities (SASAs), play a central role in collecting and compiling agricultural statistics at the state level. At the national level, the Directorate of Economics and Statistics, under the Ministry of Agriculture and Farmers Welfare, is responsible for compiling the data. Other key agencies involved include the National Statistical Office (NSO) and the State Directorates of Economics and Statistics (DESs).\nCrop statistics comprise two key components: the area sown and the average yield.\nWhile area estimates are derived from land revenue systems, yield estimates are obtained through crop estimation surveys.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistics on agriculture</span>"
    ]
  },
  {
    "objectID": "agstatindia.html#compiling-crop-statistics",
    "href": "agstatindia.html#compiling-crop-statistics",
    "title": "2  Statistics on agriculture",
    "section": "",
    "text": "2.1.1 Area statistics\nThe system for collecting area statistics across states and Union Territories (UTs) in India can be broadly classified into three categories:\n\nStates with complete enumeration systems\nThese include states with land records or temporary settlement systems, covering 86% of the states (18 states and 3 UTs).\nStates using sample surveys\nThese are states with no land record system or permanently settled states, representing 9% of the states.\nStates with no developed system for area statistics\nIn these states, the collection is still based on conventional methods such as personal assessment, accounting for 5% of the states.\n\n\n\n2.1.2 Crop yield estimation\nCrop yields are estimated through Crop Cutting Experiments (CCE), which are conducted extensively across the country. The General Crop Estimation Survey (GCES) covers 65 crops, including 51 food crops and 14 non-food crops. Approximately 9 lakh CCEs are carried out annually in India to estimate the yield of key crops such as rice, maize, bajra, groundnut, and sugarcane. These experiments are conducted systematically to ensure accurate and reliable yield data for principal crops.\n\n\n\nSampling design under General Crop Estimation Survey\n\n\nFinal estimates of crop production are calculated using area figures obtained through complete enumeration and yield rates derived from crop-cutting experiments. These estimates become available only after the harvest. However, to support timely decision-making, the Government requires advance production estimates.\nThe Directorate of Economics and Statistics (DES), under the Ministry of Agriculture and Farmers Welfare, provides advance estimates of crop area and production for key food and non-food crops such as food grains, oilseeds, sugarcane, and fibres. These estimates are issued in four stages:\n\nFirst forecast: Mid-September\n\nSecond forecast: January\n\nThird forecast: Late March\n\nFourth forecast: Late May\n\nIn addition to these forecasts, Final Estimates of crop area and production are published in December. Subsequently, Fully Revised Estimates for all-India crop statistics are released in December of the following crop year.\nAdditionally, information on the structure and characteristics of the agricultural sector is gathered through the Agricultural Census.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistics on agriculture</span>"
    ]
  },
  {
    "objectID": "agstatindia.html#agricultural-census",
    "href": "agstatindia.html#agricultural-census",
    "title": "2  Statistics on agriculture",
    "section": "2.2 Agricultural census",
    "text": "2.2 Agricultural census\nThe Agricultural census is a comprehensive exercise conducted to gather and analyze data on the structure of the agricultural sector in India. It provides essential information about operational holdings, including their number, area, land use, cropping patterns, and input usage, down to the lowest geographical levels such as villages, tehsils (sub-districts), and districts. This census serves as a statistical framework for planning and conducting future agricultural surveys.\nInitiated in 1970-71, the Agricultural census is conducted every five years by the Department of Agriculture and Farmers Welfare in collaboration with State and Union Territory administrations. In states with land records, the number and area of operational holdings are collected through complete enumeration, while detailed data on the characteristics of operational holdings are gathered on a sample basis.\nTo date, eleven Agricultural censuses have been conducted, covering the reference years 1970-71, 1976-77, 1980-81, 1985-86, 1990-91, 1995-96, 2000-01, 2005-06, 2010-11, 2015-16 and 2020-21. The reference period for each census corresponds to the agricultural year, spanning from July to June.\nThe data derived from the Agricultural census plays a crucial role in policy formulation, resource allocation, and the overall development of the agricultural sector in India.\nAdditional data pertaining to various sectors can be obtained from the sources listed in the Appendix 1\n\n\n\n\n\n\nHistorical Insights\n\n\n\n“Mahalanobis and statistical planning in India”\nIn post-independence India, statistician Prasanta Chandra Mahalanobis played a key role in shaping the country’s economic planning. He developed statistical methods to guide the allocation of resources in India’s Five-Year Plans. Mahalanobis introduced the Mahalanobis Distance, a method for analyzing multi-dimensional data, and created the Input-Output Model to understand how different sectors of the economy interacted. His work, especially in the first Five-Year Plan (1951-1956), helped optimize resource use and industrial growth. Mahalanobis’ contributions laid the foundation for using statistics in economic policy-making, which continues to guide India’s development. He is known as the ‘Father of Indian statistics’.\n\n\n\n\n\n\n\n\nQuotes to Inspire\n\n\n\n“Statistics is the art of never having to say you’re certain.” – W. Edwards Deming",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistics on agriculture</span>"
    ]
  },
  {
    "objectID": "3graph.html",
    "href": "3graph.html",
    "title": "3  Graphical representation",
    "section": "",
    "text": "3.1 Histogram\nGraphs and diagrams play a vital role in statistics by transforming complex data into clear, visual formats that are easier to interpret and analyze. While frequency distributions in tabular form help organize raw data, graphical representations provide a more intuitive way to understand patterns, trends, and relationships within the data. By converting numbers into visual elements, graphs make it simpler to convey information effectively, making them indispensable tools in research, analysis, and communication. Depending on the nature of the data and the intended purpose, various types of graphs and diagrams can be employed to illustrate key insights. This chapter focuses on the fundamental graphs and charts used in statistics to visually represent data.\nA histogram is a graphical representation used to display the frequency distribution of continuous data. It consists of adjacent rectangles, where:\nUnlike bar charts, histograms have no gaps between the rectangles, emphasizing the continuity of the data. The height of each rectangle represents the frequency for equal-width classes. Histograms are effective tools for visualizing data distribution, identifying patterns, and highlighting skewness or outliers.\nExample 3.1 Table 4.1 displays the frequency distribution of plant heights for a sample of 50 plants. This data can be visualized effectively using a histogram, as shown in Figure 3.1.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Graphical representation</span>"
    ]
  },
  {
    "objectID": "3graph.html#histogram",
    "href": "3graph.html#histogram",
    "title": "3  Graphical representation",
    "section": "",
    "text": "The base of each rectangle lies along the horizontal axis, with the width determined by the class intervals.\n\nThe height of each rectangle is proportional to the frequency of the corresponding class.\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf the class intervals are of equal width, the height of each rectangle in a histogram is directly proportional to the class frequency. In such cases, the class frequencies can be used as the heights of the rectangles.\nHowever, when class intervals have varying widths, the height of each rectangle should be proportional to the frequency density, which is calculated as:\n\\[\n\\text{Frequency Density} = \\frac{\\text{Class Frequency}}{\\text{Class Width}}\n\\]\nIn these cases, the frequency density is plotted on the y-axis to ensure that the area of each rectangle accurately represents the frequency of the class. This approach maintains the correct visual representation of the data distribution regardless of the class interval widths.\n\n\n\n\n\n\nTable 3.1: Grouped frequency table of plant heights\n\n\n\n\n\nPlant height (cm)\nFrequency\n\n\n\n\n130 – 140\n3\n\n\n140 – 150\n6\n\n\n150 – 160\n17\n\n\n160 – 170\n13\n\n\n170 – 180\n8\n\n\n180 – 190\n3\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.1: Histogram",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Graphical representation</span>"
    ]
  },
  {
    "objectID": "3graph.html#ogive",
    "href": "3graph.html#ogive",
    "title": "3  Graphical representation",
    "section": "3.2 Ogive",
    "text": "3.2 Ogive\nOgive, also known as the cumulative frequency curve, is a graphical representation that plots cumulative frequencies against class boundaries. The points are typically connected using straight lines, forming a continuous curve. This visualization effectively illustrates the accumulation of frequencies, making it useful for understanding data distribution and determining percentiles or the median.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Graphical representation</span>"
    ]
  },
  {
    "objectID": "3graph.html#types-of-ogives",
    "href": "3graph.html#types-of-ogives",
    "title": "3  Graphical representation",
    "section": "3.3 Types of ogives",
    "text": "3.3 Types of ogives\nThere are two main types of cumulative frequency curves:\n1. Less than ogive\n2. Greater than ogive\nLess than ogive\nThe less than ogive, also known as the less than type cumulative frequency curve, is created by plotting the less than cumulative frequencies against the upper class boundaries. For example, consider the plant height data for 50 plants. By using the upper class limits and their cumulative frequencies, we can construct a smooth curve that provides insights into the data distribution. See Table 3.2, which is constructed from Table 4.1. The less than ogive, shown in Figure 3.2, is drawn using Table 3.2.\n\n\n\nTable 3.2: Upper limit and LCF of plant heights\n\n\n\n\n\nUpper limit\n140\n150\n160\n170\n180\n190\n\n\nLCF\n3\n9\n26\n39\n47\n50\n\n\n\n\n\n\nNote: LCF denotes less than cumulative frequency.\n\n\n\n\n\n\nFigure 3.2: Less than ogive\n\n\n\nGreater than ogive\nThe greater than ogive, also known as the greater than type cumulative frequency curve, is constructed by plotting the greater than cumulative frequencies against the lower class boundaries. In this case, instead of using the upper limits like in the “Less than ogive”, we use the lower class limits and their corresponding cumulative frequencies. This curve helps visualize the cumulative frequency distribution from the highest class down to the lowest, providing insights into the number of observations greater than a specific value. See Table 3.3 constructed from Table 4.1. The greater than ogive, shown in Figure 3.3, is drawn using Table 3.3.\n\n\n\nTable 3.3: Lower limit and GCF of plant heights\n\n\n\n\n\nLower limit\n130\n140\n150\n160\n170\n180\n\n\nGCF\n50\n47\n41\n24\n11\n3\n\n\n\n\n\n\nNote: GCF denotes greater than cumulative frequency.\n\n\n\n\n\n\nFigure 3.3: Greater than ogive\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIntersection of both less than and greater than ogives gives the median.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Graphical representation</span>"
    ]
  },
  {
    "objectID": "3graph.html#frequency-polygon",
    "href": "3graph.html#frequency-polygon",
    "title": "3  Graphical representation",
    "section": "3.4 Frequency polygon",
    "text": "3.4 Frequency polygon\nA grouped frequency table can also be represented by a frequency polygon, a special type of line graph. To construct it, plot the class frequencies against the corresponding class midpoints and connect successive points with straight lines. The frequency polygon can also be derived by joining the midpoints of a histogram. See Table 3.4, constructed from Table 4.1. The frequency polygon, created using Table 3.4, is shown in Figure 3.4. The relation between frequency polygon and histogram can be seen in Figure 3.5\n\n\n\nTable 3.4: Midpoints and frequencies\n\n\n\n\n\nClass midpoints\n135\n145\n155\n165\n175\n185\n\n\nFrequencies\n3\n6\n17\n13\n8\n3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.4: Frequency Polygon\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.5: Frequency Polygon and Histogram",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Graphical representation</span>"
    ]
  },
  {
    "objectID": "3graph.html#stem-and-leaf-plot",
    "href": "3graph.html#stem-and-leaf-plot",
    "title": "3  Graphical representation",
    "section": "3.5 Stem and leaf plot",
    "text": "3.5 Stem and leaf plot\nA stem and leaf plot is a graphical device useful for representing a relatively small set of data that takes numerical values. To construct a stem and leaf plot, we partition each measurement into two parts: the stem (the leading digits) and the leaf (the trailing digits). This method retains the exact value of each observation, unlike a frequency distribution. It also clearly shows the distribution of data within each group. A stem and leaf plot conveys similar information as a histogram, with the added benefit of retaining individual data points. It provides insights into the range, concentration of measurements, and symmetry of the data.\nConsider the example:\n12, 16, 21, 25, 29, 26, 30, 31, 37, 42, 45.\nThe stem and leaf plot for this data is shown Figure 3.6\n\n\n\n\n\n\nFigure 3.6: Stem and leaf plot\n\n\n\nA stem-and-leaf plot is not only useful for small data sets but can also effectively represent larger sets of numerical data. For instance, consider the monthly income of 50 employees in a company:\n19710, 24096, 23618, 26490, 25626, 24653, 24297, 23609, 19120, 25942, 23591, 27302, 29569, 25332, 29396, 20725, 25202, 20763, 30556, 21961, 22910, 21826, 21547, 21015, 19825, 24124, 22275, 26127, 24297, 20564, 26943, 26627, 23602, 24585, 25725, 24322, 23198, 25590, 23366, 23313, 22840, 25514, 24959, 23194, 21337, 26030, 27215, 19260, 27467, 29737.\nThe corresponding stem-and-leaf plot for this data, shown in Figure 3.7, lists the leaves in increasing order under their respective stems. The proper choice of stems is crucial as it organizes the data effectively, revealing patterns and distribution with clarity.\n\n\n\n\n\n\nFigure 3.7: Stem and leaf plot of 5 digit data\n\n\n\nConsider a different dataset representing the percentage of adults with a college degree in 20 cities.\n48.5, 53.2, 42.1, 65.4, 70.3, 38.7, 55.9, 47.3, 59.2, 33.5, 45.6, 62.8, 50.1, 41.3, 36.2, 43.7, 39.8, 66.4, 58.1, 31.2.\nThe stem and leaf plot for this data is shown in Figure 3.8. Here, the tens digit serves as the stem, and the decimal values form the leaves.\n\n\n\n\n\n\nFigure 3.8: Stem and leaf plot of decimal data",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Graphical representation</span>"
    ]
  },
  {
    "objectID": "3graph.html#bar-chart",
    "href": "3graph.html#bar-chart",
    "title": "3  Graphical representation",
    "section": "3.6 Bar chart",
    "text": "3.6 Bar chart\nA bar chart or bar graph is a diagram consisting of a series of horizontal or vertical bars of equal width. The bars represent various categories of the data. There are three types of bar charts, and these are simple bar charts, component bar charts and grouped bar charts.\nSimple bar chart\nIn a simple bar chart, the height (or length) of each bar is equal to the value of category in the y-axis it represents. Table 3.5 presents hypothetical data on coconut production across five districts of Kerala for a specific year. The data represented using barchart is shown in Figure 3.9\n\n\n\nTable 3.5: hypothetical data on coconut production\n\n\n\n\n\nDistrict\nProduction (million nuts)\n\n\n\n\nAlappuzha\n700\n\n\nKannur\n800\n\n\nThrissur\n980\n\n\nErnakulam\n1100\n\n\nWayanad\n1400\n\n\n\n\n\n\nComponent bar chart\nIn a component bar chart, the bar for each category is subdivided into component parts; hence its name. Component bar charts are therefore used to show the division of items into components. Component bar chart is also known as stacked barchart.\nFigure 3.10 shows the distribution of sales of agricultural produce from a farm in 1995, 1996 and 1997 and its corresponding component barchart in Figure 3.11.\nThe component bar chart shows the changes of each component over the years as well as the comparison of the total sales between different years.\nGrouped bar chart\nFigure 3.10 can also be represented using a grouped bar chart shown in Figure 3.12. For a grouped bar chart, each category within a group is represented by a bar with a distinct shade or color, allowing for clear comparisons of both within and across groups.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.9: Barchart\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.10: Sales data of agricultural produce\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.11: Component bar chart\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.12: Grouped bar chart",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Graphical representation</span>"
    ]
  },
  {
    "objectID": "3graph.html#histogram-versus-bar-chart",
    "href": "3graph.html#histogram-versus-bar-chart",
    "title": "3  Graphical representation",
    "section": "3.7 Histogram versus bar chart",
    "text": "3.7 Histogram versus bar chart\nTable 3.6 highlights the key differences between histograms and bar charts, two commonly used graphical tools in data visualization. While both employ bars to represent data, they serve distinct purposes and are applied to different types of data. Understanding these differences ensures the correct choice of graph for effectively presenting and interpreting data.\n\n\n\nTable 3.6: Comparison between histogram and bar chart\n\n\n\n\n\n\n\n\n\n\nFeature\nHistogram\nBar chart\n\n\n\n\nMeaning\nA graphical representation using bars to display the frequency of numerical data.\nA pictorial representation using bars to compare different categories of data.\n\n\nPurpose\nDepicts the distribution of continuous (non-discrete) data.\nCompares discrete (categorical) data.\n\n\nType of data\nQuantitative data.\nCategorical data.\n\n\nBar spacing\nBars are adjacent with no gaps.\nBars are separated by spaces.\n\n\nGrouping of elements\nData is grouped into ranges or intervals (bins).\nData is represented as individual categories.\n\n\nBar order\nBars cannot be reordered.\nBars can be reordered.\n\n\nBar width\nBar widths may vary.\nBar widths are uniform.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Graphical representation</span>"
    ]
  },
  {
    "objectID": "3graph.html#pie-charts",
    "href": "3graph.html#pie-charts",
    "title": "3  Graphical representation",
    "section": "3.8 Pie charts",
    "text": "3.8 Pie charts\nA pie chart is a circular graph divided into sectors, each sector representing a different value or category. The angle of each sector of a pie chart is proportional to the value of the part of the data it represents. The bar chart is more precise than the pie chart for visual comparison of categories with similar relative frequencies.\nSteps for constructing a pie chart\n\nFind the sum of the category values.\n\nCalculate the angle of the sector for each category, using the following formula.Angle of the sector for category A = \\(\\frac{\\text{value of category A}}{\\text{sum of category values}} \\times 360\\)\n\nConstruct a circle and mark the centre.\n\nUse a protractor to divide the circle into sectors, using the angles obtained in step 2.\n\nLabel each sector clearly.\n\nTable 3.7 presents hypothetical data on the production of different commodities in India during a particular year. Pie chart base on this data is shown in Figure 3.13\n\n\n\nTable 3.7: Hypothetical data on the production of different commodities\n\n\n\n\n\nCommodities\nProduction(tonnes)\nAngle\n\n\n\n\nWheat\n27000\n(27000/81000)×360= 120\n\n\nGrams\n22500\n100\n\n\nMaize\n13500\n60\n\n\nRice\n6750\n30\n\n\nSugar\n11250\n50\n\n\nTotal\n81000\n360\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.13: Piechart",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Graphical representation</span>"
    ]
  },
  {
    "objectID": "3graph.html#boxplot",
    "href": "3graph.html#boxplot",
    "title": "3  Graphical representation",
    "section": "3.9 Boxplot",
    "text": "3.9 Boxplot\nA boxplot, also known as a box-and-whisker plot, visually represents the five-number summary of a dataset: the minimum value, first quartile, median, third quartile, and maximum value. These key statistics provide insights into the dataset’s central tendency, spread, and potential outliers. Quartiles and the median, explained in detail in Section 5.5, are critical components of this summary.\nIn a boxplot, a rectangular box spans from the first quartile (Q1) to the third quartile (Q3), with a vertical line inside the box indicating the median. Whiskers extend from each end of the box to the dataset’s minimum and maximum values, providing a clear picture of the range and variability.\nFigure 3.14 below shows the parts of a box plot.\n\n\n\n\n\n\nFigure 3.14: Anatomy of box plot\n\n\n\nIn a boxplot, the minimum value is defined as \\(Q_{1}- 1.5\\times IQR\\), and the maximum value is \\(Q_{3}+ 1.5\\times IQR\\), where, \\(Q_{1}\\) and \\(Q_{3}\\) represent the first and third quartiles, and IQR stands for the interquartile range. Any data points falling below the minimum or above the maximum are considered outliers.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Graphical representation</span>"
    ]
  },
  {
    "objectID": "3graph.html#advanced-visualization",
    "href": "3graph.html#advanced-visualization",
    "title": "3  Graphical representation",
    "section": "3.10 Advanced visualization",
    "text": "3.10 Advanced visualization\nWhile this book focuses on basic plots and charts, significant advancements have been made in the field of data visualization. New types of graphs and charts have been developed to help in more effective representation and communication of data. Although a detailed discussion of these advanced graphs is beyond the scope of this book, we provide an overview of some common and recently developed types for reference. For more detailed information, you can explore resources such as The R Graph Gallery.\nIt is important to be aware of the wide variety of visualization tools available, as they can enhance your understanding of data and improve your ability to communicate insights clearly. From Figure 3.15 to Figure 3.26 you can see a few popular and advanced graph types widely used in modern data analysis.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.15: Box Plot\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.16: Violin Plot\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.17: Lollipop Plot\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.18: Dendrogram\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.19: Network Graph\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.20: Heat Map\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.21: Circular Bar Plot\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.22: Sankey Diagram\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.23: Ridgeline Plot\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.24: Chord Diagram\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.25: Density Plot\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.26: Stream Graph\n\n\n\n\n\n\n\n\n\n\n\n\nHistorical Insights\n\n\n\nThe cholera map: A life-saving visualization\nIn 1854, during a devastating cholera outbreak in London, physician John Snow transformed public health through the power of visualization. By mapping the locations of cholera cases and water pumps, he revealed a striking correlation: cases clustered around a single contaminated pump on Broad Street. Snow’s map not only pinpointed the outbreak’s source but also challenged prevailing beliefs that diseases spread through “miasma” (bad air). His groundbreaking work demonstrated how data visualization could provide critical insights, save lives, and shape public health policy, marking a pivotal moment in the history of epidemiology.\n\n\n\n\n\n\n\n\nQuotes to Inspire\n\n\n\n“Statistics is the grammer of science”\n- Karl Pearson",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Graphical representation</span>"
    ]
  },
  {
    "objectID": "central1.html",
    "href": "central1.html",
    "title": "4  Central tendency I",
    "section": "",
    "text": "4.1 Arithmetic mean\nIn the previous chapter, you explored how data can be summarized using tables and visually presented through graphs, enabling important features to be highlighted effectively. In this chapter, we shift our focus to statistical measures that describe the characteristics of a dataset.\nOne key aspect of data analysis is identifying a single value that represents the overall dataset. This is where measures of central tendency come into play. These are summary statistics that capture the center or typical value of a dataset, providing a concise numerical summary.\nThere are five commonly used averages: mean, median, and mode, collectively referred to as simple averages, and geometric mean and harmonic mean, known as special averages. In addition to these, there are positional averages, such as quartiles, deciles, and percentiles, which are determined based on the position of values within an ordered dataset. These measures provide insights into the central value and distribution of the data, making them fundamental tools for understanding and interpreting data patterns.\nIn this section, we will focus on simple averages, with a detailed discussion on positional averages and special averages following later.\nRequisites of a good measure of central tendency:\nThe main objectives of measure of central tendency:\nThis is what people usually intend when they say “average”. Arithmetic mean or simply the mean of a variable is defined as the sum of the observations divided by the number of observations. Mean of set of numbers \\(x_{1},x_{2},\\ldots,x_{n}\\) is denoted as \\(\\overline{x}\\). It is given by the formula\n\\[\\overline{x} = \\frac{x_{1} + x_{2} + \\ldots + x_{n}}{n}\\]\n\\[= \\frac{1}{n}\\sum_{i = 1}^{n}x_{i} \\tag{4.1}\\]\nExample 4.1: Find the mean of the numbers 2, 4, 7, 8, 11, 12\n\\[\\overline{x} = \\frac{2 + 4 + 7 + 8 + 11 + 12}{6} = \\frac{44}{6} = 7.33\\]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Central tendency I</span>"
    ]
  },
  {
    "objectID": "central1.html#arithmetic-mean",
    "href": "central1.html#arithmetic-mean",
    "title": "4  Central tendency I",
    "section": "",
    "text": "4.1.1 Mean of ungrouped frequency distribution\nDirect method\nIf the numbers \\(x_{1},x_{2},\\ldots,x_{n}\\) occur with frequencies \\(f_{1},f_{2},\\ldots,f_{n}\\) respectively then\n\\[\\overline{x} = \\frac{x_{1}f_{1} + x_{2}f_{2\\ \\ } + \\ldots + x_{n}f_{n}}{f_{1} + f_{2} + \\ldots + f_{n}}\\]\n\\[= \\frac{\\sum_{i = 1}^{n}{f_{i}x_{i}}}{\\sum_{i = 1}^{n}f_{i}} \\tag{4.2}\\]\nExample 4.2: Table 4.1 below shows the plant height of 50 plants. Find the mean plant height.\n\n\n\nTable 4.1: Plant height of 50 plants\n\n\n\n\n\nPlant height(cm)\n159\n160\n161\n162\n163\n\n\nFrequency\n3\n9\n23\n11\n4\n\n\n\n\n\n\nSolution 4.2\nThe calculation can be arranged as shown\n\n\n\nTable 4.2: Solution using direct method\n\n\n\n\n\n\n\n\n\n\nPlant height(x)\nFrequency(f)\nfx\n\n\n\n\n159\n3\n477\n\n\n160\n9\n1440\n\n\n161\n23\n3703\n\n\n162\n11\n1782\n\n\n163\n4\n652\n\n\n\n\\(\\sum_{i = 1}^{n}f_{i}\\)= 50\n\\(\\sum_{i = 1}^{n}{f_{i}x_{i}}\\)= 8054\n\n\n\n\n\n\n\\(\\overline{x} = \\frac{\\sum_{i = 1}^{n}{f_{i}x_{i}}}{\\sum_{i = 1}^{n}f_{i}} = \\frac{8054}{50}\\)= 161.08 cm\nAssumed mean method (Indirect method)\nThe amount of computation involved above can be reduced by using the following formula:\n\\[\\overline{x} = A + \\frac{\\sum_{i = 1}^{n}{f_{i}d_{i}}}{\\sum_{i = 1}^{n}f_{i}} \\tag{4.3}\\]\nwhere, \\(A\\) is the assumed mean, which can be any value in x. \\(d_{i} = x_{i} - A\\), \\(f_{i}\\) is the frequency of \\(x_{i}\\)\nConsider the Table 4.1\nlet \\(A\\) = 161; it can be any number in x\n\n\n\nTable 4.3: Solution using assumed mean method\n\n\n\n\n\n\n\n\n\n\n\nPlant height(x)\nFrequency(f)\n\\(d_i = x_i - 161\\)\n\\(f_i d_i\\)\n\n\n\n\n159\n3\n-2\n-6\n\n\n160\n9\n-1\n-9\n\n\n161\n23\n0\n0\n\n\n162\n11\n1\n11\n\n\n163\n4\n2\n8\n\n\n\n\\(\\sum_{i = 1}^{n}f_{i}= 50\\)\n\n\\(\\sum_{i = 1}^{n}{f_{i}d_{i}}\\)= 4\n\n\n\n\n\n\nusing Equation 4.3, \\(\\overline{x} = 161 + \\frac{4}{50}\\) = 161.08 cm\nThe mean plant height is 161.08 cm.\n\n\n4.1.2 Mean of grouped frequency distribution\nDirect method\nThe mean for grouped data is obtained from the following formula:\n\\[\\overline{x} = \\frac{\\sum_{i = 1}^{k}{f_{i}x_{i}}}{n} \\tag{4.4}\\]\nWhere \\(x_{i}\\) = the mid-point of ith class (ith class mark); \\(f_{i}\\)= the frequency of ith class; \\(n\\) = the sum of the frequencies or total frequencies in a sample. Note that i =1, 2,…, k, i.e. there are k classes.\nExample 4.3: Table 4.4 shows the distribution of the marks scored by 60 students in a Maths examination. Find the mean mark.\n\n\n\nTable 4.4: Distribution of the marks scored by 60 students\n\n\n\n\n\nMark (%)\n60-65\n65-70\n70-75\n75-80\n80-85\n\n\nNumber of students\n2\n15\n25\n14\n4\n\n\n\n\n\n\nSolution 4.3\nThe solution can be arranged as shown\n\n\n\nTable 4.5: Mean of grouped frequency table using direct method\n\n\n\n\n\n\n\n\n\n\n\nMarks\nClass mark(\\({x}_{i}\\))\nFrequency(\\({f}_{i}\\))\n\\({f}_{i}{x}_{i}\\)\n\n\n\n\n60-65\n62.5\n2\n125\n\n\n65-70\n67.5\n15\n1012.5\n\n\n70-75\n72.5\n25\n1812.5\n\n\n75-80\n77.5\n14\n1085\n\n\n80-85\n82.5\n4\n330\n\n\n\n\n\\(\\sum_{i = 1}^{n}f_{i}\\)= 60\n\\(\\sum_{i = 1}^{n}{f_{i}x_{i}}\\)= 4365\n\n\n\n\n\n\n\\(\\overline{x} = \\frac{\\sum_{i = 1}^{n}{f_{i}x_{i}}}{\\sum_{i = 1}^{n}f_{i}} = \\frac{4365}{60}\\)= 72.75\nThe mean mark is 72.75%.\nCoding method or Indirect method\nIf all the class intervals of a grouped frequency distribution have equal size \\(C\\) (class width); then the following formula can be used instead of direct method above. This formula makes calculations easier.\n\\[\\overline{x} = A + C\\frac{\\sum_{i = 1}^{n}{f_{i}u_{i}}}{\\sum_{i = 1}^{n}f_{i}} \\tag{4.5}\\]\nwhere, \\(A\\) is the class mark with the highest frequency, \\(u_{i} = \\frac{x_{i} - A}{C}\\), \\(f_{i}\\) is the frequency of \\(x_{i}\\), C is the class width.\nThis is called the “coding” method for computing the mean. It is a very short method and should always be used for finding the mean of a grouped frequency distribution with equal class widths.\nConsider the Table 4.4 of the Example 4.3.\n\\(A\\) = 72.5, class mark with highest frequency; \\(C\\) = 5\n\n\n\nTable 4.6: Solution using coding method\n\n\n\n\n\n\n\n\n\n\n\n\nMarks\nClass mark(\\({x}_{i}\\))\nFrequency(\\({f}_{i}\\))\n\\[{u}_{i}=\\frac{x_i- 72.5}{5}\\]\n\\[{f}_{i}{u}_{i}\\]\n\n\n\n\n60-65\n62.5\n2\n-2\n-4\n\n\n65-70\n67.5\n15\n-1\n-15\n\n\n70-75\n72.5\n25\n0\n0\n\n\n75-80\n77.5\n14\n1\n14\n\n\n80-85\n82.5\n4\n2\n8\n\n\n\n\n\\(\\sum_{i = 1}^{k}f_{i}\\)= 60\n\n\\(\\sum_{i = 1}^{k}{f_{i}u_{i}}\\)=3\n\n\n\n\n\n\nusing Equation 4.5, \\(\\overline{x} = 72.5 + 5 \\times \\left( \\frac{3}{60} \\right)\\)= 72.75\nThe mean mark is 72.75%.\nMerits and demerits of arithmetic mean\nMerits\n\nIt is rigidly defined.\nIt is easy to understand and easy to calculate.\nIf the number of items is sufficiently large, it is more accurate and more reliable.\nIt is a calculated value and is not based on its position in the series.\nIt is possible to calculate even if some of the details of the data are lacking.\nOf all averages, it is affected least by fluctuations of sampling.\nIt provides a good basis for comparison.\n\nDemerits\n\nIt cannot be obtained by inspection nor located through a frequency graph.\nIt cannot be in the study of qualitative phenomena not capable of numerical measurement i.e. Intelligence, beauty, honesty etc.\nIt can ignore any single item only at the risk of losing its accuracy.\nIt is affected very much by extreme values.\nIt cannot be calculated for open-end classes.\nIt may lead to fallacious conclusions, if the details of the data from which it is computed are not given.\n\n\n\n4.1.3 Weighted average\nA weighted average is a method of computing an average where some data points contribute more than others.\n\n\n\n\n\n\nNote\n\n\n\nIf all the weights of the data point are equal then the weighted average is the same as the simple mean.\n\n\nThe formula for the weighted average is: \\[\\text{Weighted average} = \\frac{\\sum w_{i}x_{i}}{\\sum w_{i}} \\tag{4.6}\\] where, \\(x_{i}\\) = individual data values; \\(w_{i}\\) = corresponding weights\nExample 4.4: If \\(x\\) = [10, 20, 30] and its corresponding weights are \\(w\\) = [1, 2, 3] then calculate its weighted average\nSolution 4.4\nUsing the Equation 4.6 \\[\\text{Weighted average} = \\frac{(1\\times10)+(2\\times20)+(3\\times30)}{1+2+3} = 23.33\\]\nExample 4.5: What is the weighted average of the first n natural numbers if the weights assigned to each number are equal to the numbers themselves?\nSolution 4.5\nUsing the Equation 4.6\n\\[\\text{Weighted average} = \\frac{(1\\times1) + (2\\times2) + ... + (n\\times n)}{1+2+...+n}\\] \\[=\\frac{1^2+2^2+...+n^2}{1+2+...+n}\\] \\[=\\frac{n(2n+1)(n+1)/6}{n(n+1)/2}\\] \\[=\\frac{2n+1}{3}\\]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Central tendency I</span>"
    ]
  },
  {
    "objectID": "central1.html#the-median",
    "href": "central1.html#the-median",
    "title": "4  Central tendency I",
    "section": "4.2 The median",
    "text": "4.2 The median\nThe median is the middle value in a set of data when the values are arranged in order from smallest to largest. If there is an odd number of values, the median is the one in the middle, with half of the values smaller and half larger. If there is an even number of values, the median is the average of the two middle values. The median is a positional measure, which means it depends on the order of the data, not the actual values. It helps to find the central point of the data, especially when there are extreme values or outliers that could affect the average.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Central tendency I</span>"
    ]
  },
  {
    "objectID": "central1.html#median-of-ungrouped-or-raw-data",
    "href": "central1.html#median-of-ungrouped-or-raw-data",
    "title": "4  Central tendency I",
    "section": "4.3 Median of ungrouped or raw data",
    "text": "4.3 Median of ungrouped or raw data\nArrange the given n observations \\(x_{1\\ },x_{2},\\ldots,x_{n}\\) in ascending order. If the number of values is odd, median is the middle value. If the number of values is even, median is the mean of middle two values.\nArrange data in ascending then use the following formula\nWhen n is odd, Median = Md =\\(\\left( \\frac{n + 1}{2} \\right)^{\\text{th}}\\)value\nWhen n is even, Median = Md =\\({\\text{Average of}\\left( \\frac{n}{2} \\right)^{th}\\text{and }\\left( \\frac{n}{2} + 1 \\right)}^{\\text{th}}\\)value\nExample 4.6: Find the median of each of the following sets of numbers.\n\\(a)\\) 12, 15, 22, 17, 20, 26, 22, 26, 12\n\\(b)\\) 4, 7, 9, 10, 5, 1, 3, 4, 12, 10\nSolution 4.6\n\\(a)\\) Arranging the data in an increasing order of magnitude, we obtain 12, 12, 15, 17, 20, 22, 22, 26, 26. Here, N = 9 is odd, and so, median = \\(\\left( \\frac{9 + 1}{2} \\right)^{\\text{th}}\\)= 5th ordered observation = 20.\n\n\n\n\n\n\nNote\n\n\n\nIf a number is repeated, we still count it the number of times it appears when we calculate the median.\n\n\n\\(b)\\) Arranging the data in an increasing order of magnitude, we obtain 1, 3, 4, 4, 5, 7, 9, 10, 10, 12. Here, N = 10 is an even number and so median = \\(\\frac{1}{2}\\){5th ordered observation + 6th ordered observation} = \\(\\frac{1}{2}\\left( 5 + 7 \\right) = 6\\).\n\n\n\n\n\n\nNote\n\n\n\nYou can see in each case, the median divides the distribution into two equal parts, with 50% of the observations greater than it and the other 50% less than it.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Central tendency I</span>"
    ]
  },
  {
    "objectID": "central1.html#median-of-ungrouped-frequency-distribution",
    "href": "central1.html#median-of-ungrouped-frequency-distribution",
    "title": "4  Central tendency I",
    "section": "4.4 Median of ungrouped frequency distribution",
    "text": "4.4 Median of ungrouped frequency distribution\nThe median is the middle number in an ordered set of data. In a frequency table, the observations are already arranged in an ascending order. We can obtain the median by looking for the value in the middle position.\nOdd number of observations\nWhen the number of observations (n) is odd, then the median is the value at the \\(\\left( \\frac{n + 1}{2} \\right)^{\\text{th}}\\) positional value. For that we use less than cumulative frequency.\nExample 4.7: The Table 4.7 shows the frequency of the score obtained in a mathematics quiz. Find the median score.\n\n\n\nTable 4.7: Score obtained in a mathematics quiz\n\n\n\n\n\nScore\n0\n1\n2\n3\n4\n\n\nFrequency\n3\n4\n7\n6\n3\n\n\n\n\n\n\nSolution 4.7\nTotal frequency = 3 + 4 + 7 + 6 + 3 = 23 (odd number). Since the number of scores is odd, the median is at \\(\\left( \\frac{23 + 1}{2} \\right)^{\\text{th}} =\\) 12th position.\n\n\n\nTable 4.8: Less than cumulative frequency of the scores in mathematics quiz\n\n\n\n\n\nScore\n0\n1\n2\n3\n4\n\n\nFrequency\n3\n4\n7\n6\n3\n\n\nLess than cumulative frequency\n3\n7\n14\n20\n23\n\n\n\n\n\n\nTo find out the 12th position, we use less than cumulative frequencies in Table 4.8 which helps us track how many values are less than or equal to each score. In the table, the less than cumulative frequency for the score 0 is 3 (meaning the first 3 values are 0), for the score 1 is 7 (meaning the first 7 values are 0 and 1), and for the score 2 is 14 (meaning the first 14 values are 0, 1, and 2). If we list the data in order, it would look like this: 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2. (no need to list, this is just for reader’s understanding, less than cumulative frequency is enough).\nNow, we need to find where the 12th value falls. The 12th value is between the 7th and 14th values, so based on less than cumulative frequency it is clear that 12th value is 2. So the median is 2.\nEven number of observations\nWhen the number of observations is even, then the median is the average of \\({\\left( \\frac{n}{2} \\right)^{th}\\text{and}\\left( \\frac{n}{2} + 1 \\right)}^{\\text{th}}\\) position values.\nExample 4.8: The Table 4.9 is a frequency table of the marks obtained in a competition. Find the median score.\n\n\n\nTable 4.9: Distribution of marks obtained in a competition.\n\n\n\n\n\nMark\n0\n1\n2\n3\n4\n\n\nFrequency\n11\n9\n5\n10\n15\n\n\n\n\n\n\nSolution 4.8\nTotal frequency = 11 + 9 + 5 + 10 + 15 = 50 (even number). Since the number of scores is even, the median is at the average of the values in \\({\\left( \\frac{n}{2} \\right)^{th} = 25\\ and\\ \\left( \\frac{n}{2} + 1 \\right)}^{\\text{th}} = 26\\) positions. To find out the 25th position and 26th position, we add up the frequencies as shown:\n\n\n\nTable 4.10: Less than cumulative frequency of marks obtained.\n\n\n\n\n\nMark\n0\n1\n2\n3\n4\n\n\nFrequency\n11\n9\n5\n10\n15\n\n\nLess than cumulative frequency\n11\n20\n25\n35\n50\n\n\n\n\n\n\nThe mark at the 25th position is 2 and the mark at the 26th position is 3. The median is the average of the scores at 25th and 26th positions = \\(\\frac{2 + 3}{2} = 2.5\\)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Central tendency I</span>"
    ]
  },
  {
    "objectID": "central1.html#median-of-grouped-frequency-distribution",
    "href": "central1.html#median-of-grouped-frequency-distribution",
    "title": "4  Central tendency I",
    "section": "4.5 Median of grouped frequency distribution",
    "text": "4.5 Median of grouped frequency distribution\nThe exact value of the median of a grouped data cannot be obtained because the actual values of a grouped data are not known. For a grouped frequency distribution, the median is in the class interval which contains the \\(\\left( \\frac{N}{2} \\right)^{\\text{th}}\\)ordered observation, where \\(N\\) is the total number of observations. This class interval is called the median class. The median of a grouped frequency distribution can be estimated by either of the following two methods:\nLinear interpolation method\nThe median of a grouped frequency distribution can be estimated by linear interpolation. We assume that the observations are evenly spread through the median class. The median can then be computed by using the following formula:\n\\[Median = L + \\left( \\frac{\\frac{1}{2}N - F}{f_{m}} \\right)C \\tag{4.7}\\]\nwhere, \\(N\\) = total number of observations, \\(L\\) = lower limit of the median class, \\(F\\) = sum of all frequencies below L(cumulative frequency), \\(f_{m}\\) = frequency of the median class, \\(C\\) = class width of the median class.\nEstimation from cumulative frequency curve\nThe median of a grouped frequency distribution can be estimated from a cumulative frequency curve. A horizontal line is drawn from the point \\(\\frac{\\text{N}}{2}\\) on the vertical axis to meet the cumulative frequency curve. From the point of intersection, a vertical line is dropped to the horizontal axis. The value on the horizontal axis is equal to the median.\n\n\n\nMedian from a cumulative frequency curve\n\n\nExample 4.9 Table 4.11 below gives the distribution of the heights of 60 students in a senior high school. Find the median height of the students\n\n\n\nTable 4.11: Distribution of heights of 60 students\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHeight(cm)\n145-150\n150-155\n155-160\n160-165\n165-170\n170-175\n\n\nNumber of students\n3\n9\n16\n18\n10\n4\n\n\n\n\n\n\nSolution 4.9\n(i) Linear interpolation method\n\\(N\\) = 60 (Sum of frequencies)\nMedian class= class interval which contains the \\(\\left( \\frac{N}{2} \\right)^{\\text{th}}\\)ordered observation; here \\(\\left( \\frac{60}{2} \\right)^{\\text{th}} =\\) 30th observation. Before the class 160-165 there are 3+9+16 = 28 observations so 30th observation will be in the class 160-165, therefore it is the median class.\n\\(L\\) = lower limit of the median class = 160\n\\(F\\) = sum of all frequencies below 160(cumulative frequency) = 16+9+3 = 28\n\\(f_{m}\\) = frequency of the median class = 18\n\\(C\\) = class width of the median class = 5\nusing Equation 4.7, \\(median = 160 + \\left( \\frac{\\frac{1}{2}60 - 28}{18} \\right)5\\) = 160.56\n(ii) From a cumulative frequency curve\n\n\n\n\n\n\nFigure 4.1: Median from a cumulative frequency curve Example 4.7\n\n\n\nMerits and demerits of median\nMerits\n\nMedian is not influenced by extreme values because it is a positional average.\nMedian can be calculated in case of distribution with open-end intervals.\nMedian can be located even if the data are incomplete.\n\nDemerits\n\nA slight change in the series may bring drastic change in median value.\nIn case of even number of items or continuous series, median is an estimated value other than any value in the series.\nIt is not suitable for further mathematical treatment except its use in calculating mean deviation.\nIt does not take into account all the observations.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Central tendency I</span>"
    ]
  },
  {
    "objectID": "central1.html#the-mode",
    "href": "central1.html#the-mode",
    "title": "4  Central tendency I",
    "section": "4.6 The mode",
    "text": "4.6 The mode\nThe mode of a set of data is the value which occurs with the greatest frequency. The mode is therefore the most frequently occuring value in a dataset. The mode is an important measure in case of qualitative data. The mode can be used to describe both quantitative and qualitative data.\n\n4.6.1 Mode of ungrouped or raw data\nFor ungrouped data or a series of individual observations, mode is often found by mere inspection.\nExample 4.10:\n\\(a)\\) The modes of 1, 2, 2, 2, 3 is 2.\n\\(b)\\) The modes of 2, 3, 4, 4, 5, 5 are 4 and 5.\n\\(c)\\) The mode does not exist when every observation has the same frequency. For example, the following sets of data have no modes: (i) 3, 6, 8, 9; (ii) 4, 4, 4, 7, 7, 7, 9, 9, 9.\n\n\n\n\n\n\nNote\n\n\n\nIt can be seen that the mode of a distribution may not exist, and even if it exists, it may not be unique. Distributions with a single mode are referred to as unimodal. Distributions with two modes are referred to as bimodal. Distributions may have several modes, in which case they are referred to as multimodal.\n\n\nExample 4.11: 20 patients selected at random had their blood groups determined. The results are given in the Table 4.12\n\n\n\nTable 4.12: Blood group of 20 patients\n\n\n\n\n\nBlood group\nA\nAB\nB\nO\n\n\nNo. of patients\n2\n4\n6\n8\n\n\n\n\n\n\nSolution 4.11\nThe blood group with the highest frequency is O. The mode of the data is therefore blood group O. We can say that most of the patients selected have blood group O. Notice that the mean and the median cannot be applied to the data. This is because the variable “blood group” cannot take numerical values. However, it can be seen that the mode can be used to describe both quantitative and qualitative data.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Central tendency I</span>"
    ]
  },
  {
    "objectID": "central1.html#mode-of-grouped-data",
    "href": "central1.html#mode-of-grouped-data",
    "title": "4  Central tendency I",
    "section": "4.7 Mode of grouped data",
    "text": "4.7 Mode of grouped data\nMode of a grouped frequency distribution can be found out using the formula below.\n\\[mode = L + \\left( \\frac{f_{m}-f_{p}}{2f_{m}-f_{p} - f_{s}} \\right)C \\tag{4.8}\\]\nLocate the highest frequency the class corresponding to that frequency is called the modal class.\nwhere, \\(L\\) = lower limit of the modal class; \\(f_{m}\\) = the frequency of modal class; \\(f_{p}\\)= the frequency of the class preceding the modal class; \\(f_{s}\\)= the frequency of the class succeeding the modal class and \\(C\\) = class interval\nExample 4.12: For the frequency distribution of weights of sorghum ear-heads given in Table 4.13 below. Calculate the mode.\n\n\n\nTable 4.13: Frequency distribution of weights of sorghum ear heads\n\n\n\n\n\nWeights of ear heads (g)\nNo of ear heads (f)\n\n\n\n\n60-80\n22\n\n\n80-100\n38\n\n\n100-120\n45\n\n\n120-140\n35\n\n\n140-160\n20\n\n\n\n\n\n\nSolution 4.12\nModal class is 100-120, since it is the class with highest frequency.\nUsing Equation 4.8 mode is calculated as below.\n\\(mode = 100 + \\left( \\frac{45-35}{90-38-35} \\right)20 =\\) 111.76\nMode using histogram\nConsider the figure below. The modal class is the class interval which corresponds to rectangle \\(\\text{ABCD}\\). An estimate of the mode of the distribution is the abscissa of the point of intersection of the line segments \\(\\overline{\\text{AE}}\\) and \\(\\overline{\\text{BF}}\\) in the figure.\n\n\n\n\n\n\nFigure 4.2: Median from a cumulative frequency curve for Example 4.10\n\n\n\nMerits and demerits of mode\nMerits\n\nIt is readily comprehensible and easy to compute. In some case it can be computed merely by inspection.\nIt is not affected by extreme values. It can be obtained even if the extreme values are not known.\nMode can be determined in distributions with open classes.\nMode can be located on the graph also.\nMode can be used to describe both quantitative and qualitative data.\n\nDemerits\n\nThe mode is not unique i.e. there can be more than one mode for a given set of data.\nThe mode of a set of data may not exist.\nIt is not based upon all the observation.\n\n\n\n\n\n\n\nHistorical Insights\n\n\n\nAncient wall measuring with the mode!\nBack in the 5th century BCE, the Athenians used a clever “statistical hack” to plan their siege of Platea. Soldiers counted bricks in an unplastered section of the wall multiple times, and the most frequent count (what we now call the mode) was taken as the best estimate. They then multiplied this by the height of a brick to calculate the wall’s height and build ladders tall enough to scale it. Problem-solving with statistics!\nAstronomy and the mean\nAlthough the Greeks knew the concept of the arithmetic mean, it wasn’t generalized for multiple values until the 16th century. Simon Stevin’s invention of the decimal system in 1585 made it much easier to calculate. Astronomer Tycho Brahe was one of the first to use the mean, reducing errors in his estimates of celestial body locations.\nNavigation and the median\nThe concept of the median first appeared in 1599 in Edward Wright’s book on navigation, Certaine errors in navigation. Wright used it to determine the most likely value in a series of compass readings. Later, in 1669, Christiaan Huygens noticed the difference between the mean and the median while working with Graunt’s tables. It’s amazing how these early navigators and mathematicians paved the way for the stats we use today.\n\n\n\n\n\n\n\n\nQuotes to Inspire\n\n\n\n“If the statistics are boring, you’ve got the wrong numbers”:- Edward R. Tufte",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Central tendency I</span>"
    ]
  },
  {
    "objectID": "central2.html",
    "href": "central2.html",
    "title": "5  Central tendency II",
    "section": "",
    "text": "5.1 Geometric mean\nWhile simple averages like mean, median, and mode are widely used to summarize data, certain situations call for more specialized measures to capture the essence of a dataset. Special averages, including the geometric mean and harmonic mean, are tailored for specific contexts where the nature of the data or the relationships between data points require a different approach.\nThe geometric mean (GM) is a specialized measure of central tendency, particularly suited for datasets involving growth rates, ratios, or percentages, such as population growth, investment returns, or interest rates. Unlike the arithmetic mean, which calculates the average by summing values, the geometric mean finds the average by multiplying values and then taking the root (typically the nth root for n values).\nThis approach captures the compounding effects present in the data, making the geometric mean an essential tool for accurately summarizing proportional changes or rates over time. Its utility lies in providing a more representative measure for datasets where changes are multiplicative rather than additive.\nThe geometric mean of a series containing n observations is the nth root of the product of the values. If \\(x_{1},x_{2},\\ldots, x_{n}\\) are observations then\n\\[GM = \\sqrt[n]{\\prod_{i=1}^{n}x_{i}} \\tag{5.1}\\]\nwhere, \\(\\prod_{i=1}^{n} x_i\\) means the product of \\(x_{1},x_{2},\\ldots, x_{n}\\)\n\\[= \\left( x_1 x_2 \\cdots x_n \\right)^{\\frac{1}{n}}\\]\n\\[\\log \\text{GM} = \\frac{1}{n} \\log \\left( x_1 x_2 \\cdots x_n \\right)\\]\n\\[= \\frac{1}{n} \\left( \\log x_1 + \\log x_2 + \\cdots + \\log x_n \\right)\\]\n\\[= \\frac{\\sum_{i=1}^{n} \\log x_i}{n} \\tag{5.2}\\]\n\\[\\text{GM} = \\text{Antilog} \\left( \\frac{\\sum_{i=1}^{n} \\log x_i}{n} \\right) \\tag{5.3}\\]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Central tendency II</span>"
    ]
  },
  {
    "objectID": "central2.html#geometric-mean",
    "href": "central2.html#geometric-mean",
    "title": "5  Central tendency II",
    "section": "",
    "text": "5.1.1 Geometric mean for frequency table\n\\[\\text{GM} = \\text{Antilog} \\left( \\frac{\\sum_{i=1}^{k} f_i \\log x_i}{n} \\right) \\tag{5.4}\\]\nwhere, \\(x_{i}\\) is the ith value in the dataset and for a grouped frequency table \\(x_{i}\\) will be the midpoint of the ith class interval calculated as the average of upper and lower limit, \\(f_{i}\\) is the frequency of the ith value or class , k is the number of classes.\nExample 5.1: If the weight of sorghum ear heads are 45, 60, 48, 100, 65 gms. Find the geometric mean?\n\n\n\nTable 5.1: Log values of sorghum ear head weights\n\n\n\n\n\nWeight of ear head (x)\nlog(x)\n\n\n\n\n45\n1.653\n\n\n60\n1.778\n\n\n48\n1.681\n\n\n100\n2.000\n\n\n65\n1.813\n\n\nTotal\n8.926\n\n\n\n\n\n\nSolution 5.1\nHere n =5\nusing Equation 5.3\n\\[=\\text{Antilog}\\left( \\frac{8.926}{5} \\right) \\]\n\\[ =\\text{Antilog}(1.785) = 60.95\\] note: here \\(\\text{Antilog}\\left( x \\right) = 10^{x}\\) i.e. \\[\\text{Antilog}\\left( 1.785 \\right) = \\ 10^{1.785} = 60.95\\]\nExample 5.2: Geometric mean of a frequency distribution\n\n\n\nTable 5.2: Frequency distribution and log for GM calculation\n\n\n\n\n\nWeight of ear head (x)\nFrequency(f)\nlog(x)\nf.log(x)\n\n\n\n\n45\n5\n1.653\n8.266\n\n\n60\n4\n1.778\n7.113\n\n\n48\n6\n1.681\n10.087\n\n\n100\n8\n2.000\n16.000\n\n\n65\n9\n1.813\n16.316\n\n\nTotal\n32\n\n57.782\n\n\n\n\n\n\nSolution 5.2\nHere n =32\nusing Equation 5.4\n\\[{\\sum_{i = 1}^{k}{{f_{i}\\log}x_{i}} = 57.782\n}\\]\n\\[{\\text{GM} = \\ Antilog\\left( \\frac{57.782}{32} \\right)  }\\]\n\\[{= Antilog\\left( 1.8056 \\right)= 10^{1.8056} = 63.92}\\]\nExample 5.3: Geometric mean of a grouped frequency distribution\n\n\n\nTable 5.3: Geometric mean calculation for grouped frequency table\n\n\n\n\n\nClass\nMid value (x)\nFrequency(f)\nlog(x)\nf.log(x)\n\n\n\n\n60-80\n70\n5\n1.845\n9.225\n\n\n80-100\n90\n4\n1.954\n7.817\n\n\n100-120\n110\n6\n2.041\n12.248\n\n\n120-140\n130\n8\n2.114\n16.912\n\n\n140-160\n150\n9\n2.176\n19.585\n\n\n\nTotal\n32\n\n65.787\n\n\n\n\n\n\nSolution 5.3\nHere n = 32\nusing Equation 5.4\n\\[{\\sum_{i = 1}^{k}{{f_{i}\\log}x_{i}} = 65.787}\\] \\[{\\text{GM} = \\ Antilog\\left( \\frac{65.787}{32} \\right)}\\] \\[{= Antilog\\left( 2.0558 \\right) = 10^{2.0558} = 113.71}\\]\nMerits and demerits of geometric mean\nMerits\n\nIt is rigidly defined.\nIt is based on all the observations of the series.\nIt is suitable for measuring the relative changes.\nIt gives more weights to the small values and less weight to the large values.\nIt is used in averaging the ratios, percentages and in determining the rate gradual increase and decrease.\nIt is capable of further algebraic treatment.\n\nDemerits\n\nIt is not easy to understand.\nIt is difficult to calculate.\nIt cannot be calculated, if the number of negative values is odd.\nIt cannot be calculated, if any value of a series is zero.\nAt times it gives a value which may not be found in the series or impractical.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Central tendency II</span>"
    ]
  },
  {
    "objectID": "central2.html#harmonic-mean",
    "href": "central2.html#harmonic-mean",
    "title": "5  Central tendency II",
    "section": "5.2 Harmonic mean",
    "text": "5.2 Harmonic mean\nHarmonic means are often used in averaging things like rates (e.g. the average travel speed given duration of several trips). Harmonic mean (HM) of a set of observations is defined as the reciprocal of the arithmetic average of the reciprocal of the given value.\n\n\n\n\n\n\nNote\n\n\n\nHarmonic mean can be easily remembered as “reciprocal of the mean of the reciprocals”.\n\n\nIf \\(x_{1},\\ x_{2},\\ldots,\\ x_{n}\\) are n observations then\n\\[\\mathbf{\\text{H.M}} = \\frac{n}{\\sum_{i = 1}^{n}\\frac{1}{x_{i}}} \\tag{5.5}\\]\nSteps in calculating Harmonic Mean (H.M)\n\nCalculate the reciprocal (1/value) for every value.\nFind the average of those reciprocals (just add them and divide by how many there are).\nThen do the reciprocal of that average (=1/average).\n\nExample 5.4: From the given data 5, 10, 17, 24, 30. calculate H.M\nSolution 5.4\nHere n = 5\n\n\n\nTable 5.4: Inverse of numbers to calculate harmonic mean\n\n\n\n\n\nx\n1/x\n\n\n\n\n5\n0.2\n\n\n10\n0.1\n\n\n17\n0.059\n\n\n24\n0.042\n\n\n30\n0.033\n\n\nTotal\n0.434\n\n\n\n\n\n\nusing Equation 5.5\n\\[\\mathbf{\\text{H.M}} = \\frac{5}{0.434} = 11.525\\]\n\n5.2.1 Harmonic mean for frequency table\n\\[\\mathbf{\\text{H.M}} = \\frac{n}{\\sum_{i = 1}^{k}{f_{i}\\frac{1}{x_{i}}}} \\tag{5.6}\\]\nwhere, \\(x_{i}\\) is the ith value in the dataset and for a grouped frequency table \\(x_{i}\\) will be the midpoint of the ith class interval calculated as the average of upper and lower limit, \\(f_{i}\\) is the frequency of the ith value or class , k is the number of classes.\nExample 5.5: For the given data calculate the harmonic mean.\n\n\n\nTable 5.5: Model data for harmonic mean calculation\n\n\n\n\n\nx\n20\n21\n22\n23\n24\n25\n\n\nfrequency (f)\n4\n2\n7\n1\n3\n1\n\n\n\n\n\n\nSolution 5.5\n\n\n\nTable 5.6: Calculation of harmonic mean from frequency table\n\n\n\n\n\nx\nf\n1/x\nf . (1/x)\n\n\n\n\n20\n4\n0.050\n0.200\n\n\n21\n2\n0.048\n0.095\n\n\n22\n7\n0.045\n0.318\n\n\n23\n1\n0.043\n0.043\n\n\n24\n3\n0.042\n0.125\n\n\n25\n1\n0.040\n0.04\n\n\n\n18\n\n0.822\n\n\n\n\n\n\nHere n =18\nusing Equation 5.6\n\\[\\mathbf{\\text{H.M}} = \\frac{18}{0.822} = 21.90\\]\nExample 5.6: Cistern Problem-Two pipes are used to fill a cistern. Pipe A can fill the cistern in 3 hours. Pipe B can fill the cistern in 5 hours. If both pipes are opened at the same time, how long will it take to fill the cistern completely?\nSolution 5.6\nTo solve the cistern problem, we first calculate the rate at which each pipe fills the cistern. Pipe A fills the cistern in 3 hours, so its rate is \\(\\frac{1}{3}\\) of the cistern per hour. Pipe B fills the cistern in 5 hours, so its rate is \\(\\frac{1}{5}\\) per hour. To find the combined rate, we add these rates together; \\(\\frac{1}{3} + \\frac{1}{5} = \\frac{8}{15}\\). This means the two pipes together fill \\(\\frac{8}{15}\\) of the cistern each hour. To find the total time, we take the reciprocal of the combined rate i.e. \\(\\text{Total time} = \\frac{1}{\\frac{8}{15}} = \\frac{15}{8} = 1.875\\). To convert 1.875 hours into hours and minutes, we first separate the whole number from the decimal part. 1.875 hours consists of 1 hour (the whole number) and 0.875 hours (the decimal part). Next, we convert the decimal part into minutes. Since 1 hour is equal to 60 minutes, we multiply 0.875 by 60 i.e. \\(0.875 \\times 60 = 52.5 \\, \\text{minutes}\\). Rounding 52.5 minutes gives approximately 53 minutes. Thus, 1.875 hours is equivalent to 1 hour and 53 minutes.\nThe problem can be easily solved using the harmonic mean, the harmonic mean of two pipes is\n\\[\n\\mathbf{\\text{H.M}}=\\frac{2}{\\frac{1}{3} +\\frac{1}{5}}\n\\]\n\\[= \\frac{2 \\cdot 3 \\cdot 5}{3 + 5} = \\frac{30}{8} = 3.75 \\, \\text{hours}\\]\nthe harmonic mean of the pipes is 3.75 hours, and since both pipes are working together, the total time is half of that, which confirms the answer of 1 hour and 53 minutes.\nExample 5.7: A car travels a certain distance from City A to City B at a speed of 60 km/h, and returns the same distance from City B to City A at a speed of 90 km/h. What is the average speed for the entire trip?\nSolution 5.7\nTo find the average speed when traveling the same distance at two different speeds, we use the harmonic mean. The harmonic mean for the speed is \\(\\frac{2 \\cdot S_1 \\cdot S_2}{S_1 + S_2}\\), where \\(S_1 = 60 \\, \\text{km/h}\\) is the speed from City A to City B, \\(S_2 = 90 \\, \\text{km/h}\\) is the speed from City B to City A.\nIn this case, we are calculating the average speed over the entire round trip. The harmonic mean is used because it accounts for the fact that traveling at different speeds over the same distance results in an overall average speed that is closer to the lower of the two speeds, rather than simply averaging the two speeds.\nNow, applying the harmonic mean formula:\n\\(S_{\\text{avg}} = \\frac{2 \\cdot 60 \\cdot 90}{60 + 90} = \\frac{10800}{150} = 72 \\, \\text{km/h}\\)\nSo, the average speed for the entire trip is 72 km/h.\nMerits and demerits of harmonic mean\nMerits\n\nIt is rigidly defined.\nIt is defined on all observations.\nIt is amenable to further algebraic treatment.\nIt is the most suitable average when it is desired to give greater weight to smaller and less weight to the larger ones.\n\nDemerits\n\nIt is not easily understood.\nIt is difficult to compute.\nIt is only a summary figure and may not be the actual item in the series.\nIt gives greater importance to small items and is therefore, useful only when small items have to be given greater weightage.\nIt is rarely used in grouped data.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Central tendency II</span>"
    ]
  },
  {
    "objectID": "central2.html#am-gm-or-hm",
    "href": "central2.html#am-gm-or-hm",
    "title": "5  Central tendency II",
    "section": "5.3 AM, GM or HM ?",
    "text": "5.3 AM, GM or HM ?\nThe arithmetic mean (AM), geometric mean (GM), and harmonic mean (HM) are termed as Pythagorean means. The Pythagorean means refer to three specific types of means that were known to the ancient Greek mathematicians, particularly to Pythagoras and his followers. Choosing the right average depends on what you’re measuring and how the data behaves. Let’s break it down step by step in a simple way:\nArithmetic Mean (AM)\nThe arithmetic mean is the most common type of average. You use it when the values in your data add together in a straightforward way. This is suitable for quantities like:\n- Heights or weights\n- Lengths or distances\n- Marks in exams\nFor example, if you want to find the average height of students in a class, you add up all the heights and divide by the number of students. The arithmetic mean gives a meaningful average because height or weight adds linearly.\nHarmonic Mean (HM)\nThe harmonic mean is useful when you are working with rates, ratios, or situations where quantities add up as reciprocals. Some examples include:\n- Speeds (distance per unit time)\n- Capacitors in a series circuit\n- Rates like fuel efficiency or cost per unit\nFor instance, imagine you are driving the same distance at different speeds. If you want to find the average speed for the entire trip, the harmonic mean is the best choice. This is because speeds relate inversely to time when you go faster, you take less time, and vice versa.\nGeometric Mean (GM)\nThe geometric mean is the right choice when your data involves multiplication or compounding, such as:\n- Growth rates (like population growth or interest rates)\n- Percentages (like inflation rates)\n- Ratios\nFor example, if you have annual interest rates for 10 years and want to find a single rate that represents the same total growth over that period, the geometric mean gives you the answer. It works by multiplying the rates and taking the root, which accounts for the compounding effect.\nHere’s a simple example to understand how the geometric mean works. Suppose you invest Rs.100, and your investment changes over three years as follows: in the first year, it grows by 10%, represented by a growth factor of 1.10; in the second year, it grows by 20%, represented by 1.20; and in the third year, it drops by 10%, represented by 0.90. To find a single rate that would give the same overall growth over the three years, you multiply the growth factors: \\(1.10 \\times 1.20 \\times 0.90 = 1.188\\). Then, take the cube root (since there are three years): \\(\\sqrt[3]{1.188} \\approx 1.059\\). This gives a geometric mean rate of approximately 1.059, or 5.9% per year. In other words, if your Rs. 100 investment grew steadily at a rate of 5.9% each year, it would result in the same total growth over three years, leaving you with about Rs. 118.80 at the end.\nKey Takeaways\n1. Use Arithmetic Mean when values combine directly, like adding lengths or weights.\n2. Use Harmonic Mean for rates or quantities that work reciprocally, like speed or resistance.\n3. Use Geometric Mean for data involving multiplication or compounding, like growth rates or percentages.\nBy understanding the relationship between the data and the type of average, you can choose the most meaningful measure for your analysis.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Central tendency II</span>"
    ]
  },
  {
    "objectID": "central2.html#relation-between-am-gm-and-hm",
    "href": "central2.html#relation-between-am-gm-and-hm",
    "title": "5  Central tendency II",
    "section": "5.4 Relation between AM, GM and HM",
    "text": "5.4 Relation between AM, GM and HM\nThe formula for the relation between AM, GM, HM is the product of arithmetic mean and harmonic mean is equal to the square of the geometric mean. This can be presented here in the form of Equation 5.7\n\\[\\mathbf{\\text{AM}}\\mathbf{\\times}\\mathbf{\\text{HM}}\\mathbf{=}\\mathbf{\\text{GM}}^{\\mathbf{2}} \\tag{5.7}\\]\nalso\n\\[\\mathbf{AM \\geq GM \\geq HM} \\tag{5.8}\\]\n\n5.4.1 Geometric illustration\nConsider two numbers a and b. See Figure 5.1 a semi circle can be drawn with diameter a+b. Then its radius is half the diameter, which will be the arithmetic mean \\(\\frac{a+b}{2}\\).\n\n\n\n\n\n\nFigure 5.1: Arithmetic mean on a semi circle\n\n\n\nThe geometric mean is the length of the perpendicular where a and b meet, which is never larger than the radius of the circle as illustrated in Figure 5.2\n\n\n\n\n\n\nFigure 5.2: Geometric mean on a semi circle\n\n\n\nNow draw a line from the top of red line which is the GM in Figure 5.2 to the center of the circle. Now that line is the radius of the circle, so it is equal to AM, which is now the hypotenuse of the newly formed triangle with GM as the leg of the triangle. So from Figure 5.3 it is clear that\n\\[\\mathbf{AM \\geq GM} \\tag{5.9}\\]\n\n\n\n\n\n\nFigure 5.3: Geometric mean and aritmetic mean on a semi circle\n\n\n\nNow if we draw an altitude to the hypotenuse as shown in Figure 5.3, the upper length on the hypotenuse is the harmonic mean . We can now consider another triangle where HM is a leg and the GM is the hypotenuse, this shows the GM is never smaller than the HM. So \\[\\mathbf{GM \\geq HM} \\tag{5.10}\\]\n\n\n\n\n\n\nFigure 5.4: AM, GM and HM on a semi circle\n\n\n\nNow from Equation 5.9 and Equation 5.10 it is clear that \\(\\mathbf{AM \\geq GM \\geq HM}\\)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Central tendency II</span>"
    ]
  },
  {
    "objectID": "central2.html#sec-positional_avg",
    "href": "central2.html#sec-positional_avg",
    "title": "5  Central tendency II",
    "section": "5.5 Positional averages",
    "text": "5.5 Positional averages\nPositional averages are measures derived directly from the values in a dataset. These averages are based on the position of the values within the series and are used to represent the overall dataset or highlight specific positional characteristics.\n\n\n\n\n\n\nNote\n\n\n\nThe median although a simple average is also a positional average that represents the middle value of an ordered dataset, making it a central point of reference. Similarly, the mode, which identifies the most frequently occurring value in the dataset, is also a positional average since it is directly taken from the series.\n\n\nThe other common positional averages include percentiles, quartiles, and deciles, which divide the data into equal parts to analyze its distribution.\nIn contrast, measures like the arithmetic mean, geometric mean, and harmonic mean are referred to as mathematical averages, as they are calculated through specific mathematical operations rather than being derived from the data’s positional properties.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Central tendency II</span>"
    ]
  },
  {
    "objectID": "central2.html#sec-quartile",
    "href": "central2.html#sec-quartile",
    "title": "5  Central tendency II",
    "section": "5.6 Quartiles",
    "text": "5.6 Quartiles\nThe median divides a dataset into two equal halves. Similarly, it is possible to divide a dataset into more than two parts. When an ordered dataset is divided into four equal sections, the points that mark these divisions are called quartiles.\nThe first or lower quartile (\\(\\mathbf{Q}_{\\mathbf{1}}\\)) is a value that has one fourth, or 25% of the observations below its value.\nThe second quartile (\\(\\mathbf{Q}_{\\mathbf{2}}\\)), has one-half, or 50% of the observations below its value. The second quartile is equal to the median.\nThe third or upper quartile, (\\(\\mathbf{Q}_{\\mathbf{3}}\\)), is a value that has three-fourths, or 75% of the observations below it. If there are n items in a dataset then\n\\[Q_1 = \\left( \\frac{n + 1}{4} \\right)^{\\text{th}} {\\text{item}} \\tag{5.11}\\]\n\\[Q_3 = \\left( \\frac{3(n + 1)}{4} \\right)^{\\text{th}} {\\text{item}} \\tag{5.12}\\]\nCalculations of quartiles are explained using the example below. See in the example the procedure followed when a fraction appear in the calculation.\nExample 5.8: Compute quartiles for the data 25, 18, 30, 8, 15, 5, 10, 35, 40, 45.\nSolution 5.8\nFirst arrange the data in ascending order\n5, 8, 10, 15, 18, 25, 30, 35, 40, 45\nhere n = 10\nusing Equation 5.11 and Equation 5.12\n\\(Q_{1} = \\left( \\frac{10 + 1}{4} \\right)^{th}\\) = 2.75th item; when such a fraction appears we use the following procedure\n$Q_{1} = $2.75th item = 2nd item + 0.75(3rd item - 2nd item)\nSo from the given data \\(Q_{1}\\) = 8+0.75(10– 8) = 9.5, also \\(Q_{2}\\) is the median, here \\(Q_{2}\\) = (18+25)/2 = 21.5\n\\(Q_{3} = \\left(\\frac{3(10 + 1)}{4} \\right)^{th}\\) = 8.25th item = 8th item + 0.25(9th item - 8th item) = 35+0.25(40-35) = 36.25\nQuartiles of a discrete frequency data\nThe following steps explain how to calculate quartiles for discrete frequency data\n\nFind cumulative frequencies.\nFind \\(\\left( \\frac{n + 1}{4} \\right)\\).\nSee in the cumulative frequencies, the value just greater than \\(\\left(\\frac{n + 1}{4}\\right)\\), then the corresponding value of \\(x\\) is \\(Q_{1}\\).\nFind \\(\\left( \\frac{3(n + 1)}{4}\\right)\\).\nSee in the cumulative frequencies, the value just greater than \\(\\left(\\frac{3(n + 1)}{4}\\right)\\), then the corresponding value of \\(x\\) is \\(Q_{3}\\).\n\nExample 5.9: Compute quartiles for the data given below\n\n\n\nTable 5.7: A model frequency distribution\n\n\n\n\n\nx\n5\n8\n12\n15\n19\n24\n30\n\n\nf\n4\n3\n2\n4\n5\n2\n4\n\n\n\n\n\n\nSolution 5.9\n\n\n\nTable 5.8: Cumulative frequency data for quartile calculation\n\n\n\n\n\nx\nf\ncf\n\n\n\n\n5\n4\n4\n\n\n8\n3\n7\n\n\n12\n2\n9\n\n\n15\n4\n13\n\n\n19\n5\n18\n\n\n24\n2\n20\n\n\n30\n4\n24\n\n\n\n\n\n\nHere n =24\n\\(\\left( \\frac{n + 1}{4} \\right)\\) = \\(\\left( \\frac{25}{4} \\right)\\) = 6.25\nThe cumulative frequency value just greater than 6.25 is 7, the\n\\({x}\\) value corresponding to cumulative frequency 7 is 8. So \\({Q}_{1}\\) = 8\n\\(\\left( \\frac{3(n + 1)}{4} \\right)\\) = \\(\\left( \\frac{3 \\times 25}{4} \\right)\\) = 18.75\nThe cumulative frequency value just greater than 18.75 is 20, the\n\\(\\mathbf{x}\\) value corresponding to cumulative frequency 20 is 24. So \\({Q}_{3}\\) = 24\nQuartiles of a grouped frequency data\nBelow it is explained steps in calculating quartiles for a continuous frequency data\n\nFind cumulative frequencies.\nFind \\(\\left( \\frac{n}{4} \\right)\\).\nSee in the cumulative frequencies, the value just greater than\\(\\left( \\frac{n}{4} \\right)\\), and then the corresponding class interval is called first quartile class.\nFind \\(3\\left( \\frac{n}{4} \\right)\\).\nSee in the cumulative frequencies the value just greater than \\(33\\left( \\frac{n}{4} \\right)\\)then the corresponding class interval is called 3rd quartile class. Then apply the respective formulae\n\n\\[Q_1 = l_1 + \\frac{\\frac{n}{4} - m_1}{f_1} \\times c_1 \\tag{5.13}\\]\n\\[Q_3 = l_3 + \\frac{3 \\left( \\frac{n}{4} \\right) - m_3}{f_3} \\times c_3 \\tag{5.14}\\]\nwhere, \\(l_{1}\\) = lower limit of the first quartile class\n\\(f_{1}\\) = frequency of the first quartile class\n\\(c_{1}\\) = width of the first quartile class\n\\(m_{1}\\) = cumulative frequency preceding the first quartile class\n\\(l_{3}\\)= 1ower limit of the 3rd quartile class\n\\(f_{3}\\)= frequency of the 3rd quartile class\n\\(c_{3}\\)= width of the 3rd quartile class\n\\(m_{3}\\) = cumulative frequency preceding the 3rd quartile class\nExample 5.10: Find the quartiles for the grouped frequency data given\n\n\n\nTable 5.9: A model grouped frequency data\n\n\n\n\n\nClass\nfrequency\ncumulative frequency\n\n\n\n\n0–10\n11\n11\n\n\n10–20\n18\n29\n\n\n20–30\n25\n54\n\n\n30–40\n28\n82\n\n\n40–50\n30\n112\n\n\n50–60\n33\n145\n\n\n60–70\n22\n167\n\n\n70–80\n15\n182\n\n\n80–90\n12\n194\n\n\n90–100\n10\n204\n\n\n\n\n\n\nSolution 5.10\n\\(\\left( \\frac{n}{4} \\right)\\) = \\(\\frac{204}{4}\\) = 51\nThe cumulative frequency value just greater than 51 is 54 so the class 20-30 is the 1st quartile class\nusing Equation 5.13\n\\[Q_1 = 20 + \\frac{51 - 29}{25} \\times 10 = 28.8\\]\n\\(3\\left( \\frac{n}{4} \\right)\\)= \\(3 \\times \\frac{204}{4}\\) = 153\nThe cumulative frequency value just greater than 153 is 167 so the class 60-70 is the 3rd quartile class.\nusing Equation 5.14\n\\[Q_1 = 60 + \\frac{153 - 145}{22} \\times 10 = 63.63\\]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Central tendency II</span>"
    ]
  },
  {
    "objectID": "central2.html#percentiles",
    "href": "central2.html#percentiles",
    "title": "5  Central tendency II",
    "section": "5.7 Percentiles",
    "text": "5.7 Percentiles\nPercentiles divide an ordered dataset into 100 equal parts, with each part containing 1% of the observations. The pth percentile, denoted as \\(P_p\\), is the value below which x percent of the data falls.\nFor example:\n- The \\(\\mathbf{P}_{\\mathbf{50}}\\), 50th percentile is equivalent to the median, representing the middle value of the dataset.\n- The \\(\\mathbf{P}_{\\mathbf{25}}\\), 25th percentile corresponds to the first quartile (\\(Q_1\\)), which marks the lower 25% of the data.\n- The \\(\\mathbf{P}_{\\mathbf{75}}\\), 75th percentile is the third quartile (\\(Q_3\\)), indicating that 75% of the data falls below this value.\nFor raw data, first arrange the n observations in increasing order. Then the xth percentile is given by\n\\[P_p = \\left( \\frac{p(n + 1)}{100} \\right)^{\\text{th}} \\text{ item} \\tag{5.15}\\]\nPercentiles of discrete frequency data\nTo calculate percentiles for discrete frequency data, follow these steps which is similar to that of quartiles:\n\nFind the cumulative frequencies.\nFind the position of the percentile. For the \\(p\\)-th percentile, calculate the position using the formula: \\[\nP_p = \\left( \\frac{p(n + 1)}{100} \\right)\n\\] where \\(p\\) is the percentile and \\(n\\) is the total number of data points.\nIdentify the value in the cumulative frequencies.\n\nfind the cumulative frequency that is just greater than or equal to the calculated position \\(P_p\\). The corresponding value of \\(x\\) is the \\(p\\)-th percentile.\nFor example, to find the first percentile (\\(P_1\\)): - Calculate \\(P_1 = \\left( \\frac{1(n + 1)}{100} \\right)\\). - Locate the cumulative frequency that is just greater than \\(P_1\\), and the corresponding value of \\(x\\) is \\(P_1\\).\nPercentiles of a grouped frequency data\nCalculation of percentile is very much similar to that of quartile. For a frequency distribution the pth percentile is given by following steps\n\nFind cumulative frequencies.\nFind \\(\\left( \\frac{p.n}{100} \\right)\\).\nSee in the cumulative frequencies, the value just greater than \\(\\left( \\frac{p.n}{100} \\right)\\) and then the corresponding class interval is called Percentile class.\nUse the following formula\n\n\\[P_p = l + \\frac{\\left( \\frac{p \\times n}{100} \\right) - cf}{f} \\times c \\tag{5.16}\\]\nwhere,\n\\({l}\\) = lower limit of the percentile class\n\\({cf}\\) = cumulative frequency preceding the percentile class\n\\({f}\\) = frequency of the percentile class\n\\({c}\\) = class interval\n\\({n}\\) = total number of observations\nExample 5.11: Compute \\({P}_{25}\\) and \\({P}_{75}\\) for the data 25, 18, 30, 8, 15, 5, 10, 35, 40, 45.\nSolution 5.11\nFirst arrange the data in ascending order\n5, 8, 10, 15, 18, 25, 30, 35, 40, 45\nHere n = 10\n\\(P_{25} = \\left( \\frac{25(10 + 1)}{100} \\right)^{\\text{th}}\\) = 2.75th item\n\\(P_{25}\\) = 2.75th item = 2nd item + 0.75(3rd item – 2nd item)\nSo from the given data \\(P_{25}\\) = 8+0.75(10– 8) = 9.5\n\\(P_{75} = \\left( \\frac{75(10 + 1)}{100} \\right)^{\\text{th}}\\) = 8.25th item\ni.e. \\(P_{75} = \\left( 75 \\times \\frac{10 + 1}{100} \\right)^{th}\\) = 8.25th item = 8th item + 0.25(9th item –8th item) = 35+0.25(40-35) = 36.25\n\n\n\n\n\n\nNote\n\n\n\nData in Example 5.11 is same as Example 5.8; it can be seen that \\(P_{25} = Q_{1}\\) & \\(P_{75} = Q_{3}\\) always\n\n\n\n\n\n\n\n\nTry yourself\nFind \\(P_{25}\\), \\(P_{50}\\) & \\(P_{75}\\) for Example 5.9 & 5.10; verify that \\(P_{50} = Q_{2}\\), \\(P_{25} = Q_{1}\\) & \\(P_{75} = Q_{3}\\)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Central tendency II</span>"
    ]
  },
  {
    "objectID": "central2.html#deciles",
    "href": "central2.html#deciles",
    "title": "5  Central tendency II",
    "section": "5.8 Deciles",
    "text": "5.8 Deciles\nDeciles consist of 9 points that divide an ordered dataset into ten equal parts. The d th decile is denoted as \\(D_d\\). It is important to note that the median is the 5th decile.\n\\[D_d = \\left( \\frac{d(n + 1)}{10} \\right)^{\\text{th}} \\text{ item} \\tag{5.17}\\]\nwhere, \\(d\\) is the decile number (e.g., \\(d = 1\\) for the first decile, \\(d = 9\\) for the ninth decile), and \\(n\\) is the total number of data points.\nDeciles of discrete frequency data\nTo calculate deciles for discrete frequency data, follow these steps which are similar to that of percentiles:\n\nFind the cumulative frequencies.\nFind the position of the decile.\nFor the \\(d^{th}\\) decile, calculate the position using the formula:\n\\[D_d = \\left( \\frac{d(n + 1)}{10} \\right)\\]\n\nIdentify the value in the cumulative frequencies.\nFind the cumulative frequency that is just greater than or equal to the calculated position \\(D_d\\). The corresponding value of \\(x\\) is the \\(d^{th}\\) decile.\n\nFor example, to find the first decile (\\(D_1\\)): - Calculate \\(D_1 = \\left( \\frac{1(n + 1)}{10} \\right)\\). - Locate the cumulative frequency that is just greater than \\(D_1\\), and the corresponding value of \\(x\\) is \\(D_1\\).\nDeciles of a grouped frequency data\nFor a frequency distribution the dth decile is given by following steps\n\nFind cumulative frequencies.\nFind \\(\\left( \\frac{d.n}{10} \\right)\\).\nSee in the cumulative frequencies, the value just greater than\\(\\left( \\frac{d.n}{10} \\right)\\)and then the corresponding class interval is called decile class.\nUse the following formula\n\n\\[D_d = l + \\frac{\\left( \\frac{d \\times n}{10} \\right) - cf}{f} \\times c \\tag{5.18}\\]\nwhere,\n\\(l\\) = lower limit of the decile class\n\\({cf}\\) = cumulative frequency preceding the decile class\n\\({f}\\) = frequency of the decile class\n\\({c}\\) = class interval\n\\({n}\\) = total number of observations\n\n\n\n\n\n\nTry yourself\nFind \\(\\text{D}_{5}\\) for Example 5.9, 5.10 & 5.11; verify that \\({D}_{5} = {Q}_{2} = {P}_{50} = median\\)\n\n\n\n\n\n\n\n\n\nHistorical Insights\n\n\n\nThe harmonic mean and the perfect fourth\nThe harmonic mean a term derived from the ancient Greeks, particularly associated with Pythagoras or his followers. The harmonic mean is closely related to musical intervals, specifically the perfect fourth. In music theory, an octave change upwards corresponds to a doubling of the frequency (a 1:2 ratio). The harmonic mean of 1 and 2, which is \\(\\frac{4}{3}\\), defines the frequency ratio for the perfect fourth, making it a crucial concept in understanding musical harmony and acoustics.\n\n\n\n\n\n\n\n\nQuotes to Inspire\n\n\n\n“The best thing about being a statistician is that you get to play in everybody else’s backyard”. – John Tukey",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Central tendency II</span>"
    ]
  },
  {
    "objectID": "dispersion.html",
    "href": "dispersion.html",
    "title": "6  Measures of dispersion",
    "section": "",
    "text": "6.1 Characteristics of a good measure of dispersion\nIn the previous chapters, we have seen how a set of data can be summarized by a single representative value that describes the central tendency of the data. Consider the two sets of data, A and B, in Table 6.1.\nYou can see mean, median and mode for both the sets A & B in Table 6.1 is 3.\nThe plot of values of A and B in Table 6.1 can be seen in Figure 6.1. The figure is known as dot plot.\nIt can be seen in Figure 6.1 that, while values of data set A are grouped close to their mean, while the values of data set B are more spread out. We say that values of data set B are more dispersed (or scattered) than those of data set A. This example shows that the measures of central tendency are not enough in describing a set of data. In addition to using these measures, we need numerical measures of dispersion (or variation) of a set of data.\nAn ideal measure of dispersion is expected to possess the following properties\nThe most important measures of dispersion are range, quartile deviation, variance, inter-quartile range, mean absolute deviation and standard deviation.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Measures of dispersion</span>"
    ]
  },
  {
    "objectID": "dispersion.html#characteristics-of-a-good-measure-of-dispersion",
    "href": "dispersion.html#characteristics-of-a-good-measure-of-dispersion",
    "title": "6  Measures of dispersion",
    "section": "",
    "text": "It should be rigidly defined.\nIt should be based on all the items.\nIt should not be unduly affected by extreme items.\nIt should lend itself for algebraic manipulation.\nIt should be simple to understand and easy to calculate.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Measures of dispersion</span>"
    ]
  },
  {
    "objectID": "dispersion.html#the-range",
    "href": "dispersion.html#the-range",
    "title": "6  Measures of dispersion",
    "section": "6.2 The range",
    "text": "6.2 The range\nThis is the simplest possible measure of dispersion. The range of a set of data is defined as the difference between the largest observation and the smallest observation in the set of data.\nThus,\nRange = largest observation – smallest observation.\nIt can be denoted as, Range = L – S.\nwhere, L = Largest value; S = Smallest value.\nExample 6.1: The marks obtained by 8 students in mathematics and physics examinations are as follows:\n\n\n\nTable 6.2: Model dataset of marks obtained in two subjects\n\n\n\n\n\nStudent\nMathematics\nPhysics\n\n\n\n\n1\n35\n50\n\n\n2\n60\n55\n\n\n3\n70\n70\n\n\n4\n40\n65\n\n\n5\n85\n89\n\n\n6\n96\n68\n\n\n7\n55\n72\n\n\n8\n65\n80\n\n\n\n\n\n\nFind the ranges of the two sets of data. Are the physics marks more dispersed than the mathematics marks?\nSolution 6.1\nFor mathematics,\nHighest mark = 96, lowest mark = 35, range = 96 – 35 = 61\nFor physics,\nHighest mark = 89, lowest mark = 50, range = 89 – 50 = 39.\nThe mathematics marks have a wider range than the physics marks. The mathematics marks are therefore more dispersed than the physics marks.\nIn individual observations and discrete series, L and S are easily identified. In case of grouped frequency distribution, the following method is employed.\nL = Upper boundary of the highest class\nS = Lower boundary of the lowest class.\n\\[Range = L - S \\tag{6.1}\\]\nExample 6.2: Calculate range from the following distribution\n\n\n\nTable 6.3: Model frequency distribution table for range calculation\n\n\n\n\n\nSize\n60–63\n63–66\n66–69\n69–72\n72–75\n\n\nNumber\n5\n18\n42\n27\n8\n\n\n\n\n\n\nSolution 6.2\nL = Upper boundary of the highest class = 75\nS = Lower boundary of the lowest class = 60\nRange = L – S = 75 – 60 = 15\nMerits and demerits of range\nMerits\n\nIt is simple to understand.\nIt is easy to calculate.\nIn certain types of problems like quality control, weather forecasts, share price analysis, etc.\n\nDemerits\n\nIt is very much affected by the extreme items.\nIt is based on only two extreme observations.\nIt cannot be calculated from open-end class intervals.\nIt is not suitable for mathematical treatment.\nIt is a very rarely used measure.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Measures of dispersion</span>"
    ]
  },
  {
    "objectID": "dispersion.html#the-inter-quartile-range-iqr",
    "href": "dispersion.html#the-inter-quartile-range-iqr",
    "title": "6  Measures of dispersion",
    "section": "6.3 The inter-quartile range (IQR)",
    "text": "6.3 The inter-quartile range (IQR)\nThe range is a simple and quick measure to calculate. However, because it relies solely on the maximum and minimum values in a data set, it does not provide information about how the data is distributed between these two values. As a result, the range may not be an effective measure of dispersion, especially if one or both of these values are significantly different from the rest of the data. To address this limitation, the interquartile range is often used. The interquartile range is a more robust measure of dispersion, defined as the difference between the upper and lower quartiles of the data. IQR is also known as midspread. Thus,\n\\[IQR = Q_3 - Q_1 \\tag{6.2}\\]\nThe inter-quartile range of a set of data is therefore not affected by values of the data outside Q1 and Q3 making it a more reliable measure of spread for skewed or non-normal distributions.\nExample 6.3: Consider the two sets of data A & B below, find IQR\n\n\n\nTable 6.4: Model dataset for IQR calculation\n\n\n\n\n\nA\n3\n4\n5\n6\n8\n9\n10\n12\n15\n\n\nB\n3\n8\n8\n9\n9\n9\n10\n10\n15\n\n\n\n\n\n\nFor data set A, Q1 = 4.5, Q3 = 11; so Inter-Quartile Range = 11 – 4.5 = 6.5\nFor data set B, Q1 = 8, Q3 = 10; so Inter-Quartile Range = 10 – 8 = 2\nSince the interquartile range (IQR) of data set A is greater than that of data set B, these results indicate that data set A is more dispersed than data set B. It is also noticeable that the range is the same for both sets.\nMerits and demerits of IQR\nMerits\n\nIt is simple to calculate and easy to understand.\nIt is not affected by extreme values (outliers) in the data, making it a more reliable measure of spread than the range.\nIQR provides a clear measure of the spread of the middle 50% of the data, giving a better representation of variability when data is skewed.\nIt can be used for skewed distributions, where the range and standard deviation may not be as useful.\nIQR is particularly useful in identifying outliers, as data points outside 1.5 times the IQR from the quartiles are often considered outliers.\n\nDemerits\n\nThe IQR does not use all the data points, which means it may not represent the variability of the entire dataset.\nIt may not be as intuitive as the range or standard deviation for some users, particularly in more complex datasets.\nThe IQR is less sensitive to variations in the data outside of the interquartile range, meaning it might not fully reflect extreme values or trends.\nIt is not as effective when comparing datasets with significantly different shapes or distributions.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Measures of dispersion</span>"
    ]
  },
  {
    "objectID": "dispersion.html#mean-absolute-deviation-mad",
    "href": "dispersion.html#mean-absolute-deviation-mad",
    "title": "6  Measures of dispersion",
    "section": "6.4 Mean absolute deviation (MAD)",
    "text": "6.4 Mean absolute deviation (MAD)\nThe mean absolute deviation (MAD) is a measure of variability that indicates the average distance between observations and their mean. MAD uses the original units of the data, which simplifies interpretation. Larger values signify that the data points spread out further from the average. Conversely, lower values correspond to data points bunching closer to it. The mean absolute deviation is also known as the mean deviation and average absolute deviation.\nHere is how to calculate the mean absolute deviation.\n\nCalculate the mean.\nCalculate the difference of each observation from mean and take absolute value i.e. ignore the sign. This difference is known as absolute deviation.\nAdd those deviations together.\nDivide the sum by the number of data points.\n\n\\[MAD = \\frac{\\sum_{i = 1}^{n}\\left| x_{i} - \\overline{x} \\right|}{n} \\tag{6.3}\\]\nExample 6.4: Find the mean absolute deviation of the following 10, 15, 15, 17, 18, 21\n\n\n\nTable 6.5: Calculation of mean absolute deviation\n\n\n\n\n\n\n\n\n\n\n\\[x_{i}\\]\n\\[x_{i} - \\overline{x}\\]\n\\[\\left| x_{i} - \\overline{x}\\right|\\]\n\n\n\n\n10\n-6\n6\n\n\n15\n-1\n1\n\n\n15\n-1\n1\n\n\n17\n1\n1\n\n\n18\n2\n2\n\n\n21\n5\n5\n\n\n\\(\\overline{x} =\\) 16\n\n\\(\\sum_{i = 1}^{n}\\left| x_{i} - \\overline{x} \\right|\\) = 16\n\n\n\n\n\n\nHere n = 6 and \\(\\sum_{i = 1}^{n}\\left| x_{i} - \\overline{x} \\right|\\) = 16 therefore MAD = \\(\\frac{16}{6} = 2.67\\)\nMerits and demerits of MAD\nMerits\n\nMean deviation is simple and easy.\nDifferent items of observations can be easily compared with mean deviation.\nMean deviation is better than quartile deviation and range because it is based on all the observations of the series.\nMean deviation is less affected by the extreme values in the series while comparing to standard deviation.\nMean deviation is rigidly defined. So, it has fixed value.\nMean deviation about median will be least.\n\nDemerits\n\nMean deviation becomes difficult to compute mean deviation in case of fractions.\nIt is not applicable for algebraic calculations.\nIt cannot be calculated from open-end class intervals.\nMean deviation is not a good measure as it ignores negative signs of deviations.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Measures of dispersion</span>"
    ]
  },
  {
    "objectID": "dispersion.html#the-variance-and-standard-deviation",
    "href": "dispersion.html#the-variance-and-standard-deviation",
    "title": "6  Measures of dispersion",
    "section": "6.5 The variance and standard deviation",
    "text": "6.5 The variance and standard deviation\nThe most important measures of variability are the sample variance and the sample standard deviation. If x1, x2, …,xn is a sample of n observations, then the sample variance is denoted by s² and is defined by the equation.\n\\[\n\\mathbf{\\text{sample variance}},\\mathit{s}^{2} = \\frac{\\sum_{i = 1}^{n} (x_{i} - \\overline{x})^{2}}{n - 1}\n\\tag{6.4}\\]\nThe sample standard deviation, s, is the positive square root of the sample variance.\n\\[\\mathbf{\\text{standard deviation} ,\\mathit{s} = \\ }\\sqrt{\\frac{\\sum_{i = 1}^{n} (x_{i} - \\overline{x})^{2}}{n - 1}} \\tag{6.5}\\]\n\n\n\n\n\n\nNote\n\n\n\nWhy standard deviation?\nWhile both variance and standard deviation measure data dispersion, standard deviation is preferred for practical interpretation. Variance is expressed in squared units, making it harder to interpret. For example, if the data represents lengths in meters, the variance is in square meters (m²), which complicates understanding variability. In contrast, standard deviation is the square root of variance, preserving the original unit (e.g., meters), making it more intuitive. Thus, standard deviation is preferred for its clarity and ease of interpretation, especially when analyzing how data points deviate from the mean.\n\n\nIf the standard deviation of data set A is greater than that of data set B, it indicates that data set A is more dispersed than data set B. A higher standard deviation means that the values in data set A are more spread out from the mean compared to the values in data set B. It’s important to note that the standard deviation of any data set is always a non-negative number, as it represents the square root of the variance, which is always non-negative. Variance and standard deviation can never be negative values.\nExample 6.5: Consider the Table 6.1 discussed earlier, find the standard deviation?\nSolution 6.5\n\n\n\nTable 6.6: Calculation of standard deviation of set A\n\n\n\n\n\n\n\n\n\n\n\nSet A\n\\[x_{i}\\]\n\\[x_{i} - \\overline{x}\\]\n\\[\n\\left( x_{i} - \\overline{x} \\right)^{2}\n\\]\n\n\n\n1\n-2\n4\n\n\n\n2\n-1\n1\n\n\n\n3\n0\n0\n\n\n\n3\n0\n0\n\n\n\n4\n1\n1\n\n\n\n5\n2\n4\n\n\nSum\n18\n0\n10\n\n\n\n\n\n\nMean (\\(\\overline{x}\\)) =\\(\\ \\frac{18}{6} = 3\\)\n\\[{Sample\\ variance,\\ s}_{A}^{2} = \\frac{\\sum_{i = 1}^{n}\\left( x_{i} - \\overline{x} \\right)^{2}}{n - 1} = \\frac{10}{5} = 2\\]\n\\[\\text{Sample standard deviation,}\\ s_{A} = \\ \\sqrt{s_{A}^{2}} = \\ \\sqrt{2} = 1.414\\]\n\n\n\nTable 6.7: Calculation of standard deviation of set B\n\n\n\n\n\n\n\n\n\n\n\nSet B\n\\[x_{i}\\]\n\\[x_{i} - \\overline{x}\\]\n\\[\n\\left( x_{i} - \\overline{x} \\right)^{2}\n\\]\n\n\n\n-1\n-4\n16\n\n\n\n0\n-3\n9\n\n\n\n3\n0\n0\n\n\n\n3\n0\n0\n\n\n\n5\n2\n4\n\n\n\n8\n5\n25\n\n\nSum\n18\n0\n54\n\n\n\n\n\n\nMean (\\(\\overline{x}\\)) =\\(\\ \\frac{18}{6} = 3\\)\n\\[{Sample\\ variance,\\ s}_{B}^{2} = \\frac{\\sum_{i = 1}^{n}\\left( x_{i} - \\overline{x} \\right)^{2}}{n - 1} = \\frac{54}{5} = 10.8\\]\n\\[Sample\\ standard\\ deviation,\\ s_{B} = \\ \\sqrt{s_{B}^{2}} = \\ \\sqrt{10.8} = 3.29\\]\nIt can be seen that \\(s_{B} &gt; s_{A}\\), confirming that data set B is more dispersed than data set A as shown in Figure 6.1.\nAn alternative formula for computing the variance\nThe computation of s² requires calculations of \\(\\overline{x}\\), n subtractions and n squaring and adding operations. If the original observations or the deviations \\(\\left( x_{i} - \\overline{x} \\right)\\) are not integers, the deviations \\(\\left( x_{i} - \\overline{x} \\right)\\) may be difficult to work with, and several decimals may have to be carried to ensure numerical accuracy. A more efficient computational formula for s² is given by\n\\[s^{2} = \\frac{1}{n - 1}\\left\\{ \\sum_{i = 1}^{n}{x_{i}^{2} - \\frac{1}{n}}\\left( \\sum_{i = 1}^{n}x_{i} \\right)^{2} \\right\\} \\tag{6.6}\\]\nExample 6.6: Consider the data set below; find standard deviation?\n\n\n\nTable 6.8: Model dataset for standard deviation calculation\n\n\n\n\n\n3\n4\n5\n6\n8\n9\n10\n12\n15\n\n\n\n\n\n\nSolution 6.6\n\n\n\nTable 6.9: Calculation of sd using alternate formula\n\n\n\n\n\n\n\n\n\n\\[x_{i}\\]\n\\[x_{i}^{2}\\]\n\n\n\n\n3\n9\n\n\n4\n16\n\n\n5\n25\n\n\n6\n36\n\n\n8\n64\n\n\n9\n81\n\n\n10\n100\n\n\n12\n144\n\n\n15\n225\n\n\n\\(\\sum_{i=1}^{9} x_{i}\\) = 72\n\\(\\sum_{i=1}^{9} x_{i}^{2}\\) = 700\n\n\n\n\n\n\nusing Equation 6.6 ; here n = 9\n\\(s^{2} = \\frac{1}{8}\\left\\{ 700 - {\\frac{1}{9}\\left( 72 \\right)}^{2} \\right\\}\\) = 15.5\n\\(s = \\ \\sqrt{15.5} = 3.94\\)\n\n6.5.1 Standard deviation for frequency table\nFor discrete frequency table\n\\[s^{2} = \\frac{1}{n - 1}\\left\\{ \\sum_{i = 1}^{n}{{f_{i}x}_{i}^{2} - \\frac{1}{n}}\\left( \\sum_{i = 1}^{n}{f_{i}x}_{i} \\right)^{2} \\right\\} \\tag{6.7}\\]\nwhere, \\(x_i\\) is the ith observation and \\(f_{i}\\) is the corresponding frequency\nExample 6.7: The frequency distributions of seed yield of 50 sesamum plants are given below. Find the standard deviation.\n\n\n\nTable 6.10: A model frequency distributions of seed yield\n\n\n\n\n\nSeed yield in gms (x)\n3\n4\n5\n6\n7\n\n\nFrequency (f)\n4\n6\n15\n15\n10\n\n\n\n\n\n\nSolution 6.7\n\n\n\nTable 6.11: Calculation of standard deviation for frequency table\n\n\n\n\n\n\\(x_{i}\\)\n\\(f_{i}\\)\n\\(f_{i}.x_{i}\\)\n\\(f_{i}.x_{i}^{2}\\)\n\n\n3\n4\n12\n36\n\n\n4\n6\n24\n96\n\n\n5\n15\n75\n375\n\n\n6\n15\n90\n540\n\n\n7\n10\n70\n490\n\n\nTotal\n50\n271\n1537\n\n\n\n\n\n\nusing Equation 6.7\n\\[{sample\\ variance,\\ s}^{2} = \\frac{1}{50 - 1}\\left\\{ 1537 - \\frac{271^{2}}{50} \\right\\} = 1.3914\\]\n\\(standard\\ deviation,\\ s = \\sqrt{1.3914}\\) = 1.179\nFor grouped frequency table\n\\[s^{2} = \\frac{1}{n - 1}\\left\\{ \\sum_{i = 1}^{n}{{f_{i}d}_{i}^{2} - \\frac{1}{n}}\\left( \\sum_{i = 1}^{n}{f_{i}d}_{i} \\right)^{2} \\right\\} \\tag{6.8}\\]\nwhere, \\(f_{i}\\) is the frequency of ith class, \\(d_{i} = \\frac{x_{i} - A}{c}\\), where \\(x_{i}\\) is the class mark, \\(A\\) is the class mark with the highest frequency and c is the class interval.\nExample 6.8: The frequency distributions of seed yield of 50 sesamum plants are given below. Find the standard deviation\n\n\n\nTable 6.12: A model gropued frequency distributions of seed yield\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeed yield in gms (x)\n2.5–3.5\n3.5–4.5\n4.5–5.5\n5.5–6.5\n6.5-7.5\n\n\nFrequency (f)\n4\n6\n15\n15\n10\n\n\n\n\n\n\nSolution 6.8\nHere n = 50; c = 1\n\n\n\nTable 6.13: Calculation of sd for grouped frequency distribution\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeed yield\n\\(f_{i}\\)\n\\(x_{i}\\)\n\\[d_{i} = \\frac{x_{i} - A}{c}\\]\n\\(f_{i}.d_{i}\\)\n\\(f_{i}.d_{i}^{2}\\)\n\n\n\n\n2.5–3.5\n4\n3\n-2\n-8\n16\n\n\n3.5–4.5\n6\n4\n-1\n-6\n6\n\n\n4.5–5.5\n15\n5\n0\n0\n0\n\n\n5.5–6.5\n15\n6\n1\n15\n15\n\n\n6.5–7.5\n10\n7\n2\n20\n40\n\n\nTotal\n50\n25\n0\n21\n77\n\n\n\n\n\n\nA = 5\nusing Equation 6.8\n\\[{sample\\ variance,\\ s}^{2} = \\frac{1}{49}\\left( 77 - \\frac{\\left( 21 \\right)^{2}}{50} \\right) = \\ 1.3914\\]\n\\(standard\\ deviation,\\ s = \\sqrt{1.3914}\\) = 1.179\n\n\n6.5.2 Merits and demerits of standard deviation\nMerits\n\nIt is rigidly defined and its value is always definite and based on all the observations.\nAs it is based on arithmetic mean, it has all the merits of arithmetic mean.\nIt is the most important and widely used measure of dispersion.\nIt is possible for further algebraic treatment.\nIt is less affected by the fluctuations of sampling and hence stable.\nIt is the basis for measuring the coefficient of correlation and other measures.\n\nDemerits\n\nIt is not easy to understand and it is difficult to calculate.\nIt gives more weight to extreme values because the values are squared up.\nAs it is an absolute measure of variability, it cannot be used for the purpose of comparison.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Measures of dispersion</span>"
    ]
  },
  {
    "objectID": "dispersion.html#coefficient-of-variation",
    "href": "dispersion.html#coefficient-of-variation",
    "title": "6  Measures of dispersion",
    "section": "6.6 Coefficient of variation",
    "text": "6.6 Coefficient of variation\nThe standard deviation is an absolute measure of dispersion. It is expressed in terms of units in which the original figures are collected and stated. The standard deviation of heights of plants cannot be compared with the standard deviation of weights of the grains, as both are expressed in different units, i.e. heights in centimetre and weights in kilograms.\nTherefore, the standard deviation must be converted into a relative measure of dispersion for the purpose of comparison. The relative measure is known as the coefficient of variation. The coefficient of variation is obtained by dividing the standard deviation by the mean and expressed in percentage.\n\\[\\text{Coefficient of variation} \\, (C.V) = \\frac{\\text{standard deviation}}{\\text{mean}} \\times 100 \\tag{6.9}\\]\nA higher C.V. indicates greater variability in the dataset, meaning the data values are more dispersed relative to the mean. In contrast, a lower C.V. signifies lower variability, indicating that the data values are more closely clustered around the mean. This measure is particularly useful when comparing datasets with different units or scales.\nExample 6.9: Consider the measurement on yield and plant height of a paddy variety. The mean and standard deviation for yield are 50 kg and 10 kg respectively. The mean and standard deviation for plant height are 55 cm and 5 cm respectively. Compare the variability.\nSolution 6.9\nHere, the measurements for yield and plant height are in different units. Hence the variability can be compared only by using coefficient of variation.\nFor yield, CV = \\(\\ \\frac{10}{50} \\times 100 =\\) 20%\nFor plant height, CV = \\(\\frac{5}{55} \\times 100 =\\) 9.1%\nThe yield is subject to more variation than the plant height.\n \n \n\n\n\n\n\n\n\nHistorical Insights\n\n\n\nExploring variability\nThe term “standard deviation” was first introduced in writing by Karl Pearson in 1894 in his paper “Contributions to the Mathematical Theory of Evolution.” Prior to this, the concept was referred to by other names, including “mean error,” “mean square error,” and “error of mean square,” reflecting its origins in the study of measurement errors and variability.(Pearson 1894)\nThe concept of variance was formalized in 1918 by Sir Ronald Aylmer Fisher in his seminal paper “The Correlation Between Relatives on the Supposition of Mendelian Inheritance.” While earlier mathematicians like Carl Friedrich Gauss made significant contributions to the development of probability and error theory, which influenced the understanding of variability, the term “variance” as we know it today was introduced by Fisher.(Fisher 1918)\n\n\n\n\n\n\n\n\nQuotes to Inspire\n\n\n\n“The object of our being statistical is to learn how to improve the whole health of humanity” – Florence nightingale\n\n\n\n\n\n\n\nFisher, Ronald A. 1918. “The Correlation Between Relatives on the Supposition of Mendelian Inheritance.” Transactions of the Royal Society of Edinburgh 52 (2): 399–433. https://doi.org/10.1017/S0080456800012163.\n\n\nPearson, Karl. 1894. “Contributions to the Mathematical Theory of Evolution.” Philosophical Transactions of the Royal Society of London. A 185: 71–110. https://doi.org/10.1098/rsta.1894.0003.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Measures of dispersion</span>"
    ]
  },
  {
    "objectID": "skewness.html",
    "href": "skewness.html",
    "title": "7  Skewness and kurtosis",
    "section": "",
    "text": "7.1 Skewness\nIn the previous chapter, we explored numerical measures of central tendency and dispersion. Together, these measures give us insights into the location and spread of our data. However, they don’t fully describe the data distribution. What about its shape?\nThe shape of a distribution helps us understand the symmetry, peakedness, and presence of tails in the data. While a histogram provides a visual summary of the shape, we often need numerical measures for precise analysis. These measures include:\nIn this chapter, we will have a detailed discussion on these two important measures of shape, understanding how they are calculated and interpreted. By the end, you’ll be able to evaluate whether a distribution is symmetric, positively or negatively skewed, and whether it has light or heavy tails.\nSkewness is a measure of symmetry, or more precisely, the lack of symmetry. Then you may ask, what will a symmetric distribution looks like. Histogram of a symmetric distribution is showed in Figure 7.1.\nA distribution, or data set, is symmetric if it looks the same to the left and right of the centre point. In our discussion we are including only unimodal cases.\nFigure 7.3 shows a model data set with skewness = 0 (symmetric distribution)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Skewness and kurtosis</span>"
    ]
  },
  {
    "objectID": "skewness.html#skewness",
    "href": "skewness.html#skewness",
    "title": "7  Skewness and kurtosis",
    "section": "",
    "text": "Figure 7.1: Histogram of a symmetric distribution\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFor a symmetric distribution skewness = 0; mean = median = mode. Figure 7.2 shows how a symmetric distribution looks like.\n\n\n\n\n\n\n\n\nFigure 7.2: Symmetric distribution\n\n\n\n\n\n\n\n\n\n\nFigure 7.3: Data set with skewness = 0\n\n\n\n\n7.1.1 Negatively skewed\nA negatively skewed distribution, also known as a left-skewed distribution, is characterized by a longer tail on the left side of the distribution. The bulk of the data values, or the “mass” of the distribution, is concentrated on the right, as shown in Figure 7.4.\nThis type of distribution is referred to as left-skewed, left-tailed, or skewed to the left because of the extended left tail. In such cases, the numerical relationship between the mean, median, and mode typically follows this pattern:\nMean &lt; Median &lt; Mode\nThis occurs because the mean is pulled towards the longer tail, while the median and mode remain closer to the center of the data’s bulk.\n\n\n\n\n\n\nFigure 7.4: Left skewed or negatively skewed distribution\n\n\n\nFigure 7.5 shows a model dataset with negative skewness.\n\n\n\n\n\n\nFigure 7.5: Negatively skewed data set\n\n\n\n\n\n7.1.2 Positively skewed\nA positively skewed distribution, also known as a right-skewed distribution, is characterized by a longer tail on the right side. The bulk of the data values, or the “mass” of the distribution, is concentrated on the left, as illustrated in Figure 7.6.\nThis type of distribution is referred to as right-skewed, right-tailed, or skewed to the right, due to the extended tail on the right. In such cases, the relationship between the mean, median, and mode typically follows this pattern:\nMean &gt; Median &gt; Mode\nThis occurs because the mean is influenced by the extreme values in the longer right tail, while the median and mode remain closer to the center of the data’s bulk.\n\n\n\n\n\n\nFigure 7.6: Right skewed or positively skewed distribution\n\n\n\nFigure 7.7 shows a model dataset with positive skewness.\n\n\n\n\n\n\nFigure 7.7: Data set with positive skewness or right skewed",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Skewness and kurtosis</span>"
    ]
  },
  {
    "objectID": "skewness.html#measures-of-skewness",
    "href": "skewness.html#measures-of-skewness",
    "title": "7  Skewness and kurtosis",
    "section": "7.2 Measures of skewness",
    "text": "7.2 Measures of skewness\nThe direction and extent of skewness can be measured in various ways. We shall discuss four measures.\n\n7.2.1 Karl Pearson’s coefficient of skewness (\\(S_{k}\\))\nYou have noticed that the mean, median and mode are not equal in a skewed distribution. The Karl Pearson’s measure of skewness is based upon the divergence of mean from mode in a skewed distribution.\n\\[S_{k} = \\frac{mean - mode}{\\text{standard deviation}} \\tag{7.1}\\]\nThe sign of \\(S_{k}\\) gives the direction of skewness and its magnitude gives the extent of skewness. If \\(S_{k}\\) &gt; 0, the distribution is positively skewed, and if \\(S_{k}\\) &lt; 0 it is negatively skewed.\nIn Equation 7.1 since mode is used, there is a problem that if mode is not defined for a distribution we cannot find \\(S_{k}\\). But empirical relation between mean, median and mode states that, for a moderately symmetrical distribution \\(\\ mean - mode \\approx 3(mean - median)\\). So Equation 7.1 can be written as\n\\[S_{k} = \\frac{3(mean - median)}{\\text{standard deviation}} \\tag{7.2}\\]\nExample 7.1: Compute the Karl Pearson’s coefficient of skewness from the following data:\n\n\n\nTable 7.1: Model dataset for skewness calculation\n\n\n\n\n\nHeight (x)\nfrequency (f)\n\n\n\n\n58\n10\n\n\n59\n18\n\n\n60\n30\n\n\n61\n42\n\n\n62\n35\n\n\n63\n28\n\n\n64\n16\n\n\n65\n8\n\n\n\n\n\n\nSolution 7.1\n\n\n\nTable 7.2: Karl Pearson’s coefficient of skewness\n\n\n\n\n\n\nHeight (\\(x_{i}\\))\n\n\nfrequency (\\(f_{i}\\))\n\n\n\\(f_{i}x_{i}\\)\n\n\n\\(f_{i}x_{i}^{2}\\)\n\n\n\n\n\n\n58\n\n\n10\n\n\n580\n\n\n33640\n\n\n\n\n59\n\n\n18\n\n\n1062\n\n\n62658\n\n\n\n\n60\n\n\n30\n\n\n1800\n\n\n108000\n\n\n\n\n61\n\n\n42\n\n\n2562\n\n\n156282\n\n\n\n\n62\n\n\n35\n\n\n2170\n\n\n134540\n\n\n\n\n63\n\n\n28\n\n\n1764\n\n\n111132\n\n\n\n\n64\n\n\n16\n\n\n1024\n\n\n65536\n\n\n\n\n65\n\n\n8\n\n\n520\n\n\n33800\n\n\n\n\nSum\n\n\n187\n\n\n11482\n\n\n705588\n\n\n\n\n\n\n\nMean, \\(\\overline{x} = \\frac{\\sum_{i = 1}^{n}{f_{i}x_{i}}}{\\sum_{i = 1}^{n}f_{i}}\\) = \\(\\frac{11482}{187} = 61.40\\)\n\\({Sample\\ variance,\\ s}^{2}\\) using Equation 6.7 = \\(\\frac{705588 - \\frac{\\left( 11482 \\right)^{2}}{187}}{186} = 3.123\\)\n\\(Standard\\ deviation,\\ s = \\sqrt{3.123} = 1.76\\)\nTo calculate the median, refer to the Table 7.3. Locate the cumulative frequency just greater than \\(\\frac{n + 1}{2}\\), and the corresponding value of \\(x\\) will be the median (\\(Q_2\\)).\nHere, \\(\\frac{n + 1}{2} = \\frac{187 + 1}{2} = \\frac{188}{2} = 94\\).\nFrom the Table 7.3, it is evident that the median is 61.\n\n\n\nTable 7.3: Cumulative frequency for skewness calculation\n\n\n\n\n\nHeight (\\(x_{i}\\))\nfrequency (\\(f_{i}\\))\ncumulative frequency\n\n\n\n\n58\n10\n10\n\n\n59\n18\n28\n\n\n60\n30\n58\n\n\n61\n42\n100\n\n\n62\n35\n135\n\n\n63\n28\n163\n\n\n64\n16\n179\n\n\n65\n8\n187\n\n\n\n\n\n\nusing Equation 7.2\n\\[S_{k} = \\frac{3(61.40 - 61)}{1.76} = \\frac{1.2}{1.76} = 0.68\\]\nHence, the Karl Pearson’s coefficient of skewness \\(S_{k}\\) = \\(0.68\\), Thus the distribution is positively skewed.\n\n\n7.2.2 Bowley’s measure of skewness (SQ)\nKarl Pearson’s coefficient of skewness is most commonly used skewness measure. However, in order to use it you must know the mean, mode (or median) and standard deviation for your data. Sometimes you might not have that information; instead you might have information about quartiles. If that’s the case, you can use Bowley’s measure of skewness as an alternative to find out more about the asymmetry of your distribution. It’s very useful if you have extreme data values (outliers) or if you have an open-ended distribution.\n\\[{Bowley’s\\ measure\\ of\\ Skewness,\\ S}_{Q} = \\frac{\\left( Q_{3} - Q_{2} \\right) - \\left( Q_{2} - Q_{1} \\right)}{\\left( Q_{3} - Q_{2} \\right) + \\left( Q_{2} - Q_{1} \\right)} \\tag{7.3}\\]\nwhere, \\(Q_{1}\\) = 1st quartile; \\(Q_{2}\\) = median; \\(Q_{3}\\) = 3rd quartile\nEquation can be further modified into\n\\[S_{Q} = \\frac{Q_{3} - 2Q_{2} + Q_{1}}{Q_{3} - Q_{1}} \\tag{7.4}\\]\n\n\\(S_{Q}\\) = 0 means that the curve is symmetrical.\n\\(S_{Q}\\) &gt; 0 means the curve is positively skewed.\n\\(S_{Q}\\)&lt; 0 means the curve is negatively skewed.\n\nLets find Bowley’s measure of skewness for Table 7.1 in Example 7.1 from the cumulative frequency in Table 7.3, quartiles can be calculated. Calculation of \\({Q}_{1}\\), \\(Q_{2}\\), \\(Q_{3}\\) is given in Section 5.6.\n\\[{Q}_{1} = 60\\]\n\\[Q_{2} = 61\\]\n\\[Q_{3} = 63\\]\n\\[S_{Q} = \\frac{63 - (2 \\times 61) + 60}{63 - 60} = \\ \\frac{1}{3} = 0.33\\]\nSince \\(S_{Q}\\) &gt; 0 means the curve is positively skewed.\n\n\n7.2.3 Kelly’s measure of skewness (Sp)\nBowley’s measure of skewness is based on the middle 50% of the observations; it leaves 25% of the observations on each extreme of the distribution. As an improvement over Bowley’s measure, Kelly has suggested a measure based on Percentiles, including P10 and P90 so that only 10% of the observations on each extreme are ignored.\n\\[{Kelly's\\ Measure\\ of\\ Skewness,\\ S}_{p} = \\frac{\\left( P_{90} - P_{50} \\right) - \\left( P_{50} - P_{10} \\right)}{\\left( P_{90} - P_{50} \\right) + \\left( P_{50} - P_{10} \\right)} \\tag{7.5}\\]\n\n\n\n\n\n\nTry yourself\nTry to find Kelly’s measure of skewness for Table 7.1\n\n\n\n\n\n7.2.4 Measure based on moments\nBefore going into measuring skewness using moments, one should know what a moment is:\nMoments\nThe rth moment about mean of a distribution, denoted by \\(\\mu_r\\) is given by\n\\[\\mu_{r} = \\frac{\\sum_{i = 1}^{N}{f_{i}\\left( x_{i} - \\overline{x} \\right)^{r}}}{N} \\tag{7.6}\\]\nwhere, \\(f_{i}\\) is the frequency of ith observation or class mark\\(\\ x_{i}\\), \\(N = \\sum_{}^{}f_{i}\\), number of observations\nMoment about mean is also called as central moment.\nIf r = 0, \\(\\mu_{0} = \\frac{\\sum_{i = 1}^{N}{f_{i}\\left( x_{i} - \\overline{x} \\right)^{0}}}{N}\\) = 1\nIf r = 1, \\(\\mu_{1} = \\frac{\\sum_{i = 1}^{N}{f_{i}\\left( x_{i} - \\overline{x} \\right)^{1}}}{N}\\) = 0 (sum of deviation about mean is zero)\nIf r = 2, \\(\\mu_{2} = \\frac{\\sum_{i = 1}^{N}{f_{i}\\left( x_{i} - \\overline{x} \\right)^{2}}}{N}\\) = \\(\\sigma^{2}\\), Population variance\n\n\n\n\n\n\nNote\n\n\n\nIt should be remembered that first moment about mean is 0 and second moment about mean is variance.\n\n\nFor Table 7.1 in Example 7.1 given above, calculate third central moment, \\(\\mu_{3}\\)\n\n\n\nTable 7.4: Third central moment calculation\n\n\n\n\n\n\nHeight (\\(x_{i}\\))\n\n\nfrequency (\\(f_{i}\\))\n\n\n\\(\\left( x_{i}-\\overline{x} \\right)^{3}\\)\n\n\n\\({f_{i}\\left( x_{i}-\\overline{x}\\right)}^{3}\\)\n\n\n\n\n\n\n58\n\n\n10\n\n\n-39.304\n\n\n-393.040\n\n\n\n\n59\n\n\n18\n\n\n-13.824\n\n\n-248.832\n\n\n\n\n60\n\n\n30\n\n\n-2.744\n\n\n-82.320\n\n\n\n\n61\n\n\n42\n\n\n-0.064\n\n\n-2.688\n\n\n\n\n62\n\n\n35\n\n\n0.216\n\n\n7.560\n\n\n\n\n63\n\n\n28\n\n\n4.096\n\n\n114.688\n\n\n\n\n64\n\n\n16\n\n\n17.576\n\n\n281.216\n\n\n\n\n65\n\n\n8\n\n\n46.656\n\n\n373.248\n\n\n\n\nSum\n\n\n187\n\n\n12.608\n\n\n49.832\n\n\n\n\n\n\n\nMean = 61.40\n\\[\\mu_{3} = \\frac{\\sum_{i = 1}^{N}{f_{i}\\left( x_{i} - \\overline{x} \\right)^{3}}}{N} = \\ \\frac{49.832}{187} = 0.266\\]\nIn short values of following moments about mean are\n\n\n\nTable 7.5: Moments about mean\n\n\n\n\n\nMoments about mean\nValue\n\n\n\n\n\\[\\mu_{0}\\]\n1\n\n\n\\[\\mu_{1}\\]\n0\n\n\n\\[\\mu_{2}\\]\n\\(\\sigma^{2}\\)\n\n\n\n\n\n\n\nBeta one and gamma one\nThe moment measure of skewness is based on the property that, for a symmetrical distribution, all odd ordered central moments are equal to zero. We note that \\(\\mu_{1}\\) = 0, for every distribution, therefore, the lowest order moment that can provide an absolute measure of skewness is \\(\\mu_{3}\\). So measures of skewness are based on \\(\\mu_{3}\\).\n\\[\\beta_{1} = \\frac{\\mu_{3}^{2}}{\\mu_{2}^{3}} \\tag{7.7}\\]\nPronounced as ‘beta one’.\n\\(\\beta_{1}\\)= 0 means that the curve is symmetrical. The greater the value of \\(\\beta_{1}\\) the more skewed the distribution. One serious limitation of \\(\\beta_{1}\\) is that it cannot tell the direction of skewness i.e. whether it is positive or negative. Since \\(\\mu_{2}\\) is always positive (as it is variance) and \\(\\mu_{3}^{2}\\) is positive, \\(\\beta_{1}\\) will be positive always. This drawback is removed by calculating \\(\\text{γ}_{1}\\), called as Karl Pearson’s \\(\\text{γ}_{1}\\), pronounced as ‘gamma one’.\n\\[\\gamma_{1} = \\sqrt{\\beta_{1}} = \\frac{\\mu_{3}}{\\mu_{2}^{3/2}} \\tag{7.8}\\]\nIf \\(\\mu_{3}\\) is positive \\(\\gamma_{1}\\) is positive, If \\(\\mu_{3}\\) is negative \\(\\gamma_{1}\\) is negative\n\n\\(\\gamma_{1}\\)= 0 means that the curve is symmetrical.\n\\(\\gamma_{1}\\) &gt; 0 means the curve is positively skewed.\n\\(\\gamma_{1}\\)&lt; 0 means the curve is negatively skewed.\n\nFor Table 7.1 in Example 7.1, \\(\\beta_{1}\\) and \\(\\gamma_{1}\\) can be calculated as follows\n\\(\\mu_{3}\\) = 0.226\n\\(\\mu_{2}\\) = 3.123\n\\(\\beta_{1} = \\frac{\\mu_{3}^{2}}{\\mu_{2}^{3}}\\) = \\(\\frac{\\left( 0.226 \\right)^{2}}{\\left( 3.123 \\right)^{3}} = \\ \\frac{0.051}{30.46} = 0.0016\\)\n\\(\\gamma_{1} = \\sqrt{\\beta_{1}} = \\ \\sqrt{0.0016} = + 0.04\\)\nSince \\(\\mu_{3}\\) is positive \\(\\gamma_{1}\\)is positive. Since \\(\\gamma_{1}\\) is slightly greater than 0, distribution is a slightly skewed to right.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Skewness and kurtosis</span>"
    ]
  },
  {
    "objectID": "skewness.html#kurtosis",
    "href": "skewness.html#kurtosis",
    "title": "7  Skewness and kurtosis",
    "section": "7.3 Kurtosis",
    "text": "7.3 Kurtosis\nKurtosis is a statistical measure that describes the shape of a distribution’s frequency curve, focusing on its relative peakedness. While skewness measures the asymmetry or lack of symmetry in a distribution, kurtosis evaluates how sharp or flat the peak of the curve is. There are three categories of frequency curves depending upon the shape of their peak as shown in Figure 7.8.\n\n\n\n\n\n\nFigure 7.8: Three categories of frequency curves\n\n\n\nKurtosis refers to degree of flatness or peakedness of the curve. It is measured relative to the peakedness of normal curve. The normal curve is considered as mesokurtic. If a curve is more peaked than normal curve, it is called leptokurtic. If a curve is more flat-topped than normal curve, it is called platykurtic. The condition of peakedness (leptokurtic) or flatness (platykurtic) is called kurtosis of excess.\n\n7.3.1 Measure of kurtosis\nKurtosis is measured using \\(\\beta_{2}\\) ‘beta two’ and \\(\\gamma_{2}\\) ‘gamma two’ given by Karl Pearson\n\\[\\beta_{2} = \\frac{\\mu_{4}}{\\mu_{2}^{2}} \\tag{7.9}\\]\nwhere, \\(\\mu_{4}\\) is the 4th central moment, \\(\\mu_{2}\\) is the 2nd central moment\n\n\\(\\beta_{2}\\) = 3 means that the curve is mesokurtic.\n\\(\\beta_{2}\\) &gt; 3 means the curve is leptokurtic.\n\\(\\beta_{2}\\)&lt; 3 means the curve is platykurtic.\n\n\\[\\gamma_{2} = \\beta_{2} - 3 \\tag{7.10}\\]\n\n\\(\\gamma_{2}\\) = 0 means that the curve is mesokurtic.\n\\(\\gamma_{2}\\) &gt; 0 means the curve is leptokurtic.\n\\(\\gamma_{2}\\)&lt; 0 means the curve is platykurtic.\n\nFor Table 7.1 in Example 7.1, kurtosis can be examined as follows\n\n\n\nTable 7.6: Measures of kurtosis\n\n\n\n\n\n\nHeight (\\(x_{i}\\))\n\n\nfrequency (\\(f_{i}\\))\n\n\n\\(\\left( x_{i}-\\overline{x} \\right)^{4}\\)\n\n\n\\({f_{i}\\left( x_{i}-\\overline{x}\\right)}^{4}\\)\n\n\n\n\n\n\n58\n\n\n10\n\n\n133.634\n\n\n1336.336\n\n\n\n\n59\n\n\n18\n\n\n33.178\n\n\n597.197\n\n\n\n\n60\n\n\n30\n\n\n3.842\n\n\n115.248\n\n\n\n\n61\n\n\n42\n\n\n0.026\n\n\n1.075\n\n\n\n\n62\n\n\n35\n\n\n0.130\n\n\n4.536\n\n\n\n\n63\n\n\n28\n\n\n6.554\n\n\n183.501\n\n\n\n\n64\n\n\n16\n\n\n45.698\n\n\n731.162\n\n\n\n\n65\n\n\n8\n\n\n167.962\n\n\n1343.693\n\n\n\n\nSum\n\n\n187\n\n\n391.021\n\n\n4312.747\n\n\n\n\n\n\n\nMean, \\(\\overline{x}\\) = 61.40\n\\(\\mu_{2}\\) = 3.123 (calculation shown in previous example)\n\\(\\mu_{4} = \\frac{\\sum_{i = 1}^{N}{f_{i}\\left( x_{i} - \\overline{x} \\right)^{4}}}{N} = \\frac{4312.747}{187} = 23.062\\)\n\\(\\beta_{2} = \\frac{\\mu_{4}}{\\mu_{2}^{2}} = \\frac{23.062}{\\left( 3.123 \\right)^{2}} = 2.364\\)\n\\(\\beta_{2}\\) is 2.364, which is close to 3, distribution can be considered slightly platykurtic close to symmetric.\nYou can verify the frequency curve of Example 7.1 Figure 7.9, it can be seen that it is slightly right tailed (positively skewed).\n\n\n\n\n\n\nFigure 7.9: Frequency curve of Example 7.1\n\n\n\n \n \n \n\n\n\n\n\n\nHistorical Insights\n\n\n\n“Crabs and kurtosis”\nThe story of kurtosis and skewness begins with a fascinating scientific journey involving crabs! In the late 1800s, Karl Pearson, a pioneering statistician, worked with biologist Walter Weldon to study variations in the size of crustaceans, like crabs. They noticed that the data didn’t follow the usual normal pattern, so Pearson developed new tools to better understand the shapes of these unusual data distributions.\nHe created the concept of skewness to measure whether the data was symmetrical or had long tails on one side. Then, he developed kurtosis, a measure of how “peaked” or “flat” the data distribution was compared to the normal curve. These ideas helped statisticians better analyze data that didn’t fit the typical patterns, paving the way for modern statistical tools we still use today! (Fiori and Zenga 2009)\n\n\n\n\n\n\n\n\nQuotes to Inspire\n\n\n\n“We are just statistics, born to consume resources” – Horace\n\n\n\n\n\n\n\nFiori, Anna M., and Michele Zenga. 2009. “Karl Pearson and the Origin of Kurtosis.” International Statistical Review 77 (1): 40–50. https://doi.org/10.1111/j.1751-5823.2009.00076.x.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Skewness and kurtosis</span>"
    ]
  },
  {
    "objectID": "association.html",
    "href": "association.html",
    "title": "8  Measures of association",
    "section": "",
    "text": "8.1 Linear and monotonic relationship\nIn the previous chapters, we examined measures of central tendency, which summarize the location of data, and measures of dispersion, which describe the spread around that location. Together, these tools provided us with a foundation for understanding and summarizing a single dataset. However, in many real-world scenarios, we are interested in understanding the relationships between variables rather than focusing on one variable in isolation.\nMeasures of association allow us to explore and quantify the connections between two or more variables. For example, does fertilizer use influence crop yield? Is there a relationship between farm size and agricultural income? Do education levels impact the adoption of new farming technologies? By studying association, we can answer such questions and uncover meaningful patterns in data.\nIn this chapter, we will introduce key measures of association, such as covariance, correlation coefficients, and other related metrics. We will discuss how these measures are calculated, interpreted, and applied to analyze relationships between variables. By the end of this chapter, you will be equipped to assess both the strength and direction of relationships, providing deeper insights into agricultural and social science research.\nLinear relationship\nA linear relationship (or linear association) is a statistical term used to describe a straight-line relationship between variables. Linear relationships can be expressed either in a graphical format where the variable plotted on X-Y plane gives a straight line or relation between two variables (consider x and y) can be expressed with an equation of a straight line (y = a + bx) (will be more clear when we discuss regression in Chapter 9).\nMonotonic relation\nA monotonic relationship between two variables means that as one variable increases or decreases, the other tends to move in the same direction, but not necessarily at a constant rate. In contrast, a linear relationship implies that the variables move in the same direction at a constant rate. While all linear relationships are monotonic, not all monotonic relationships are linear. Refer to the illustration Figure 8.1 for a clearer distinction.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Measures of association</span>"
    ]
  },
  {
    "objectID": "association.html#linear-and-monotonic-relationship",
    "href": "association.html#linear-and-monotonic-relationship",
    "title": "8  Measures of association",
    "section": "",
    "text": "(a) Strong positive linear relationship\n\n\n\n\n\n\n\n\n\n\n\n(b) Strong negative linear relationship\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Weak linear relationship\n\n\n\n\n\n\n\n\n\n\n\n(d) Non-linear relationship\n\n\n\n\n\n\n\n\n\n\n\n\n\n(e) Monotonic relationship\n\n\n\n\n\n\n\nFigure 8.1: Linear and monotonic relationship",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Measures of association</span>"
    ]
  },
  {
    "objectID": "association.html#sec-scatterdiag",
    "href": "association.html#sec-scatterdiag",
    "title": "8  Measures of association",
    "section": "8.2 Scatter diagram",
    "text": "8.2 Scatter diagram\nConsider two variables x and y, a scatter diagram is used to visually investigate whether there is any relationship between them. This graphical method helps explore whether an association exists between the two variables. If the variables x and y are plotted along the X-axis and Y-axis respectively in the X-Y plane of a graph sheet the resultant diagram of dots is known as scatter diagram. From the scatter diagram we can say whether there is any association between x and y. Figure 8.2 gives the scatter diagram of Example 8.1\nExample 8.1: Consider the data on sepal length (x) and sepal width (y) of Iris setosa.\n\n\n\nTable 8.1: Data on sepal length (x) and sepal width (y) of Iris setosa.\n\n\n\n\n\nSepal length (x)\nSepal width (y)\n\n\n\n\n5.1\n3.5\n\n\n4.9\n3\n\n\n4.7\n3.2\n\n\n4.6\n3.1\n\n\n5\n3.6\n\n\n7\n3.2\n\n\n6.4\n3.2\n\n\n6.9\n3.1\n\n\n5.5\n2.3\n\n\n6.5\n2.8\n\n\n6.3\n3.3\n\n\n5.8\n2.7\n\n\n7.1\n3\n\n\n6.3\n2.9\n\n\n6.5\n3\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8.2: Scatter diagram of data in Example 8.1\n\n\n\nExample 8.2: A research station investigates the relationship between the average daily soil moisture content (as influenced by irrigation levels, in percentage) and the corresponding monetary yield (in Rs.) of a crop. The data collected over different periods are as follows:\n\n\n\nTable 8.2: Model data on crop yield influenced by soil moisture\n\n\n\n\n\nSoil moisture (%)\nCrop yield (in Rs./cent)\n\n\n\n\n14.2\n215\n\n\n16.4\n325\n\n\n11.9\n185\n\n\n15.2\n332\n\n\n18.5\n406\n\n\n22.1\n522\n\n\n19.4\n412\n\n\n25.1\n614\n\n\n23.4\n544\n\n\n18.1\n421\n\n\n22.6\n445\n\n\n17.2\n408\n\n\n\n\n\n\nScatter diagram for the Example 8.2 is given in Figure 8.3. You can see a linear association between the two variables i.e. between soil moisture and crop yield in rupees. It can be shown using a line as in Figure 8.4. It is clear that as soil moisture percentage increases crop yield in rupees increases, indicating a positive correlation.\n\n\n\n\n\n\nFigure 8.3: Scatter diagram of data in Example 8.2\n\n\n\n\n\n\n\n\n\nFigure 8.4: Linear relationship between variables\n\n\n\nFrom the examples above it is clear that scatter diagram gives an idea on linear association between variables, so it can also used as a graphical tool to see whether there any association is present or not. But we cannot quantify the association using scatter diagram.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Measures of association</span>"
    ]
  },
  {
    "objectID": "association.html#correlation",
    "href": "association.html#correlation",
    "title": "8  Measures of association",
    "section": "8.3 Correlation",
    "text": "8.3 Correlation\nCorrelation is a statistical technique used to examine the relationship between two or more variables. It quantifies the degree and strength of the linear association between two variables, expressed as a single numerical value.\nThis measure allows us to summarize the extent to which changes in one variable are associated with changes in another. When two or more quantities vary in a related manner, such that movements in one variable are consistently accompanied by movements in the other, the variables are said to be correlated. Based on the nature of relationship, correlation can be classified into three categories positive correlation, negative correlation and no correlation.\nPositive correlation\nPositive correlation refers to a relationship between two variables in which both move in the same direction. A positive correlation exists when an increase in one variable is accompanied by an increase in the other, or when a decrease in one variable corresponds with a decrease in the other.\nExamples of positive correlation:\n\nFertilizer use and crop yield: The more fertilizer (x) a farmer applies to a field, the higher the crop yield (y). Here, as x increases, y also increases.\nRainfall and crop growth: Increased rainfall (x) often leads to better crop growth (y).\nFarm size and agricultural output: Larger farm sizes (x) are associated with greater total agricultural output (y).\nLabor hours and harvest quantity: The more hours spent harvesting (x), the higher the quantity of crops harvested (y).\n\nNegative correlation\nNegative correlation refers to a relationship between two variables in which one variable increases as the other decreases, and vice versa.\nExamples of negative correlation:\n\nWeed density and crop yield: As the density of weeds in a field (x) increases, the crop yield (y) decreases. Here, as x increases, y decreases.\nSoil salinity and plant growth: Higher soil salinity levels (x) result in reduced plant growth (y).\nAge of livestock and milk production: As a cow’s age (x) increases, the amount of milk it produces (y) decreases. Here, as x increases, y decreases.\nPesticide application and pest population: As the amount of pesticide applied (x) increases, the pest population (y) decreases. Here, x increases while y decreases.\n\nNo correlation refers to a statistical relationship where two variables exhibit no apparent association with each other. In other words, changes in one variable do not systematically correspond to changes in the other.\nDirection of correlation can be identified using a scatter diagram as shown below in Figure 8.5\n\n\n\n\n\n\nFigure 8.5: Scatter plot and nature of relationship",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Measures of association</span>"
    ]
  },
  {
    "objectID": "association.html#correlation-types",
    "href": "association.html#correlation-types",
    "title": "8  Measures of association",
    "section": "8.4 Correlation types",
    "text": "8.4 Correlation types\nSimple and multiple\nIn simple correlation the relationship is confined to two variables only. In multiple correlation the relationship between more than two variables is studied.\nLinear and non-linear correlation\nLinear correlation: A type of correlation in which the relationship between two variables can be represented by a straight line. In this case, a change in one variable corresponds to a proportional change in the other, either in a positive or negative direction.\nNon-linear correlation: A type of correlation where the relationship between two variables cannot be represented by a straight line. Instead, the relationship follows a curved pattern, indicating that the variables do not change at a constant rate relative to each other. This is also referred to as curvilinear correlation.\nPartial and total correlation\nPartial correlation: In multiple correlation analysis, partial correlation examines the relationship between two variables after controlling for or eliminating the linear effect of other correlated variables.\nTotal correlation: Total correlation considers the relationship between variables based on all relevant variables without controlling for the influence of any specific variable.\nPartial and multiple correlation are discussed in detail in Section 8.8.\n\n\n\n\n\n\nNote\n\n\n\nPerfect Correlation: If there is any change in the value of one variable, the value of the other variable is changed in a fixed proportion then the correlation between them is said to be in perfect correlation. If there is a perfect correlation, the points will lie in the straight line. If there was a perfect correlation the data will look like in Figure 8.6 below.\n\n\n\n\n\n\nFigure 8.6: Perfect correlation",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Measures of association</span>"
    ]
  },
  {
    "objectID": "association.html#measuring-correlation",
    "href": "association.html#measuring-correlation",
    "title": "8  Measures of association",
    "section": "8.5 Measuring correlation",
    "text": "8.5 Measuring correlation\nWhile a scatter diagram provides a visual representation to examine whether there is an association between two variables, it does not give a precise measure of the strength or direction of the relationship. To understand the degree and nature of the correlation more quantitatively, we use numerical measures. In this section, we discuss various methods to quantify correlation, enabling a more comprehensive and objective analysis of the relationship between variables.\n\n8.5.1 Karl Pearson’s coefficient of correlation\nIt is the most important and widely used measure of correlation. A measure of the intensity or degree of linear relationship between two variables is developed by Karl Pearson, a British Biometrician - known as the Pearson’s correlation coefficient denoted by ‘r’ which is expressed as the ratio of the covariance to the product of the standard deviations of the two variables.\nCovariance\nCovariance is a measure of the joint linear variability of the two variables. Covariance of two variables x and y is denoted as cov(x, y). Covariance measure is used to find correlation coefficient. Consider two variables x and y with n observations each, then covariance is given by Equation 8.1\n\\[cov(x,y)=\\frac{1}{n}\\sum_{i = 1}^{n}{\\left( x_{i} - \\overline{x} \\right)\\left( y_{i} - \\overline{y} \\right)} \\tag{8.1}\\]\nWhen covariance = 0 there is no joint variability or there is no linear relationship. The unit of covariance is the product of the units of the two variables.\nPearson’s correlation coefficient\nAssumptions of Pearson’s correlation coefficient\nTo ensure the validity of Pearson’s correlation coefficient, the following assumptions must be met:\n\nLinearity: The relationship between the two variables must be linear. Pearson’s correlation only measures the strength of linear associations, so it may not accurately describe relationships that are non-linear.\nContinuous data: Both variables should be measured on continuous scales (interval or ratio).\nNormality: Both variables should be approximately normally distributed, especially for small sample sizes. This assumption is less critical for large sample sizes due to the Central Limit Theorem.\nHomoscedasticity: The variability in one variable should remain constant across the range of the other variable. In other words, the scatter of points around the regression line should be uniform.\nIndependence: Observations in the data should be independent of each other.\nNo significant outliers: Outliers can have a disproportionate effect on Pearson’s correlation, potentially distorting the results. An outlier is an observation in a dataset that is significantly different from other observations. It deviates markedly from the overall pattern of the data and may occur due to variability in the data, measurement errors, or rare events.\n\nViolations of these assumptions may lead to incorrect or misleading interpretations of the correlation coefficient. In such cases, alternative methods like Spearman’s correlation discussed in Section 8.5.2 may be more appropriate.\nThe Pearson’s correlation coefficient between the two variables (x and y) is calculated using Equation 8.2. Equation 8.2 can be also written as in Equation 8.3\n\\[r=\\frac{cov(x,y)}{sd(x)sd(y)} \\tag{8.2}\\]\nwhere sd is the standard deviation.\n\\[r = \\frac{\\frac{1}{n}\\sum_{i = 1}^{n}{\\left( x_{i} - \\overline{x} \\right)\\left( y_{i} - \\overline{y} \\right)}}{\\sqrt{\\frac{1}{n}\\sum_{i = 1}^{n}\\left( x_{i} - \\overline{x} \\right)^{2}\\frac{1}{n}\\sum_{i = 1}^{n}\\left( y_{i} - \\overline{y} \\right)^{2}}} \\tag{8.3}\\]\n\n\n\n\n\n\nNote\n\n\n\nProduct-moment correlation\nKarl Pearson’s correlation coefficient (r) is also called the product-moment correlation coefficient because it is calculated using the product of the deviations of the two variables from their respective means. In other words, the Equation 8.3 involves multiplying the deviations of each data point from the mean of its variable and then averaging those products. The term product-moment refers to the multiplication (product) of the deviations (moments) of the variables from their means.\n\n\nA simplified formula for by hand computation of correlation coefficient can be derived by modifying Equation 8.3\n\\[r = \\frac{n\\left( \\sum_{i = 1}^{n}{x_{i}y_{i}} \\right) - \\sum_{i = 1}^{n}{x_{i}\\sum_{i = 1}^{n}y_{i}}}{\\sqrt{\\left\\lbrack n\\sum_{i = 1}^{n}{x_{i}^{2} - \\left( \\sum_{i = 1}^{n}x_{i} \\right)^{2}} \\right\\rbrack\\left\\lbrack n\\sum_{i = 1}^{n}{y_{i}^{2} - \\left( \\sum_{i = 1}^{n}y_{i} \\right)^{2}} \\right\\rbrack}} \\tag{8.4}\\]\nProperties of the correlation coefficient (r)\n\nIt is a pure number independent of both origin and scale of the units of the observations.\nIt always lies between −1 and +1 (absolute value cannot exceed unity).i.e. −1 ≤ r ≤ +1\nr = +1, indicates perfect positive correlation. r = −1, indicates perfect negative correlation. r = 0, indicates no correlation.\nWhen the correlation is zero then there is no linear relationship between the variables.\nKarl Pearson’s correlation coefficient is also called as product-moment correlation coefficient.\nTwo independent variables are always uncorrelated, meaning that there is no relationship between them. However, the reverse is not always true. Just because two variables are uncorrelated doesn’t mean they are independent. Uncorrelated variables may still have a relationship, but the relationship might be non-linear, which correlation cannot capture. Independence implies no relationship in any form (linear or non-linear), but uncorrelated means there’s no linear relationship between them.\n\n\n\n\n\n\n\nNote\n\n\n\nSpurious correlation\nWhen we calculate the correlation between two variables, we often get a numerical value that quantifies the strength and direction of the relationship between them. However, if there is no actual meaningful relationship between these variables, the correlation value obtained may be misleading. For example, even if there’s no practical or causal link, we might find a correlation between variables like “fertilizer price” and “Kohli’s batting average.” Despite the absence of any real-world connection between these two variables, we can still compute a correlation value. This type of correlation is known as a spurious correlation, which means it exists purely due to random chance or statistical artifacts rather than any practical or causal relationship.\n\n\nExample 8.3: Consider the Table 8.2 in Example 8.2; find correlation coefficient (r)\n\n\n\nTable 8.3: Calculation of Pearson’s correlation coefficient for Table 8.2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSl. No.\nMoisture \\((x_i)\\)\nCrop yield \\((y_i)\\)\n\\(x_{i}-\\overline{x}\\)(1)\n\\(y_{i}-\\overline{y}\\)(2)\n(1).(2)\n\\((x_{i}-\\overline{x})^{2}\\)\n\\((y_{i}-\\overline{y})^{2}\\)\n\n\n1\n14.2\n215\n-4.48\n-187.42\n838.69\n20.03\n35125.01\n\n\n2\n16.4\n325\n-2.28\n-77.42\n176.12\n5.18\n5993.34\n\n\n3\n11.9\n185\n-6.78\n-217.42\n1473\n45.90\n47270.01\n\n\n4\n15.2\n332\n-3.48\n-70.42\n244.70\n12.08\n4958.51\n\n\n5\n18.5\n406\n-0.18\n3.58\n-0.63\n0.031\n12.84\n\n\n6\n22.1\n522\n3.43\n119.58\n409.57\n11.73\n14300.17\n\n\n7\n19.4\n412\n0.73\n9.58\n6.9479\n0.53\n91.84\n\n\n8\n25.1\n614\n6.43\n211.58\n1359.42\n41.28\n44767.51\n\n\n9\n23.4\n544\n4.73\n141.58\n668.98\n22.33\n20045.84\n\n\n10\n18.1\n421\n-0.58\n18.58\n-10.69\n0.33\n345.34\n\n\n11\n22.6\n445\n3.93\n42.58\n167.14\n15.41\n1813.34\n\n\n12\n17.2\n408\n-1.48\n5.58\n-8.24\n2.16\n31.17\n\n\nTotal\n224.1\n4829\n0\n0\n5325.03\n176.98\n174754.9\n\n\n\n\n\n\nn =12\n\\[mean,\\overline{x} = \\ \\frac{224.1}{12} = 18.68\\]\n\\[mean,\\overline{y} = \\ \\frac{4829}{12} = 402.42\\] cov (x,y) = \\(\\frac{1}{n}\\sum_{i = 1}^{n}{\\left( x_{i} - \\overline{x} \\right)\\left( y_{i} - \\overline{y} \\right)}\\)\n\\(\\sum_{i = 1}^{12}{\\left( x_{i} - \\overline{x} \\right)\\left( y_{i} - \\overline{y} \\right)} = 5325.03\\)\nCov (x,y) = \\(\\frac{5325.03}{12} = 443.75\\)\n\\[Standard\\ deviation,\\ S.D\\left( x \\right) = \\ \\sqrt{\\frac{1}{n}\\sum_{i = 1}^{n}\\left( x_{i} - \\overline{x} \\right)^{2}} = \\sqrt{\\frac{176.98}{12}} = 3.84\\]\n\\[Standard\\ deviation,\\ S.D\\left( y \\right) = \\ \\sqrt{\\frac{1}{n}\\sum_{i = 1}^{n}\\left( y_{i} - \\overline{y} \\right)^{2}} = \\sqrt{\\frac{174754.9}{12}} = 120.68\\]\nUsing Equation 8.2, \\(r = \\frac{443.75}{3.84\\  \\times 120.68} = 0.96\\), which indicates a strong positive correlation\n\n\n8.5.2 Spearman’s rank order correlation coefficient\nThe Spearman’s correlation coefficient evaluates the strength and direction of a monotonic relationship between two variables, whether they are continuous or ordinal. Spearman’s correlation is denoted by a Greek letter \\(\\rho\\) pronounced as “rho”. The range of Spearman’s rank correlation also lies between −1 and +1 always, i.e. −1 ≤ ρ ≤+1\nUnlike Pearson’s correlation, which measures linear relationships, Spearman’s correlation is based on the ranked values of the variables rather than their raw data. This makes it particularly useful in situations where:\n\nThe relationship between variables is non-linear but monotonic.\nThe data contains outliers or is not normally distributed, as ranking reduces the influence of extreme values.\nVariables are measured on an ordinal, interval, or ratio scale.\n\nSpearman’s correlation is a more robust alternative to Pearson’s when the assumptions of Pearson’s correlation coefficient are violated.\nThere are two cases in calculating \\(\\rho\\) :\n\nNo tied rank case\nTied rank case\n\nNo tied rank case\nWhen two or more distinct observations have the same value, thus being given the same rank, they are said to be tied. The formula for the Spearman rank correlation coefficient when there are no tied ranks is:\n\\[\\rho = 1 - \\frac{6\\sum_{i = 1}^{n}d_{i}^{2}}{n\\left( n^{2} - 1 \\right)} \\tag{8.5}\\]\nwhere \\(d_{i}\\) is the difference between ranks of ith pair of observation\nExample 8.4: Calculation of Spearman’s rank correlation when there is no tied rank is explained step by step by using the simple example below\nThe scores for nine students in physics and mathematics are as follows:\nPhysics: 35, 23, 47, 17, 10, 43, 9, 6, 28\nMathematics: 30, 33, 45, 23, 8, 49, 12, 4, 31\nCompute the student’s ranks in the two subjects and compute the Spearman rank correlation.\n\n\n\nTable 8.4: Non tied rank case example dataset\n\n\n\n\n\nPhysics\nMathematics\n\n\n\n\n35\n30\n\n\n23\n33\n\n\n47\n45\n\n\n17\n23\n\n\n10\n8\n\n\n43\n49\n\n\n9\n12\n\n\n6\n4\n\n\n28\n31\n\n\n\n\n\n\nStep 1: Find the ranks for each individual subject. Rank the scores from greatest to smallest; assign the rank 1 to the highest score, 2 to the next highest and so on:\n\n\n\nTable 8.5: Non tied rank case calculation table1\n\n\n\n\n\nPhysics (x)\nRankx\nMathematics (y)\nRanky\n\n\n\n\n35\n3\n30\n5\n\n\n23\n5\n33\n3\n\n\n47\n1\n45\n2\n\n\n17\n6\n23\n6\n\n\n10\n7\n8\n8\n\n\n43\n2\n49\n1\n\n\n9\n8\n12\n7\n\n\n6\n9\n4\n9\n\n\n28\n4\n31\n4\n\n\n\n\n\n\nStep 2: Add a column d, to your data. The d is the difference between ranks.\nd = Rankx – Ranky\nFor example, the first student’s physics rank is 3 and math rank is 5, so the difference is -2. In the next column, square your d values.\n\n\n\nTable 8.6: Non tied rank case calculation table2\n\n\n\n\n\n\n\n\n\n\n\n\n\nPhysics (x)\nRankx\nMathematics (y)\nRanky\nd\nd2\n\n\n\n\n35\n3\n30\n5\n-2\n4\n\n\n23\n5\n33\n3\n2\n4\n\n\n47\n1\n45\n2\n-1\n1\n\n\n17\n6\n23\n6\n0\n0\n\n\n10\n7\n8\n8\n-1\n1\n\n\n43\n2\n49\n1\n1\n1\n\n\n9\n8\n12\n7\n1\n1\n\n\n6\n9\n4\n9\n0\n0\n\n\n28\n4\n31\n4\n0\n0\n\n\nTotal\n\n\n\n\n12\n\n\n\n\n\n\nStep 4: Sum (add up) all of your d2 values. \\(\\sum_{i = 1}^{n}d_{i}^{2} =\\) 4 + 4 + 1 + 0 + 1 + 1 + 1 + 0 + 0 = 12.\nStep 5: Insert the values into Equation 8.5\n\\[\\rho = 1 - \\frac{6 \\times 12}{9\\left( 81 - 1 \\right)} = 0.90\\]\nThe Spearman’s rank correlation for this set of data is 0.90. This indicates there is a high correlation between the marks of physics and mathematics in the sample.\nTied rank case\nWhen two or more data points have the same value, same ranks were given to these data points and a tied rank case occurs. When there are tied ranks, the formula for calculating \\(\\rho\\) is given below \\[\\rho = 1 - \\frac{6\\left( \\sum_{i = 1}^{n}d_{i}^{2} + T_{x} + T_{y} \\right)}{n\\left( n^{2} - 1 \\right)} \\tag{8.6}\\]\nIf there are m individuals tied (having same rank), and s such sets of ranks are there in X- series then, \\[T_{x} = \\ \\frac{1}{12}\\sum_{i = 1}^{s}{m_{i}\\left( m_{i}^{2} - 1 \\right)} \\tag{8.7}\\]\nIf there are w individuals tied (having same rank), and s’ such sets of ranks are there in Y- series then, \\[T_{y} = \\ \\frac{1}{12}\\sum_{i = 1}^{s'}{w_{i}\\left( w_{i}^{2} - 1 \\right)} \\tag{8.8}\\]\nCalculation of Spearman’s rank correlation when there is tied rank is explained step by step by using the example below\nExample 8.5: The scores for nine students in physics and mathematics are as follows:\n\n\n\nTable 8.7: Tied rank case example dataset\n\n\n\n\n\nPhysics (x)\nMathematics (y)\n\n\n\n\n35\n30\n\n\n23\n33\n\n\n47\n45\n\n\n23\n23\n\n\n10\n8\n\n\n43\n49\n\n\n9\n12\n\n\n6\n33\n\n\n28\n33\n\n\n\n\n\n\nStep 1: Consider the marks in Physics, ranked as usual without considering the repeated value. Here you can see 23 is repeated but first value is given rank 5 and second repeated value is given the next rank 6.\n\n\n\nTable 8.8: Tied rank case calculation table-1\n\n\n\n\n\nPhysics (x)\nRank\n\n\n\n\n35\n3\n\n\n23\n5\n\n\n47\n1\n\n\n23\n6\n\n\n10\n7\n\n\n43\n2\n\n\n9\n8\n\n\n6\n9\n\n\n28\n4\n\n\n\n\n\n\nThen the average of two ranks 5 and 6 is assigned to both the values; \\(\\left( \\frac{5 + 6}{2}\\right)\\) = 5.5\n\n\n\nTable 8.9: Tied rank case calculation table-2\n\n\n\n\n\nPhysics (x)\nRank\n\n\n\n\n35\n3\n\n\n23\n5.5\n\n\n47\n1\n\n\n23\n5.5\n\n\n10\n7\n\n\n43\n2\n\n\n9\n8\n\n\n6\n9\n\n\n28\n4\n\n\n\n\n\n\nSimilarly for marks in mathematics you can see 33 is repeated thrice.\n\n\n\nTable 8.10: Tied rank case calculation table-3\n\n\n\n\n\nMathematics (y)\nRank\n\n\n\n\n30\n6\n\n\n33\n3\n\n\n45\n2\n\n\n23\n7\n\n\n8\n9\n\n\n49\n1\n\n\n12\n8\n\n\n33\n4\n\n\n33\n5\n\n\n\n\n\n\nYou can see the value 33 is repeated thrice, so the average of three ranks 3, 4 and 5 is given \\(\\left( \\frac{3 + 4 + 5}{3} \\right)\\) = 4\n\n\n\nTable 8.11: Tied rank case calculation table-4\n\n\n\n\n\nMathematics (y)\nRank\n\n\n\n\n30\n6\n\n\n33\n4\n\n\n45\n2\n\n\n23\n7\n\n\n8\n9\n\n\n49\n1\n\n\n12\n8\n\n\n33\n4\n\n\n33\n4\n\n\n\n\n\n\nStep 2: Calculate \\(T_{x}\\) and \\(T_{y}\\)\nIn our example marks in Physics (x) there are two 23 values tied therefore m = 2; since only one such a set is there s = 1. Applying these values in Equation 8.7 \\(T_{x} = \\ \\frac{1}{12}\\left( 2 \\times (2^{2} - 1 \\right)\\) = 0.5\nIn our example marks in Mathematics (y) there are three 33 values tied therefore w = 3; since only one such a set is there s = 1. Applying these values in Equation 8.8 \\(T_{y} = \\ \\frac{1}{12}\\left( 3 \\times (3^{2} - 1 \\right)\\) = 2\nStep 2: Calculate d and then use the Equation 8.6\n\n\n\nTable 8.12: Tied rank case calculation table-5\n\n\n\n\n\nPhysics (x)\nRank\nMathematics (y)\nRank\nd\nd2\n\n\n\n\n35\n3\n30\n6\n-3\n9\n\n\n23\n5.5\n33\n4\n1.5\n2.25\n\n\n47\n1\n45\n2\n-1\n1\n\n\n23\n5.5\n23\n7\n-1.5\n2.25\n\n\n10\n7\n8\n9\n-2\n4\n\n\n43\n2\n49\n1\n1\n1\n\n\n9\n8\n12\n8\n0\n0\n\n\n6\n9\n33\n4\n5\n25\n\n\n28\n4\n33\n4\n0\n0\n\n\nTotal\n\n\n\n0\n44.5\n\n\n\n\n\n\nusing Equation 8.6, \\(\\rho = 1 - \\frac{6 \\times \\left( 44.5 + 0.5 + 2 \\right)}{9\\left( 9^{2} - 1 \\right)} = \\ 1 - \\frac{282}{720}\\) = 0.61\nA Spearman rank correlation of 0.61 indicates a moderately strong positive monotonic relationship between the two variables.\n\n\n\n\n\n\nNote\n\n\n\nSpearman’s rank correlation is a non-parametric method, meaning it does not rely on assumptions about the data’s distribution and is suitable for ordinal data or non-linear relationships. In contrast, Karl Pearson’s correlation is a parametric method that assumes the data follows a normal distribution and is used for continuous variables with a linear relationship. Parametric methods are more sensitive to outliers and deviations from assumptions, while non-parametric methods are more robust and versatile for different types of data.\n\n\n\n\n8.5.3 Kendall’s Rank Correlation Coefficient\nKendall’s rank correlation coefficient, also known as Kendall’s \\(\\tau\\) or the coefficient of concordance, is a measure of association that evaluates the degree of agreement between ranked variables. It ranges from 0 to 1 i.e. \\(0\\leq \\tau \\leq1\\), where \\(\\tau = 0\\): indicates no agreement. \\(\\tau = 1\\): indicates perfect agreement.\nWhen there are multiple sets of rankings (k sets), Kendall’s coefficient of concordance (\\(\\tau\\)) can be used to assess the association among them. This measure is particularly useful in evaluating the reliability or consistency of scores assigned by multiple judges or raters. By quantifying the agreement across different rankings, it provides a robust way to analyze consensus in subjective evaluations or repeated assessments.\nTo calculate Kendall’s coefficient of concordance (\\(\\tau\\)), the data is organized into a table where each row represents the ranks assigned by a different judge to the same set of n objects. Each column in the table corresponds to the ranks given by a judge to n objects.\nFor k judges, there will be k sets of rankings for each object. The table should look like this:\n\n\n\nTable 8.13: Model table for the calculation of Kendall’s \\(\\tau\\)\n\n\n\n\n\nObject\nJudge 1\nJudge 2\n…\nJudge k\n\n\n\n\n1\nR11\nR12\n…\nR1k\n\n\n2\nR21\nR22\n…\nR2k\n\n\n…\n…\n…\n…\n…\n\n\nn\nRn1\nRn2\n…\nRnk\n\n\n\n\n\n\nIn Table 8.13 each row represents the ranks assigned to an object by all (k) judges. Each column represents the ranks for each object assigned by a particular judge. \\(R_{ij}\\) is the rank assigned to the ith object by the jth object, where i = 1, 2, …, n and j = 1, 2, …, k.\nKendall’s coefficient of concordance (\\(\\tau\\)) is calculated using Equation 8.9.\n\\[\\tau = \\frac{12\\left\\lbrack \\sum_{i = 1}^{n}{R_{i}^{2} - \\frac{\\left( \\sum_{i = 1}^{n}R_{i} \\right)^{2}}{n}} \\right\\rbrack}{k^{2}n\\left( n^{2} - 1 \\right)} \\tag{8.9}\\]\nwhere \\(R_{i} =\\sum_{j = 1}^{k}{R_{ij}}\\), i.e. the sum of ranks obtained by each object. Calculation of Kendall’s \\(\\tau\\) is explained in Example 8.6\nExample 8.6: In a crop production competition, 10 entries of farmers were ranked by agricultural scientists (judges). Find the degree of agreement among the scientist for the competition result given below.\n\n\n\nTable 8.14: Rankings of 10 farmers by agricultural scientists\n\n\n\n\n\nFarmers\nScientist 1\nScientist 2\nScientist 3\nScientist 4\n\n\n1\n4\n5\n3\n7\n\n\n2\n10\n9\n8\n6\n\n\n3\n8\n6\n10\n9\n\n\n4\n3\n4\n2\n1\n\n\n5\n1\n3\n4\n2\n\n\n6\n2\n1\n1\n4\n\n\n7\n5\n7\n6\n5\n\n\n8\n6\n2\n5\n3\n\n\n9\n7\n8\n9\n10\n\n\n10\n9\n10\n7\n8\n\n\n\n\n\n\nSolution 8.6\n\n\n\nTable 8.15: Calculation of Kendall’s \\(\\tau\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFarmers\nS1\nS2\nS3\nS4\n\\(R _{i}\\) (sum of ranks)\n\\(R _ {i} ^ {2}\\)\n\n\n\n\n1\n4\n5\n3\n7\n19\n361\n\n\n2\n10\n9\n8\n6\n33\n1089\n\n\n3\n8\n6\n10\n9\n33\n1089\n\n\n4\n3\n4\n2\n1\n10\n100\n\n\n5\n1\n3\n4\n2\n10\n100\n\n\n6\n2\n1\n1\n4\n8\n64\n\n\n7\n5\n7\n6\n5\n23\n529\n\n\n8\n6\n2\n5\n3\n16\n256\n\n\n9\n7\n8\n9\n10\n34\n1156\n\n\n10\n9\n10\n7\n8\n34\n1156\n\n\nTotal\n\n\n\n\n220\n5900\n\n\n\n\n\n\nhere k = number of judges = 4; n = number of farmers = 10; \\(\\sum_{i = 1}^{10}R_{i}^{2}\\) = (220)2 = 48400; \\(\\sum_{i = 1}^{10}R_{i}^{2}\\) = 5900. Using Equation 8.9\n\\[\\tau = \\frac{12\\left\\lbrack 5900 - \\frac{48400}{10} \\right\\rbrack}{16 \\times 10\\left( 100 - 1 \\right)} = 0.80\\]\nA Kendall’s \\(\\tau\\) of 0.80 suggests that the judges’ rankings are highly consistent. This level of concordance implies that the rankings are strongly aligned, indicating a high level of agreement across the judges’ evaluations.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Measures of association</span>"
    ]
  },
  {
    "objectID": "association.html#correlation-matrix",
    "href": "association.html#correlation-matrix",
    "title": "8  Measures of association",
    "section": "8.6 Correlation matrix",
    "text": "8.6 Correlation matrix\nA correlation matrix is a table that displays the correlation coefficients between multiple variables. It is particularly useful for presenting the correlation values of several variables at the same time. The matrix can be computed using either Pearson’s correlation or Spearman’s correlation. Each cell in the matrix represents the correlation coefficient between the two variables at the intersecting row and column. These values range from -1 to 1. The diagonal elements are always equal to 1, as a variable is perfectly correlated with itself. Additionally, the matrix is symmetrical across the diagonal, since the correlation between x and y is the same as the correlation between y and x.\nExample 8.7: Data below gives the measurement on several plant growth characters. Create a correlation matrix.\n\n\n\nTable 8.16: Plant growth characters\n\n\n\n\n\nGrowth\nWater\nSunlight\nFertilizer\nNutrient\n\n\n\n\n4.37\n167.55\n11.08\n56.60\n243.43\n\n\n9.56\n325.36\n6.71\n40.68\n519.04\n\n\n7.59\n231.40\n5.65\n35.86\n455.06\n\n\n6.39\n209.62\n14.49\n78.06\n342.38\n\n\n2.40\n77.79\n14.66\n81.01\n127.70\n\n\n2.40\n115.16\n13.08\n70.34\n148.98\n\n\n1.52\n76.76\n8.05\n45.48\n92.12\n\n\n8.80\n280.54\n5.98\n34.18\n532.97\n\n\n6.41\n195.48\n11.84\n59.45\n401.31\n\n\n7.37\n236.65\n9.40\n48.08\n431.84\n\n\n1.19\n51.96\n6.22\n31.41\n146.65\n\n\n9.73\n328.38\n9.95\n56.11\n566.87\n\n\n8.49\n286.58\n5.34\n29.84\n443.16\n\n\n2.91\n131.66\n14.09\n75.54\n234.76\n\n\n2.64\n102.81\n7.59\n47.03\n185.93\n\n\n\n\n\n\n\n\nCorrelation matrix for Table 8.16 can be created as shown in Table 8.17 below. Here the diagonal elements are 1 because correlation of any variable to itself is 1. Also from the matrix below it is easy to find the correlation between any two variables, for example correlation between growth and water is 0.988. Correlation matrix is an effective way of presenting correlation results of several variables.\n\n\n\nTable 8.17: Correlation matrix of data in Table 8.16\n\n\n\n\n\n\nGrowth\nWater\nSunlight\nFertilizer\nNutrient\n\n\n\n\nGrowth\n1\n0.988\n-0.343\n-0.332\n0.983\n\n\nWater\n0.988\n1\n-0.328\n-0.314\n0.967\n\n\nSunlight\n-0.343\n-0.328\n1\n0.986\n-0.371\n\n\nFertilizer\n-0.332\n-0.314\n0.986\n1\n-0.366\n\n\nNutrient\n0.983\n0.967\n-0.371\n-0.366\n1",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Measures of association</span>"
    ]
  },
  {
    "objectID": "association.html#correlogram",
    "href": "association.html#correlogram",
    "title": "8  Measures of association",
    "section": "8.7 Correlogram",
    "text": "8.7 Correlogram\nA correlogram is a graphical representation of a correlation matrix. It is used to visualize the pairwise correlation coefficients between multiple variables in a dataset. The correlogram uses color and size to represent the strength and direction of the correlation, helping to quickly identify relationships among variables. The most common way to display a correlogram is through a matrix of circles or squares, where each shape represents the correlation between two variables. Correlogram can be drawn using packages like corrplot (Wei and Simko 2021) in R software (Team 2024). Different colours and styles are available to make the presentation attractive. Correlation matrix and correlogram can also be generated using our online platform available at www.kaugrapes.com (Pratheesh P. Gopinath 2020).\n\n\n\nCorrelogram of growth characters in Example 8.7",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Measures of association</span>"
    ]
  },
  {
    "objectID": "association.html#sec-partialmulti",
    "href": "association.html#sec-partialmulti",
    "title": "8  Measures of association",
    "section": "8.8 Partial and multiple correlation",
    "text": "8.8 Partial and multiple correlation\n\n\n\n\n\n\nHistorical Insights\n\n\n\nCorrelation and Sir Francis Galton\nThe concept of correlation dates back to Sir Francis Galton, who introduced the idea of “co-relation” in the 19th century while studying the relationship between physical traits, such as the height of parents and their children. Galton’s cousin, Karl Pearson, further developed this idea by formalizing the calculation of correlation and introducing the formula we use today in terms of Pearson’s correlation coefficient. Pearson’s work in the late 1800s provided a mathematical framework for understanding the strength and direction of relationships between two variables, a concept that has since become fundamental in statistics, especially in the fields of genetics, psychology, and social sciences.\n\n\n\n\n\n\n\n\nQuotes to Inspire\n\n\n\n“Statistics is like a high-caliber weapon: helpful when used correctly and potentially disastrous in the wrong hands”.- Herman Chernoff\n\n\n\n\n\n\n\nPratheesh P. Gopinath, Brigit Joseph, Rajender Parsad. 2020. GRAPES: General r-Shiny Based Analysis Platform Empowered by Statistics (version 1.0.0). https://doi.org/10.5281/zenodo.4923220.\n\n\nTeam, R Core. 2024. “R: A Language and Environment for Statistical Computing.” R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nWei, Taiyun, and Viliam Simko. 2021. “Corrplot: Visualization of a Correlation Matrix.” https://cran.r-project.org/web/packages/corrplot/corrplot.pdf.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Measures of association</span>"
    ]
  },
  {
    "objectID": "regression.html",
    "href": "regression.html",
    "title": "9  Regression analysis",
    "section": "",
    "text": "9.1 Simple linear regression\nRegression analysis is one of the most important tools in statistics, used to understand and quantify the relationships between variables. In essence, regression helps us answer questions such as, “How does a change in one factor (like fertilizer usage) affect another factor (like crop yield)?” It provides a mathematical framework to explore these relationships based on observed data.\nRegression analysis involves two types of variables:\nWhy use regression analysis?\nRegression analysis is particularly useful because it allows you to:\nTypes of regression\nThere are different types of regression techniques, depending on the nature of the data and the relationship between variables:\nPractical applications\nRegression analysis has a wide range of applications across fields:\nBy the end of this chapter, you will learn how to perform regression analysis, interpret its results, and understand its assumptions and limitations. This will enable you to use regression as a powerful tool for making informed decisions and predictions.\nRegression can be simply defined as a technique of fitting best line or line of best fit to estimate value of one variable on the basis of another variable. Now what is a best line? or line of best fit?. To understand this concept better, consider the data presented in Table 8.2, which shows the average daily soil moisture content and the corresponding monetary yield from crops in Example 8.2 of Section 8.2. This example helps visualize how the relationship between two variables—soil moisture content (independent variable) and crop yield (dependent variable).\nWe can use regression analysis to answer the following questions. What will be the crop yield in rupees when soil moisture content is maintained at 20%?. What is the functional form of relationship between soil moisture content and monetary crop yield?.\nRefer to the scatter diagram of Table 8.2 in Figure 8.3. To represent the relationship between soil moisture content and monetary crop yield, we might attempt to draw a line, as illustrated in Figure 9.1. However, as shown in Figure 9.1, it’s possible to draw numerous lines through the data points. This raises the question: which line is the best fit?\nThe best fit line is defined as the one that minimizes the distances between the observed data points and the line itself. These distances, which represent how far each data point is from the line, are minimized collectively using a specific criterion. The regression technique provides a systematic approach to determine and draw this best fit line. Before diving further into regression, it is essential to understand the concepts of error and residuals, which play a critical role in determining the best fit line. The entire topic of regression is on how to draw a best fit line?.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Regression analysis</span>"
    ]
  },
  {
    "objectID": "regression.html#simple-linear-regression",
    "href": "regression.html#simple-linear-regression",
    "title": "9  Regression analysis",
    "section": "",
    "text": "(a)\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\nFigure 9.1: lines drawn to show functional relationship between soil moisture and yield\n\n\n\n\n\n9.1.1 Error and residual\nIn regression analysis, an error represents the difference between an observed value (a data point) and the true regression line, which reflects the actual relationship between the dependent and independent variables in the population. Since the true regression line is based on the entire population and is usually unknown, the error is a theoretical concept that cannot be directly calculated.\nA residual is the difference between an observed value and the value predicted by a regression line based on sample data. Specifically, for a given data point, the residual is calculated as the observed value minus the predicted value from the regression line. Residuals are measurable because they are derived from the observed data and the regression line that is obtained using the sample. Unlike errors, which are theoretical and represent deviations from the true underlying model, residuals provide a practical estimate of these deviations, allowing us to assess the goodness of fit and identify any patterns or discrepancies in the model.\nIn essence, a residual serves as an estimate of the error. While errors represent the deviation from the actual true value, residuals reflect the discrepancy between observed data and the fitted model. From Figure 9.2 you can see the residual \\(e_i\\) of an i^th observation in a fitted regression line.\n\n\n\n\n\n\nFigure 9.2: Residual and a best fit line\n\n\n\nThe distance of \\(i^{th}\\) observation (\\(e_i\\)) from the fitted line can be considered as the residual (error). Best fit line can be obtained by minimizing this distance. This can be achieved using the mathematical technique “principle of least squares” discussed in Section 9.1.3. Before going to identify a best fit line on should know the concept of a straight line.\n\n\n9.1.2 Straight lines\nA straight line is the simplest figure in geometry. Mathematical equation of a straight line is\n\\[Y = \\alpha + \\beta X \\tag{9.1}\\]\nTwo important features of a line intercept (\\(\\alpha\\)) and slope(\\(\\beta\\)). \\(\\alpha\\) is the Y-intercept, the intercept of a line is the y-value of the point where it crosses the y-axis. \\(\\beta\\) is the slope of a line, which is a number that measures its “steepness”. It is the change in Y for a unit change in X along the line. In regression \\(\\beta\\) is called as regression coefficient explained in Section 9.1.4.\nIntercept and slope\n\n\n\n\n\n\n\n\n\n\n\n(a) Intercept of a straight line\n\n\n\n\n\n\n\n\n\n\n\n(b) Slope of a straight line\n\n\n\n\n\n\n\nFigure 9.3: Intercept and slope\n\n\n\n\\(\\alpha\\) and \\(\\beta\\) can be considered as a finger print of a line; with these values we can easily identify the line. So now our problem is simple, to find a line of best, estimate \\(\\alpha\\) and \\(\\beta\\), such that error ei of each observation is minimized. For that we use the method of least squares.\n\n\n9.1.3 Method of least squares\nOn considering the error term \\(e_i\\); equation of a straight line is\n\\[y_i = \\alpha +\\beta x_i + e_i \\tag{9.2}\\]\nWhere \\(e_i\\) is the ith error term corresponding to \\(y_i\\), i =1, 2, …, n\n\n\n\n\n\n\nNote\n\n\n\nOne way to obtain line of best fit is by estimating \\(\\alpha\\) and \\(\\beta\\) by minimizing error sum \\(\\sum_{i = 1}^{n}{e_i}\\). By theorem \\(\\sum_{i = 1}^{n}{e_i} = 0\\). So \\(\\alpha\\) and \\(\\beta\\) are estimated by minimizing \\(\\sum_{i = 1}^{n}{e_i}^2\\). The term \\(\\sum_{i = 1}^{n}e_i^2\\) is called as error sum of squares. As we are minimizing the sum of the squares of error term the process is known by the name principle of least squares.\n\n\nPrinciple of least squares\nPrinciple of least squares is the statistical method used to determine a line of best fit by minimizing the sum of squares of the error term i.e minimizing \\(\\sum_{i = 1}^{n}{e_i}^2\\).\nConsider Equation 9.2\n\\[y_i = \\alpha +\\beta x_i + e_i\\]\n\\[e_i = y_i -(\\alpha +\\beta x_i) \\tag{9.3}\\]\n\\[e_i^2 = [y_i -(\\alpha +\\beta x_i)]^2 \\tag{9.4}\\]\n\\[\\sum_{i = 1}^{n}e_i^2 = \\sum_{i = 1}^{n}[y_i -(\\alpha +\\beta x_i)]^2 \\tag{9.5}\\]\nwe want to minimize Equation 9.5 and estimate \\(\\alpha\\) and \\(\\beta\\). \\(\\sum_{i = 1}^{n}e_i^2\\) can be minimized by taking derivative with respect to \\(\\alpha\\) and \\(\\beta\\) and equating to zero. On doing so we will get two equations, these equations are termed as normal equations and solving those normal equations will give the formulas for estimating \\(\\alpha\\) and \\(\\beta\\).\nDifferentiating \\(E=\\sum_{i = 1}^{n}e_i^2\\) with respect to \\(\\alpha\\) and equating to 0.\n\\[\\frac{\\partial E}{\\partial \\alpha} = \\sum_{i=1}^n 2 \\left[ y_i - (\\alpha + \\beta x_i) \\right](-1) \\tag{9.6}\\]\n\\[= -2 \\sum_{i=1}^n \\left[ y_i - \\alpha - \\beta x_i \\right] \\tag{9.7}\\]\nequating the derivative in Equation 9.7 to \\(0\\) and on simplifying:\n\\[\\sum_{i=1}^n \\left[ y_i - \\alpha - \\beta x_i \\right] = 0 \\tag{9.8}\\]\nexpand the summation in Equation 9.8:\n\\[=\\sum_{i=1}^n y_i - n\\alpha - \\beta \\sum_{i=1}^n x_i = 0 \\tag{9.9}\\]\non rearranging Equation 9.9 you will get the first normal equation.\n\\[\\sum_{i=1}^n y_i =n\\alpha + \\beta \\sum_{i=1}^n x_i \\tag{9.10}\\]\nDifferentiating \\(E=\\sum_{i = 1}^{n}e_i^2\\) with respect to \\(\\beta\\):\n\\[\\frac{\\partial E}{\\partial \\beta} = \\sum_{i=1}^n 2 \\left[ y_i - (\\alpha + \\beta x_i) \\right](-x_i) \\tag{9.11}\\]\n\\[= -2 \\sum_{i=1}^n x_i \\left[ y_i - \\alpha - \\beta x_i \\right] \\tag{9.12}\\]\nequating the derivative in Equation 9.12 to \\(0\\) and on simplifying:\n\\[\\sum_{i=1}^n x_i \\left[ y_i - \\alpha - \\beta x_i \\right] = 0 \\tag{9.13}\\]\nexpand the summation in Equation 9.13:\n\\[=\\sum_{i=1}^n x_i y_i - \\alpha \\sum_{i=1}^n x_i - \\beta \\sum_{i=1}^n x_i^2 = 0 \\tag{9.14}\\]\non rearranging Equation 9.14 you will get the second normal equation.\n\\[\\sum_{i=1}^n y_i x_i = \\alpha \\sum_{i=1}^n x_i + \\beta \\sum_{i=1}^n x_i^2 \\tag{9.15}\\]\nOn solving normal equations Equation 9.10 and Equation 9.15, we derive the equations to estimate \\(\\alpha\\) and \\(\\beta\\), which are considered population parameters. Since these parameters are usually unknown, we estimate them using equations derived from sample data. The estimated values of \\(\\alpha\\) and \\(\\beta\\) are denoted as \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\), where the hats indicate that they are sample-based estimates. These are pronounced as “alpha cap” and “beta cap,” respectively, and are used as approximations of the true population parameters.\n\n\n9.1.4 Regression coefficient\nThe regression coefficient, \\(\\beta\\) in linear regression, represents the slope of the regression line. It quantifies the relationship between the independent variable (\\(x\\)) and the dependent variable (\\(y\\)). Specifically, \\(\\beta\\) indicates the expected change in \\(y\\) for a one-unit increase in \\(x\\), holding other factors constant. Regression coefficients can take any real value, ranging from \\(-\\infty\\) to \\(+\\infty\\), depending on the nature of the relationship between the variables. A positive \\(\\beta\\) implies a direct relationship (as \\(x\\) increases, \\(y\\) increases), while a negative \\(\\beta\\) implies an inverse relationship (as \\(x\\) increases, \\(y\\) decreases). A coefficient of \\(0\\) suggests no linear relationship between the variables. The magnitude of \\(\\beta\\) reflects the strength of the association, with larger absolute values indicating stronger relationships. Equation 9.16 and Equation 9.16 for the calculation of estimate of \\(\\beta\\) is obtained by solving normal equations Equation 9.10 and Equation 9.15. Equation 9.16 can be used for hand calculations.\n\\[\\hat{\\beta}=\\frac{\\sum_{i = 1}^{n}{y_{i}x_{i} - \\frac{\\sum_{i = 1}^{n}{y_{i}\\sum_{i = 1}^{n}x_{i}}}{n}}}{\\sum_{i = 1}^{n}x_{i}^{2} - \\frac{\\left( \\sum_{i = 1}^{n}x_{i} \\right)^{2}}{n}} \\tag{9.16}\\]\nEquation 9.16 can be written as\n\\[\\hat{\\beta} =\\frac{cov(x,y)}{var(x)} \\tag{9.17}\\]\n\n\n9.1.5 Intercept\nThe intercept, often denoted as \\(\\alpha\\) in linear regression, represents the value of the dependent variable \\(y\\) when the independent variable \\(x\\) is equal to zero. It is the point at which the regression line crosses the \\(y\\)-axis. The intercept provides a baseline value for the dependent variable before any influence from the independent variable is considered. The intercept can take any real value (\\(-\\infty\\) to \\(+\\infty\\)), and its meaning depends on the specific context of the data. In some cases, it may not have a practical interpretation, especially if \\(x = 0\\) is not within the range of observed data. Equation 9.18 for estimating \\(\\alpha\\) is obtained by solving normal equations Equation 9.10 and Equation 9.15.\n\\[\\hat{\\alpha} =\\overline{y}-\\hat{\\beta}.\\overline{x} \\tag{9.18}\\]\nwhere \\(\\overline{y}\\) = mean of \\(y\\); \\(\\overline{x}\\) = mean of \\(x\\)\n\n\n\n\n\n\nNote\n\n\n\nOnce the estimates of \\(\\alpha\\) and \\(\\beta\\), which are denoted as \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\) respectively are obtained using Equation 9.17 and Equation 9.18. The estimated regression line can be written as:\n\\[y =\\hat{\\alpha}+ \\hat{\\beta} x \\tag{9.19}\\]\n\n\n\n\n9.1.6 Assumptions\nThe goal of linear regression is to estimate the coefficients of the regression equation, which help explain how changes in the independent variables affect the dependent variable. However, for the results of a regression analysis to be reliable and meaningful, certain underlying assumptions must be met. These assumptions ensure that the estimates are accurate, the predictions are unbiased, and the conclusions drawn from the model are valid. Before conducting a regression analysis, it is crucial to understand and verify these assumptions to avoid misleading results.These assumptions are:\n\nLinearity\nThe relationship between the independent variable(s) and the dependent variable is linear. This means that the changes in the dependent variable are proportional to changes in the independent variable(s).\nIndependence\nThe observations in the dataset are independent of each other. Additionally, the residuals (errors) are assumed to be independent.\nHomoscedasticity\nThe variance of the residuals is constant across all levels of the independent variable(s). In other words, the spread of the residuals should remain consistent and not show patterns of increasing or decreasing variance.\nNormality of residuals\nThe residuals (errors) are normally distributed. This is particularly important for hypothesis testing and constructing confidence intervals. Normality assumption doesnot much influence the estimation of regression coefficient.\nNo multicollinearity\nIn the case of multiple regression, the independent variables should not be highly correlated with each other, as multicollinearity can distort the estimates of regression coefficients.\nNo autocorrelation\nThere should be no autocorrelation in the residuals. This means that the residuals of one observation should not be correlated with the residuals of another.\nCorrect model specification\nThe model should include all relevant variables and exclude irrelevant ones. The functional form of the relationship between variables should be correctly specified.\n\nViolations of these assumptions can lead to biased, inconsistent, or inefficient estimates, affecting the validity of the regression analysis.\n\n\n\n\n\n\nNote\n\n\n\nThe essence of the assumptions in linear regression can be summarized as \\(e \\sim \\text{i.i.d.}(0, \\sigma^2)\\). This denotes that the errors are independent and identically distributed (i.i.d.), with a mean of zero and a constant variance \\(\\sigma^2\\). Independence ensures that the error for one observation neither depends on nor influences the error for another. Identically distributed means that all errors are drawn from the same probability distribution, without variation across observations. A mean of zero ensures that the errors do not introduce systematic bias into the model’s predictions. Additionally, the constant variance (homoscedasticity) implies that the errors maintain a consistent level of variability across all values of the independent variable(s).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Regression analysis</span>"
    ]
  },
  {
    "objectID": "regression.html#two-lines-of-regression",
    "href": "regression.html#two-lines-of-regression",
    "title": "9  Regression analysis",
    "section": "9.2 Two lines of regression",
    "text": "9.2 Two lines of regression\nConsider the data presented in Table 8.2, which shows the average daily soil moisture content and the corresponding monetary yield from crops in Example 8.2 of Section 8.2. For the data we can draw two lines of regression interchanging variables in X and Y axis as shown in Figure 9.4.\n\n\n\n\n\n\n\n\n\n\n\n(a) Moisture in X-axis and yield in Y-axis\n\n\n\n\n\n\n\n\n\n\n\n(b) Yield in X-axis and moisture in Y-axis\n\n\n\n\n\n\n\nFigure 9.4: Two lines of regression\n\n\n\nFrom Figure 9.4 it is clear that two lines of regression that of \\(y\\) on \\(x\\) and \\(x\\) on \\(y\\) is possible.\nRegression of \\(y\\) on \\(x\\)\nConsider the two variables \\(x\\) and \\(y\\), if you are considering \\(y\\) as dependent variable and \\(x\\) as independent variable then your equation is:\n\\[y = \\alpha + \\beta_{yx}x \\tag{9.20}\\]\nThis is used to predict the unknown value of variable \\(y\\) when value of variable \\(x\\) is known. Usually \\(\\beta\\) here is denoted as \\(\\beta_{yx}\\) and it is obtained using Equation 9.21.\n\\[\\beta_{yx} =\\frac{cov(x,y)}{var(x)} \\tag{9.21}\\]\nRegression of \\(x\\) on \\(y\\)\nConsider the two variables \\(x\\) and \\(y\\), if you are considering \\(x\\) as dependent variable and \\(y\\) as independent variable then your equation is:\n\\[x = \\alpha_1 + \\beta_{xy}.x \\tag{9.22}\\]\nThis is used to predict the unknown value of variable \\(x\\) when value of variable \\(y\\) is known. Usually \\(\\beta\\) here is denoted as \\(\\beta_{xy}\\) and it is obtained using Equation 9.23.\n\\[\\beta_{xy} =\\frac{cov(x,y)}{var(y)} \\tag{9.23}\\]\nYou can see from Equation 9.21 and Equation 9.23 both the regression coefficients were different. It depends on the experimenter to choose dependent and independent variable. In the Example 8.2 there may be situation that considering moisture as dependent variable is meaningless, i.e. it depends on the fact that what is the usefulness in predicting soil moisture based on monetary crop yield?. So the selection of dependent and independent variable is entirely the discretion of experimenter based on the objective of his study.\n\n9.2.1 Properties of regression coefficients\n\nThe correlation coefficient between \\(x\\) and \\(y\\) denoted as \\(r_{xy}\\) is the geometric mean of the two regression coefficients \\(\\beta_{yx}\\) and \\(\\beta_{xy}\\)\n\n\\[r_{xy} = \\sqrt{\\beta_{yx}.\\beta_{xy}} \\tag{9.24}\\]\n\nRegression coefficients are independent of change of origin but not of scale. Regression coefficients exhibit specific behaviors under transformations of the variables \\(x\\) or \\(y\\), particularly when there are changes in origin or scale. They are independent of a change in origin for both \\(x\\) and \\(y\\). This means that adding or subtracting a constant to either variable (e.g., transforming \\(x\\) to \\(x'= x + c\\) or \\(y\\) to \\(y'= y + c\\), where \\(c\\) is a constant) does not affect the slope \\(\\beta\\) of the regression line. The slope depends on the relative differences between values, which remain unchanged by shifts in origin. However, changes in origin will affect the intercept \\(\\alpha\\) by adjusting it to accommodate the shift in \\(y\\). In contrast, regression coefficients are not independent of changes in scale. When either \\(x\\) or \\(y\\) is multiplied or divided by a constant (e.g., \\(x'= kx\\) or \\(y'= ky\\), where \\(k\\) is a non-zero constant), the slope \\(\\beta\\) changes proportionally. Specifically, the regression coefficient is inversely proportional to the scale factor for \\(x\\) because scaling affects the covariance and variance of the variables. Similarly, scaling \\(y\\) affects both the slope and the intercept, as the entire regression equation is scaled by the factor \\(k\\). Understanding these effects is crucial for accurately interpreting regression results, especially when transformations are applied during data preprocessing.\nIf one regression coefficient is greater than unity, then the other must be less than unity but not vice versa. i.e. both the regression coefficients can be less than unity but both cannot be greater than unity, i.e. if \\(\\beta_{yx} &gt;1\\) then \\(\\beta_{xy} &lt;1\\) and if \\(\\beta_{xy} &gt;1\\), then \\(\\beta_{yx} &lt;1\\).\nAlso if one regression coefficient is positive the other must be positive (in this case the correlation coefficient is positive) and if one regression coefficient is negative the other must be negative (in this case the correlation coefficient is negative). This relationship arises because the regression coefficients and the correlation coefficient share the same sign, reflecting the direction of the association between the two variables.\nThe range of regression coefficients is \\(- \\infty\\) to \\(+ \\infty\\).\nIf the variables (\\(x\\) ) and (\\(y\\)) are independent, the regression coefficients are zero. This is referred to as the independence property of regression coefficients.\n\n\n\n9.2.2 Properties of regression lines\n\nRegression lines minimize the sum of squared deviations of observed values from the predicted values, ensuring the best possible fit.\nThe regression lines intersect at the mean values of \\(x\\) and \\(y\\) i.e., at (\\(\\overline{x}\\),\\(\\overline{y}\\))\nThe slopes of the regression lines are related to the correlation coefficient r. If r=0, the lines are perpendicular, indicating no linear relationship.\n\nThe position of regression lines is closely related to the strength of the correlation between \\(x\\) and \\(y\\). As shown in Figure 9.5, the placement of the two lines changes with the correlation value, demonstrating how the relationship between \\(x\\) and \\(y\\) influences the regression line’s position.\n\n\n\n\n\n\n\n\n\n\n\n(a) When correlation = 0\n\n\n\n\n\n\n\n\nPerfect positive correlation\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Perfect negative correlation\n\n\n\n\n\n\n\n\n\n\n\n(c) High degree positive correlation\n\n\n\n\n\n\n\n\n\n\n\n\n\n(d) High degree negative correlation\n\n\n\n\n\n\n\n\n\n\n\n(e) Low degree negative correlation\n\n\n\n\n\n\n\n\n\n\n\n\n\n(f) Low degree positive correlation\n\n\n\n\n\n\n\nFigure 9.5: Effect of correlation strength on the position of regression lines",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Regression analysis</span>"
    ]
  },
  {
    "objectID": "regression.html#uses-of-regression",
    "href": "regression.html#uses-of-regression",
    "title": "9  Regression analysis",
    "section": "9.3 Uses of regression",
    "text": "9.3 Uses of regression\n\nPrediction\nRegression analysis is used to predict the value of a dependent variable (\\(y\\)) based on one or more independent variables (\\(x\\)). Examples in agricultural research include:\n\nPredicting crop yield based on weather parameters such as temperature, rainfall, and humidity.\n\nEstimating soil nutrient levels using remote sensing data or environmental variables.\n\nForecasting pest or disease outbreaks based on climatic and ecological conditions.\n\nIdentify the strength of relationships\nRegression helps quantify the strength of the relationship between variables. This is essential in agricultural research to identify influential factors. Examples include:\n\nDetermining the effect of fertilizer dosage on crop yield.\n\nAnalyzing the relationship between irrigation frequency and plant growth.\n\nUnderstanding the impact of livestock feed composition on milk production.\n\nForecast effects or impact of changes\nRegression models allow researchers to evaluate how changes in one or more independent variables affect the dependent variable. For example:\n\nAssessing how seed quality impacts overall harvest productivity.\n\nAnalyzing the effects of varying water availability on crop output in drought-prone areas.\n\nEstimating the economic benefits of adopting precision farming techniques.\n\nPredict Trends and Future Values\nRegression is valuable for modeling trends and forecasting future values, aiding in strategic planning and policy-making. Applications include:\n\nPredicting future crop yields under different climate change scenarios.\n\nEstimating long-term price trends for agricultural commodities such as rice, wheat, or coffee.\n\nForecasting the adoption rates of new agricultural technologies among farmers.\n\n\n\n\n\n\n\n\nNote\n\n\n\nMultiple regression is an extension of simple linear regression that models the relationship between a dependent variable and two or more independent variables. It allows researchers to account for the combined effect of multiple factors on an outcome, making it particularly useful in agricultural research. For instance, crop yield can be predicted based on a combination of variables such as soil nutrients, rainfall, temperature, and fertilizer application.\nIn a multiple regression model, the relationship between the dependent variable \\(y\\) and independent variables \\(x_1, x_2, \\dots, x_n\\) is expressed as:\n\\[y = \\alpha + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_n x_n + e \\tag{9.25}\\]\nWhere:\n- \\(y\\): Dependent variable (response).\n- \\(\\alpha\\): Intercept (value of \\(Y\\) when all \\(X\\) values are zero).\n- \\(\\beta_1, \\beta_2, \\dots, \\beta_n\\): Coefficients representing the effect of each independent variable on \\(y\\).\n- \\(x_1, x_2, \\dots, x_n\\): Independent variables (predictors).\n- \\(e\\): Error term accounting for variability not explained by the predictors.\n\n\nExample 9.1: We will be using the data presented in Table 8.2, which shows the average daily soil moisture content and the corresponding monetary yield from crops in Example 8.2 of Section 8.2 to demonsrtate how regression analysis can be used to answer the following questions.\n\nWhat is the functional form of relationship between soil moisture and monetary crop yield?\nWhat will be the estimated monetary crop yield when average daily soil moisture is maintained around 20%?\n\nSolution 9.1:\n\nFit a model considering monetary crop yield as dependent variable (\\(y\\)) and average soil moisture as independent variable (\\(x\\)). Fitting a model means estimating \\(\\beta\\) and \\(\\alpha\\) using equation.\nAfter fitting the model put 20 in the \\(x\\) value you will get the predicted \\(y\\) value\n\n\n\n\nTable 9.1: Calculation table for regression\n\n\n\n\n\n\nSl No. \n\n\nSoil moisture (\\(x\\))\n\n\nCrop yield in Rs(\\(y\\))\n\n\n\\((x_{i}-\\overline{x})\\)\n\n\n\\((y_{i}-\\overline{y})\\)\n\n\n\\((x_{i}-\\overline{x})(y_{i}-\\overline{y})\\)\n\n\n\\((x_{i}-\\overline{x})^2\\)\n\n\n\n\n\n\n1\n\n\n14.2\n\n\n215\n\n\n-4.48\n\n\n-187.42\n\n\n838.69\n\n\n20.03\n\n\n\n\n2\n\n\n16.4\n\n\n325\n\n\n-2.28\n\n\n-77.42\n\n\n176.12\n\n\n5.18\n\n\n\n\n3\n\n\n11.9\n\n\n185\n\n\n-6.78\n\n\n-217.42\n\n\n1473.00\n\n\n45.90\n\n\n\n\n4\n\n\n15.2\n\n\n332\n\n\n-3.48\n\n\n-70.42\n\n\n244.70\n\n\n12.08\n\n\n\n\n5\n\n\n18.5\n\n\n406\n\n\n-0.18\n\n\n3.58\n\n\n-0.63\n\n\n0.03\n\n\n\n\n6\n\n\n22.1\n\n\n522\n\n\n3.43\n\n\n119.58\n\n\n409.57\n\n\n11.73\n\n\n\n\n7\n\n\n19.4\n\n\n412\n\n\n0.73\n\n\n9.58\n\n\n6.95\n\n\n0.53\n\n\n\n\n8\n\n\n25.1\n\n\n614\n\n\n6.43\n\n\n211.58\n\n\n1359.42\n\n\n41.28\n\n\n\n\n9\n\n\n23.4\n\n\n544\n\n\n4.73\n\n\n141.58\n\n\n668.98\n\n\n22.33\n\n\n\n\n10\n\n\n18.1\n\n\n421\n\n\n-0.58\n\n\n18.58\n\n\n-10.69\n\n\n0.33\n\n\n\n\n11\n\n\n22.6\n\n\n445\n\n\n3.93\n\n\n42.58\n\n\n167.14\n\n\n15.41\n\n\n\n\n12\n\n\n17.2\n\n\n408\n\n\n-1.48\n\n\n5.58\n\n\n-8.24\n\n\n2.18\n\n\n\n\nSUM\n\n\n224.1\n\n\n4829\n\n\n0.00\n\n\n0.00\n\n\n5325.03\n\n\n176.98\n\n\n\n\n\n\n\n\\(n\\) =12\n\\[mean,\\overline{x} = \\ \\frac{224.1}{12} = 18.675\\]\n\\[mean,\\overline{y} = \\ \\frac{4829}{12} = 402.416\\]\n\\[cov (x,y) =\\frac{1}{n}\\sum_{i = 1}^{n}{\\left( x_{i} - \\overline{x} \\right)\\left( y_{i} - \\overline{y} \\right)} = \\frac{5325.03}{12} = 443.752\\]\nvariance of \\(x\\):\n\\[var\\left( x \\right) = \\ \\frac{1}{n}\\sum_{i = 1}^{n}\\left( x_{i} - \\overline{x} \\right)^{2} = \\frac{176.983}{12} = 14.7485\\] now using Equation 9.17\n\\[\\hat{\\beta} =\\frac{cov(x,y)}{var(x)}\\]\n\\[\\hat{\\beta} =\\frac{443.752}{14.7485} =30.088\\] using Equation 9.18\n\\[\\hat{\\alpha} =\\overline{y}-\\hat{\\beta}.\\overline{x}\\]\n\\[\\hat{\\alpha} = 402.416 - 30.088\\left( 18.675 \\right) = \\  - 159.477\\]\nSo our estimated model is\n\\[y = \\  - 159.477 + 30.088x\\]\n\\[\\text{crop yield in Rs} =- 159.477 + 30.088\\text{(soil moisture)}\\]\nfor soil moisture content at 20%, i.e. \\(x = 20\\)\n\\[y = \\  - 159.477 + 30.088(20)\\] = 442.28\nSo the predicted monetary yield in rupees at a average soil moisture of 20% is \\(442.283\\)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Regression analysis</span>"
    ]
  },
  {
    "objectID": "regression.html#correlation-and-regression",
    "href": "regression.html#correlation-and-regression",
    "title": "9  Regression analysis",
    "section": "9.4 Correlation and regression",
    "text": "9.4 Correlation and regression\nCorrelation and regression are fundamental concepts in statistics, often used to explore and model relationships between variables. While both techniques examine how variables relate to one another, they differ in their purpose, interpretation, and methodology. Correlation focuses on measuring the strength and direction of an association between two variables, without assuming causation. In contrast, regression goes a step further by modeling the relationship, enabling predictions of one variable based on the other(s). The Table 9.2 below provides a detailed comparison of these two approaches, helping to clarify their unique characteristics and applications.\n\n\n\nTable 9.2: Correlation versus regression\n\n\n\n\n\n\n\n\n\n\nItem\nCorrelation\nRegression\n\n\n\n\nDefinition\nMeasures the strength and direction of the relationship between two variables.\nModels the relationship between a dependent variable and one or more independent variables.\n\n\nObjective\nTo quantify the degree of association between variables.\nTo predict the value of the dependent variable based on the independent variable(s).\n\n\nCausation\nDoes not imply causation; it only measures association. Causation means changes in one variable cause changes in another.\nCan imply causation if assumptions are met and the model is well-specified.\n\n\nEquation\nNo equation is derived.\nDerives an equation: \\(y=\\alpha + \\beta x + e\\).\n\n\nVariables involved\nConsiders two variables at a time.\nCan involve one or multiple independent variables to predict a dependent variable.\n\n\nSymmetry\nCorrelation between (x) and (y) is the same as (y) and (x).\nThe regression coefficient of y on x is different from x on y.\n\n\nRange\nThe correlation coefficient (r) ranges from -1 to 1.\nRegression coefficients (\\(\\alpha, \\beta\\)) ranges from \\(-\\infty\\) to \\(+\\infty\\).\n\n\nUnits\nUnit less measure.\nDependent on the units of the variables involved.\n\n\nPurpose\nTo understand the strength of the relationship.\nTo predict outcomes or explain variability in the dependent variable.\n\n\n\n\n\n\n\n\n\n\n\n\nHistorical Insights\n\n\n\n“Regression and study of heights”\nThe story of regression begins with Sir Francis Galton’s groundbreaking work on heredity in the late 19th century. While studying the heights of parents and their children, Galton noticed a fascinating pattern: tall parents tended to have slightly shorter children, and shorter parents tended to have slightly taller children. He called this phenomenon “regression toward the mean”, as the offspring’s heights seemed to move closer to the population average. This observation not only introduced the term “regression” but also inspired the development of statistical tools for studying relationships between variables. Galton’s work, later expanded by Karl Pearson, laid the foundation for modern regression analysis, which remains an essential technique in several fields ranging from agriculture to space exploration.\n\n\n\n\n\n\n\n\nQuotes to Inspire\n\n\n\n“Statistics is the art of never having to say you’re wrong”:-Robert P. Abelson",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Regression analysis</span>"
    ]
  },
  {
    "objectID": "appendix1.html",
    "href": "appendix1.html",
    "title": "10  Appendix 1",
    "section": "",
    "text": "Source of data related to different sectors\n\n\n\nSl No\nData Particulars\nSource\nOrganisation\n\n\n\n\n1.\nEstimates of area, production of important crops in India\nhttps://agriwelfare.gov.in/en/AgricultureEstimates\nMoAFW, GOI\n\n\n2.\nState level, district level aggregates of Area, production and productivity of principal crops in Kerala\nhttps://www.ecostat.kerala.gov.in\nDepartment of Economics & Statistics, Govt of Kerala\n\n\n3.\nDistrict Wise Birth & Death Data, State Level Birth & Death Data\nhttps://www.ecostat.kerala.gov.in\nDepartment of Economics & Statistics, Govt of Kerala\n\n\n4.\nMinimum Support Price (MSP) Statement\nhttps://desagri.gov.in/statistics-type/latest-minimum-support-price-msp-statement/\nMoAFW, GOI\n\n\n5.\nAnnual Survey of Industries, Index of Industrial Production, Household Consumer Expenditure, Economic Census, Enterprises Surveys, Periodic Labour Force Survey, CPI, etc\nhttps://www.mospi.gov.in/\nMinistry of Statistics and Programme Implementation\n\n\n6.\nState/UT wise estimates on population, health, family planning and nutrition related key indicators like fertility, mortality, maternal, child and adult health, women and child nutrition, domestic violence, etc\nhttps://main.mohfw.gov.in/\nMinistry of Health & Family Welfare\n\n\n7.\nCommodity wise export import data\nhttps://tradestat.commerce.gov.in, https://ftddp.dgciskol.gov.in\nMinistry of Commerce and Industry\n\n\n8.\nData on various aspects of Indian economy, banking and finance\nhttps://www.rbi.org.in\nReserve Bank of India\n\n\n9.\nSustainable Development Goal report\nhttps://www.niti.gov.in/\nNITI Aayog",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Appendix 1</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "11  References",
    "section": "",
    "text": "Ball, Philip. 2004. Critical Mass. Farrar, Straus; Giroux.\n\n\nFiori, Anna M., and Michele Zenga. 2009. “Karl Pearson and the\nOrigin of Kurtosis.” International Statistical Review 77\n(1): 40–50. https://doi.org/10.1111/j.1751-5823.2009.00076.x.\n\n\nFisher, Ronald A. 1918. “The Correlation Between Relatives on the\nSupposition of Mendelian Inheritance.” Transactions of the\nRoyal Society of Edinburgh 52 (2): 399–433. https://doi.org/10.1017/S0080456800012163.\n\n\nGoon, Gupta, A. M., and B Dasgupta. 1983. Fundamentals of\nStatistics. Vol. I. TheWorld Press.\n\n\nGupta, S. C., and V. K. Kapoor. 1997. Fundamentals of Mathematical\nStatistics. Sulthan Chand Publications, New Delhi.\n\n\nPearson, Karl. 1894. “Contributions to the Mathematical Theory of\nEvolution.” Philosophical Transactions of the Royal Society\nof London. A 185: 71–110. https://doi.org/10.1098/rsta.1894.0003.\n\n\nPratheesh P. Gopinath, Brigit Joseph, Rajender Parsad. 2020. GRAPES:\nGeneral r-Shiny Based Analysis Platform Empowered by Statistics\n(version 1.0.0). https://doi.org/10.5281/zenodo.4923220.\n\n\nTeam, R Core. 2024. “R: A Language and Environment for Statistical\nComputing.” R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nWei, Taiyun, and Viliam Simko. 2021. “Corrplot: Visualization of a\nCorrelation Matrix.” https://cran.r-project.org/web/packages/corrplot/corrplot.pdf.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>References</span>"
    ]
  },
  {
    "objectID": "association.html#example-calculating-mean",
    "href": "association.html#example-calculating-mean",
    "title": "8  Measures of association",
    "section": "8.9 Example: Calculating Mean",
    "text": "8.9 Example: Calculating Mean\nA farmer measures the yield of wheat in five plots (in kg): 20, 25, 22, 23, and 30. Calculate the mean yield.\nSolution",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Measures of association</span>"
    ]
  },
  {
    "objectID": "probability.html",
    "href": "probability.html",
    "title": "10  Probability",
    "section": "",
    "text": "10.1 Random experiment\nProbability is a cornerstone of statistical methods, providing the mathematical framework to quantify and analyze uncertainty. In the diverse and dynamic field of agricultural research, uncertainty often arises in areas such as yield predictions, pest control strategies, and the impact of environmental variables on crop growth. Understanding probability equips researchers and students with the tools to make informed decisions in the face of variability and randomness.\nThis chapter introduces fundamental probability concepts. We begin with the basic definitions and axioms of probability. The chapter progresses to explore key topics such as conditional probability, Bayes’ theorem, and independence of events.\nA random experiment is a process or procedure that produces an outcome which cannot be predicted with certainty beforehand. The outcome is determined by chance, and the set of all possible outcomes is known as the sample space. Random experiments form the basis of probability theory and are critical for understanding uncertainty in various contexts.\nCharacteristics of a random experiment\nSome common examples of random experiments used to illustrate probability theory were discussed below:\nTossing a coin\nTossing a coin is one of the simplest and most fundamental random experiments used to illustrate the principles of probability. Despite its simplicity, this seemingly mundane activity provides profound insights into random processes and lays the groundwork for understanding more complex probabilistic systems.\nWhen a coin is tossed, there are two possible outcomes:\nWe say that for an unbiassed coin the probability of the coin landing \\(H\\) is \\(\\frac{1}{2}\\) and the probability of the coin landing \\(T\\) is \\(\\frac{1}{2}\\)\nThrowing dice\nA standard die has six faces, numbered from 1 to 6. Under fair conditions, each face has an equal probability of appearing when the die is rolled. Thus, the probability of getting any one face is \\(\\frac{1}{6}\\).\nPlaying cards\nA standard deck of playing cards consists of 52 cards, divided into four suits: spades, hearts, diamonds, and clubs, with each suit containing 13 cards. Four suits in playing cards were given in the Figure 10.2. The spades suit includes 9 numbered cards from 2 to 10, along with the picture cards ace, king, queen, and jack. Similarly, the hearts, diamonds, and clubs suits each contain 13 cards. Of these 52 cards, 26 are red, consisting of the hearts and diamonds suits, while the other 26 cards are black, comprising the spades and clubs suits. Playing cards can be used to create examples for random experiments, such as drawing a single card from a shuffled deck to determine its specific identity. Other experiments include drawing a card and checking its suit etc. These simple experiments can be easily modified with conditions or repetition, making playing cards a versatile tool for exploring probability and random events.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "probability.html#random-experiment",
    "href": "probability.html#random-experiment",
    "title": "10  Probability",
    "section": "",
    "text": "Uncertainty of outcome: The result of the experiment cannot be predetermined.\nRepeatability: The experiment can be repeated under the same conditions.\nDefined sample space: All possible outcomes are known and well-defined.\n\n\n\n\n\n\nHeads (\\(H\\))\nTails (\\(T\\))\n\n\n\n\n\n\n\n\nNote\n\n\n\nAn unbiased coin is a theoretical concept in probability, referring to a coin designed or assumed to have no preference for either side when tossed, such that the likelihood of landing on heads (H) is exactly the same as landing on tails (T), with each outcome having a probability of \\(\\frac{1}{2}\\) or 50%. Its characteristics include perfect symmetry, ensuring the coin is balanced without physical imperfections or uneven weight distribution that could influence the result, and equal probability, where each side is equally likely to face upward after a toss. This assumption generally holds under ideal experimental conditions, free from external factors like uneven surfaces or human bias in the tossing technique.\n\n\n\n\n\n\n\n\n\n\nFigure 10.1: Two dice\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Spades\n\n\n\n\n\n\n\n\n\n\n\n(b) Hearts\n\n\n\n\n\n\n\n\n\n\n\n(c) Diamonds\n\n\n\n\n\n\n\n\n\n\n\n(d) Clubs\n\n\n\n\n\n\n\nFigure 10.2: Four suits in playing cards",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "probability.html#probability",
    "href": "probability.html#probability",
    "title": "10  Probability",
    "section": "10.3 Probability",
    "text": "10.3 Probability\nProbability is a measure of the likelihood or chance that a particular event will occur. It quantifies uncertainty and is expressed as a number between 0 and 1, where 0 indicates an impossible event and 1 represents a certain event. Mathematically, the probability of an event \\(A\\) is defined as the ratio of the number of favorable outcomes to the total number of possible outcomes, assuming all outcomes are equally likely, and is given by\nProbability of an event \\(A\\) happening:\n\\[p(A) = \\frac{\\text{Number of favourable outcomes}}{\\text{Total possible outcomes}} \\tag{10.1}\\]\nhere favourable outcome means outcomes favouring the happening of event \\(A\\).\nExample 10.1: What is the chances of rolling a “4” with a die ?\nSolution 10.1: Let the event \\(A\\) = rolling a “4” with a die. Number of ways event \\(A\\) can happen is one, as there is only 1 face with a “4” on it. Total number of outcomes is 6 (there are 6 faces altogether). So using Equation 10.1; probability,\\(p(A)\\) = \\(\\frac{1}{6}\\).\nExample 10.2: There are 5 marbles in a bag: 4 are blue, and 1 is red. What is the probability that a blue marble gets picked?\nSolution 10.2: Let the event \\(A\\) = a blue marble gets picked. Number of ways event \\(A\\) can happen is 4 (there are 4 blues). Total number of possible outcomes is 5 (there are 5 marbles in total). So using Equation 10.1; probability,\\(p(A)\\) = \\(\\frac{4}{5}\\) = 0.8\nCommon terms used\nOutcome: An outcome is the result of a random experiment, representing a single occurrence or observation. For instance, when rolling a die, the outcomes could include the appearance of 1 dot, 2 dots, and so on.\nSample space: The sample space (denoted as \\(S\\)) of a random experiment is the set that contains all possible outcomes of that experiment. For example, when tossing a coin, the sample space is \\(S\\) = {H, T}, and when throwing a die, the sample space is \\(S\\) = {1, 2, 3, 4, 5, 6}.\nSample point: A sample point is an individual element of the sample space, representing a single possible outcome. For example, “heads” in a coin toss or the “5 of clubs” in a deck of cards are sample points. In the case of rolling a die, there are six distinct sample points within the sample space.\n\n\n\n\n\n\nFigure 10.3: Sample space and sample point\n\n\n\n\n\n\n\n\n\nTry yourself\nIf a die is tossed\n\nWhat is the sample space?\nWhat is the probability of getting a 1?\nWhat is the probability of obtaining a even number?\nWhat is the probability of getting a 7?",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "probability.html#event",
    "href": "probability.html#event",
    "title": "10  Probability",
    "section": "10.4 Event",
    "text": "10.4 Event\nAn event refers to one or more outcomes of a random experiment. It can represent any subset of the sample space, where the event consists of specific outcomes that are of interest in the context of the experiment. An event can be a single outcome. For example, when tossing a coin, getting a “Tail” is an event that represents just one outcome from the sample space {H, T}. Similarly, rolling a “5” on a die is an event that includes only one specific outcome, 5, from the sample space {1, 2, 3, 4, 5, 6}.\nOn the other hand, an event can also include multiple outcomes. For instance, when selecting a “king” from a deck of cards, the event includes any one of the four kings (king of hearts, king of diamonds, king of clubs, king of spades), forming a set of four possible outcomes. Similarly, when rolling an “even number” on a die, the event would encompass the outcomes {2, 4, 6}, which are all the even numbers in the sample space of a die roll. Thus, an event can be a single outcome or a combination of multiple outcomes depending on the context of the experiment.\nExample 10.3: Ram wants to see how many times a “double” (both dice have same number) comes up when throwing 2 dice.\nSolution 10.3: The sample space is all possible outcomes (36 Sample Points): {1,1} {1,2} {1,3} {1,4} … {6,3} {6,4} {6,5} {6,6}. The event Ram is looking for is a “double”, where both dice have the same number. It is made up of these 6 sample points: {1,1} {2,2} {3,3} {4,4} {5,5} and {6,6}. You can calculate the probability of the event using Equation 10.1. Probability = \\(\\frac{6}{36}=0.17\\).\n\n10.4.1 Types of events\nIndependent events\nIndependent events are events where the occurrence of one does not influence the occurrence of another. For instance, consider tossing a coin. If it lands on “heads” three times in a row, the outcome of the next toss remains unaffected by the previous results. The probability of getting “heads” on the next toss is still \\(\\frac{1}{2}\\) (or 0.5), just as it is for any fair coin toss. This demonstrates that past outcomes do not influence the probabilities of future events in independent scenarios.\nExample 10.4: A coin is tossed 100 times, and heads appear in 99 of those tosses. What is the probability that heads will appear on the \\(100^{th}\\) toss?\nSolution 10.4: Answer is \\(\\frac{1}{2}\\) as each toss is independent of previous toss.\nDependent events\nDependent events are events where the outcome of one event influences the probabilities of subsequent events. For example, consider drawing cards from a deck. The probabilities change as cards are removed from the deck, altering the available outcomes. Initially, the chance of drawing a king on the first card is \\(\\frac{4}{52}\\). However, for the second draw, the probabilities depend on the outcome of the first draw. If the first card is a king, only 3 kings remain among the 51 remaining cards, reducing the likelihood of drawing another king to \\(\\frac{3}{51}\\). Conversely, if the first card is not a king, all 4 kings remain, making the probability \\(\\frac{4}{51}\\). This illustrates how previous outcomes affect probabilities in dependent events.\n\n\n\n\n\n\nNote\n\n\n\nWhen cards are drawn with replacement, each card is returned to the deck after being drawn, so the total number of cards remains constant. This keeps the probabilities unchanged, making the events independent. However, when cards are drawn without replacement, the total number of cards decreases after each draw, which alters the probabilities and makes the events dependent.\n\n\nMutually exclusive events\nMutually exclusive events are events that cannot occur at the same time. It is either one event or the other, but not both. For example, turning left or right is mutually exclusive because you cannot do both simultaneously. Similarly, heads and tails in a coin toss is mutually exclusive. Drawing a king and drawing an ace are mutually exclusive events because you can’t draw both in a single card. However, not all events are mutually exclusive. For instance, kings and hearts are not mutually exclusive because it is possible to have a king of hearts, an outcome that belongs to both categories.\nExhaustive events\nA set of events is called exhaustive if, together, they encompass the entire sample space. For example, when tossing a die, the sample space is \\(S = \\{1, 2, 3, 4, 5, 6\\}\\). Consider the events: event \\(A\\), which is getting an even number \\(\\{2, 4, 6\\}\\), and event \\(B\\), which is getting an odd number \\(\\{1, 3, 5\\}\\). These events are exhaustive because, when combined, they include all possible outcomes in the sample space.\nEqually likely events\nEqually likely events are events that have the same theoretical probability of occurring. For example, when tossing a coin, event \\(A\\) (getting a head) and event \\(B\\) (getting a tail) are equally likely events because both have an equal probability of occurring.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "probability.html#definitions-of-probability",
    "href": "probability.html#definitions-of-probability",
    "title": "10  Probability",
    "section": "10.5 Definitions of probability",
    "text": "10.5 Definitions of probability\nIn probability theory, different approaches are used to define and calculate the likelihood of events occurring. These approaches vary depending on the nature of the experiment and the available information. The three primary definitions of probability are mathematical (classical), statistical (empirical), and axiomatic, each offer unique perspectives and methods for determining probabilities. The classical approach is based on equally likely outcomes, the empirical approach relies on experimental data, and the axiomatic approach, developed by A.N. Kolmogorov, is based on a set of foundational principles. Understanding these definitions helps in applying probability theory to various real-world situations, from simple experiments to more complex scenarios.\n\n10.5.1 Mathematical approach\nIt is also known as classical, theoretical or a priori approach to probability. If an experiment with \\(n\\) exhaustive, mutually exclusive and equally likely outcomes, \\(m\\) outcomes are favourable to the happening of an event \\(E\\), the probability \\(p\\) of happening of E is given by\n\\[p\\left( E \\right) = \\ \\frac{m}{n} \\tag{10.2}\\]\n\\(p\\) is termed as probability of success.\nExample 10.5: When a coin is tossed, there are two possible outcomes head or tail. Outcomes are exhaustive, mutually exclusive and equally likely. What is the probability of getting head?\nSolution 10.5: Solution using mathematical approach, consider the event \\(E\\) : getting a head; probability of event \\(E\\), \\(p(E)\\) can be determined using Equation 10.2.\nhere, number of outcomes are favourable to the happening of event \\(E\\), \\(m =1\\); \\(n\\) = total number of possible outcomes (head and tail) = 2\n\\[p\\left( E \\right) =  \\ \\frac{1}{2}\\]\ni.e. probability of getting a head is \\(\\frac{1}{2}\\)\nLimitations\nThe mathematical approach has certain limitations. For instance, it cannot account for situations where the outcomes are not equally likely, such as tossing a biased die. Additionally, this approach cannot define probability when the total number of possible outcomes \\(n\\) is unknown or tends to infinity, as in determining the probability of raining tomorrow. To address these limitations, other definitions of probability have been developed.\n\n\n10.5.2 Statistical approach\nThe statistical approach to probability, also known as the empirical approach, is based on observing and recording outcomes from repeated experiments. The probability of an event is determined as the ratio of the number of times the event occurs \\(m\\) to the total number of trials \\(n\\) as when \\(n\\) approaches infinity. This method is useful when theoretical probabilities cannot be calculated but relies on the practicality of conducting a large number of trials and assumes consistency in experimental conditions.\nThe probability \\(p\\) of happening of \\(E\\) is given by\n\\[p\\left( E \\right) = \\lim_{n \\rightarrow \\infty}\\frac{m}{n} \\tag{10.3}\\]\nwhere \\(n\\) is the number of times the process is performed which tends to infinity, and \\(m\\) is the number of times the outcome ‘\\(E\\)’ happens.\nLimitations\nThe statistical approach also has limitations. In some cases, the experiment may not be practically repeatable, making it impossible to rely on repeated trials. Additionally, it raises the question of how large (\\(n\\)) must be to provide a good approximation of the probability. To address these issues, Russian mathematician A.N. Kolmogorov introduced the axiomatic approach, which does not rely on precise definitions but instead establishes probability based on a set of fundamental axioms or postulates.\n\n\n10.5.3 Axiomatic Approach\nThe axiomatic approach to probability, introduced by A.N. Kolmogorov, provides a formal framework based on a set of foundational axioms rather than specific definitions. This approach overcomes the limitations of earlier methods by establishing universally accepted rules that apply to all probability calculations, making it applicable to a wide range of theoretical and practical scenarios.\nWhole field of probability theory is based on the following three axioms\n\nProbability of an event, \\(p(E)\\) lies between \\(0\\) and \\(1\\). That is \\(0 \\leq p(E) \\leq 1\\)\nProbability of entire sample space is 1. That is \\(p\\left( S \\right) = 1\\)\nIf \\(A\\) and \\(B\\) are mutually exclusive events then probability of occurrence of either \\(A\\) or \\(B\\) is denoted by \\(p(A \\cup B)\\) shall be given by \\(p\\left( A \\cup B \\right) = p\\left( A \\right) + p(B)\\)\n\n\n\n\n\n\n\nNote\n\n\n\nThe probability \\(p\\) of the occurrence of an event is referred to as the probability of success, while the probability \\(q\\) of its non-occurrence is known as the probability of failure. Both \\(p\\) and \\(q\\) are non-negative and cannot exceed unity, i.e., \\(0 \\leq p \\leq 1\\) and \\(0 \\leq q \\leq 1\\). This means the probability of an event always lies between \\(0\\) and \\(1\\), inclusive. The probability of an impossible event is \\(0\\), and the probability of a certain event is \\(1\\). For instance, if \\(p(A) = 1\\), event \\(A\\) is guaranteed to occur, and if \\(p(A) = 0\\), event \\(A\\) cannot occur. Additionally, the number of favorable outcomes \\(m\\) for an event cannot exceed the total number of outcomes \\(n\\).\n\n\n\n\n\n\n\n\nFigure 10.4: Probability always lies between 0 and 1\n\n\n\nExample 10.6: In a simultaneous toss of two coins, find the probability of (i) getting 2 heads. (ii) exactly 1 head?\nSolution 10.6: Here, the possible outcomes are HH, HT, TH, TT. i.e., Total number of possible outcomes = 4.\n\nNumber of outcomes favorable to the event (2 heads i.e.,HH) = 1. \\(p\\text{(2 heads)} = \\frac{1}{4}\\).\nNow the event consisting of exactly one head has two favourable cases, namely HT and TH. \\(p\\text{(exactly one head)} = \\frac{2}{4} = \\frac{1}{2}\\)\n\nExample 10.7: In a single throw of two dice, what is the probability that the sum is 9?\nSolution 10.7: The number of possible outcomes is 6 × 6 = 36.\n(1,1) (1,2) (1,3) (1,4) (1,5) (1,6)\n(2,1) (2,2) (2,3) (2,4) (2,5) (2,6)\n(3,1) (3,2) (3,3) (3,4) (3,5) (3,6)\n(4,1) (4,2) (4,3) (4,4) (4,5) (4,6)\n(5,1) (5,2) (5,3) (5,4) (5,5) (5,6)\n(6,1) (6,2) (6,3) (6,4) (6,5) (6,6)\nLet the event \\(A\\) be sum the is 9. Four outcomes are there with sum 9, they are (5,4), (6,3), (3,6), (4,5). \\(p (A) = \\frac{4}{36} =\\frac{1}{9}\\)\nExample 10.8: From a bag containing 10 red, 4 blue and 6 black balls, a ball is drawn at random. What is the probability of drawing (i) a red ball? (ii) a blue ball? (iii) not a black ball?\nSolution 10.8: There are 20 balls in all. So, the total number of possible outcomes is 20\n\nNumber of red balls = 10, \\(p\\text{(getting a red ball)}=\\frac{10}{20} =\\frac{1}{2}\\)\n\nNumber of blue balls = 4, \\(p\\text{(getting a blue ball)}=\\frac{4}{20} =\\frac{1}{5}\\)\n\nNumber of balls which are not black = 14, \\(p\\text{(not a black ball)}=\\frac{14}{20} =\\frac{7}{10}\\)",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "probability.html#event-relations",
    "href": "probability.html#event-relations",
    "title": "10  Probability",
    "section": "10.6 Event relations",
    "text": "10.6 Event relations\nIn probability theory, relationships between events help in analyzing their combined or individual outcomes within a sample space. These relations include concepts such as union, intersection, complement and mutual exclusivity, which are fundamental for understanding how events interact and influence probabilities.\nComplement of an event\nIn probability, the complement of an event refers to all outcomes in the sample space that are not favorable to the given event. If \\(A\\) is an event, its complement is denoted by \\(A^c\\), and it represents the occurrence of all outcomes where \\(A\\) does not happen.\nFor example, consider the experiment of tossing a die. Let \\(A\\) be the event of getting an even number. The favorable outcomes for \\(A\\) are \\(\\{2, 4, 6\\}\\). The remaining outcomes, \\(\\{1, 3, 5\\}\\), do not satisfy \\(A\\) and are therefore the complement of \\(A\\). These outcomes represent the occurrence of \\(A^c\\), where \\(A\\) does not occur, therefore in this case \\(A^c=\\{1, 3, 5\\}\\) .\nThe complement of an event is essential in probability calculations and is related to the event by the formula:\n\\[P(A^c) = 1 - P(A) \\tag{10.4}\\]\nThe relationship in Equation 10.4 highlights that the probability of an event and its complement together always sum to 1. i.e. \\(P(A) + P(A^c) = 1\\)\nEvent \\(A\\) or \\(B\\)\nDenoted as ‘\\(A \\cup B\\)’, spelled as \\(A\\) union \\(B\\), represents the occurrence of either event \\(A\\), event \\(B\\) or both. Let us consider the example of throwing a die. Suppose \\(A\\) is an event of getting a multiple of 2 and \\(B\\) be another event of getting a multiple of 3. The outcomes 2, 4 and 6 are favourable to the event \\(A\\) and the outcomes 3 and 6 are favourable to the event \\(B\\) i.e. \\(A = \\{2, 4, 6\\}\\), \\(B= \\{3, 6\\}\\) then \\(A \\cup B = \\{ 2, 3, 4, 6\\}\\).\n\n\n\n\n\n\nFigure 10.5: Venn diagram showing union of two events\n\n\n\nEvent \\(A\\) and \\(B\\)\nDenoted as ’ \\(A \\cap B\\) ’ spelled as \\(A\\) intersection \\(B\\), represents the occurrence of both events \\(A\\) and \\(B\\) simultaneously. It includes only those outcomes that are common to both events. For example, on throwing a die in which \\(A\\) is the event of getting a multiple of 2 and \\(B\\) is the event of getting a multiple of 3. The outcomes favorable to \\(A\\) are 2, 4, 6 and the outcomes favorable to \\(B\\) are 3 and 6. Here 6 is present in both \\(A\\) and \\(B\\) so here \\(A \\cap B = 6\\). Figure 10.6 below shows the venn diagram of intersection of two events.\n\n\n\n\n\n\nFigure 10.6: Venn diagram showing intersection of two events",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "probability.html#additive-law-of-probability",
    "href": "probability.html#additive-law-of-probability",
    "title": "10  Probability",
    "section": "10.7 Additive law of probability",
    "text": "10.7 Additive law of probability\nAccording to additive law of probability, for any two events \\(A\\) and \\(B\\) of a sample space \\(S\\), the probability of the union of two events \\(A\\) and \\(B\\) is equal to the sum of their individual probabilities, minus the probability of their intersection.\n\\[p(A \\cup B) = p(A)+p(B) − p(A \\cap B) \\tag{10.5}\\]\nFor mutually exclusive case \\(p(AꓵB)=0\\); in that case:\n\\[p(A \\cup B) = p(A )+ p(B ) \\tag{10.6}\\]\nExample 10.9: A card is drawn from a well-shuffled deck of 52 cards. What is the probability that it is either a spade or a king?\nSolution 10.9: If \\(A\\) denotes the event of drawing a ‘spade card’. \\(B\\) denotes the events of drawing a ‘king’ respectively. The event \\(A\\) consists of 13 sample points, whereas the event \\(B\\) consists of 4 sample points. \\(p(A)= \\frac{13}{52}\\), \\(p(B)= \\frac{4}{52}\\), \\(p(A \\cap B) = \\frac{1}{52}\\), \\(p(A \\cup B) = p(A)+ p(B) − p(AꓵB)\\) = \\(\\frac{13}{52}+\\frac{4}{52}-\\frac{1}{52}= \\frac{4}{13}\\)\nExample 10.10: In a single throw of two dice, find the probability of a total of 9 or 11.\nSolution 10.10: Let the events \\(A\\) = a total of 9 and \\(B\\) = a total of 11. Events are mutually exclusive \\(A \\cap B = 0\\). Now there are four such cases were sum = 9, such as (3, 6), (4, 5), (5, 4), (6, 3); therefore \\(p(A) = \\frac{4}{36}\\). Similarly there are two cases were sum = 11, such as (5, 6), (6, 5); therefore \\(p(B) = \\frac{2}{36}\\). Thus from Equation 10.6, \\(p(A \\cup B) = \\frac{4}{36}+\\frac{2}{36}= \\frac{6}{36} = \\frac{1}{6}\\)",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "probability.html#conditional-probability",
    "href": "probability.html#conditional-probability",
    "title": "10  Probability",
    "section": "10.8 Conditional probability",
    "text": "10.8 Conditional probability\nConditional probability measures the likelihood of an event occurring, given that another event has already occurred. If \\(A\\) and \\(B\\) are two events, the conditional probability of \\(A\\) given \\(B\\) is denoted by \\(P(A \\mid B)\\) and is defined as:\n\\[p(A \\mid B) = \\frac{p(A \\cap B)}{p(B)}, \\quad \\text{provided } p(B) &gt; 0 \\tag{10.7}\\]\nEquation 10.7 gives the probability of \\(A\\) happening under the condition that \\(B\\) has occurred.\nExample 10.11: If a card is drawn from a standard deck of 52 cards, what is the probability that it is a king, given that the card drawn is a face card?\nSolution 10.11: Let \\(A\\) be the event of drawing a king, \\(B\\) be the event of drawing a face card (king, queen, or jack). The probability of \\(B\\), drawing a face card, is \\(p(B) = \\frac{\\text{Number of face cards}}{\\text{Total cards}} = \\frac{12}{52}\\). The probability of \\(A \\cap B\\), drawing a King that is also a face card, is:\\(p(A \\cap B) = \\frac{\\text{Number of Kings}}{\\text{Total cards}} = \\frac{4}{52}\\). Using Equation 10.7 the conditional probability of drawing a king given that the card is a face card is \\(p(A \\mid B) = \\frac{P(A \\cap B)}{P(B)} = \\frac{\\frac{4}{52}}{\\frac{12}{52}} = \\frac{4}{12} = \\frac{1}{3}\\).",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "probability.html#multiplication-law-of-probability",
    "href": "probability.html#multiplication-law-of-probability",
    "title": "10  Probability",
    "section": "10.9 Multiplication law of probability",
    "text": "10.9 Multiplication law of probability\nThe multiplication law of probability states that if \\(A\\) and \\(B\\) are two events, the probability of both events occurring (i.e., \\(A \\cap B\\)) is given by:\n\\[p(A \\cap B) = p(A) \\cdot p(B \\mid A) \\tag{10.8}\\]\nThis formula expresses the relationship between the joint probability of two events and their conditional probability. If \\(A\\) and \\(B\\) are independent events, then \\(p(B \\mid A) = p(B)\\), and the formula simplifies to:\n\\[p(A \\cap B) = p(A) \\cdot p(B) \\tag{10.9}\\]\nExample 10.12: What is the probability of drawing two aces in succession from a standard deck of 52 cards?\nSolution 10.12: Let \\(A\\) be the event that the first card drawn is an ace. \\(B\\) be the event that the second card drawn is an ace. The probability of \\(A\\) (drawing an ace on the first draw) is \\(p(A) = \\frac{4}{52}\\). If the first card is an ace, there are only 3 aces left in a deck of 51 cards. The probability of \\(B\\) (drawing an ace on the second draw given that the first card was an ace) is \\(p(B \\mid A) = \\frac{3}{51}\\). Now using Equation 10.8 the probability of drawing two aces in succession is given by \\(p(A \\cap B) = p(A) \\cdot p(B \\mid A) = \\frac{4}{52} \\cdot \\frac{3}{51} = \\frac{12}{2652} = \\frac{1}{221}\\)\nExample 10.13: A die is tossed twice. Find the probability of a number greater than 4 on each throw?\nSolution 10.13: Let \\(A\\) be the event that ‘a number greater than 4’ on first throw. \\(B\\) be the event that ‘a number greater than 4’ in the second throw. Clearly \\(A\\) and \\(B\\) are independent events. In the first throw, there are two outcomes, namely, 5 and 6 favourable to the event \\(A\\). \\(p(A) = \\frac{2}{6} = \\frac{1}{3}\\). Similarly in the second throw, there are two outcomes, namely, 5 and 6 favourable to the event \\(B\\), therefore \\(p(B) = \\frac{2}{6} = \\frac{1}{3}\\). Now by Equation 10.6, probability to get a number greater than 4 on each throw is given by \\(p(A \\cap B) = p(A).p(B) = \\frac{1}{3}.\\frac{1}{3} = \\frac{1}{9}\\)",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "probability.html#probability-using-combinations",
    "href": "probability.html#probability-using-combinations",
    "title": "10  Probability",
    "section": "10.10 Probability using combinations",
    "text": "10.10 Probability using combinations\nCombinations can be used to calculate the total number of possible outcomes in a probability problem. The formula for combinations is given by:\n\\[^n C_r = \\frac{n!}{r!(n - r)!} \\tag{10.10}\\]\nExample 10.14: Calculate \\(^3 C_2\\)\nSolution 10.14: \\(^3 C_2 = \\frac{3!}{2!(3 - 2)!} = \\frac{3 \\times 2 \\times 1}{2 \\times 1} = 3\\)\nExample 10.15: A bag contains 3 red, 6 white, and 7 blue balls. What is the probability that two balls drawn are white and blue?\nSolution 10.15: Total number of balls = 3 + 6 + 7 = 16. The number of ways to draw 2 balls from 16 is given by \\(^{16} C_2 =120\\). The number of ways to draw one white ball from 6 white balls is \\(^6 C_1=6\\), and the number of ways to draw one blue ball from 7 blue balls is \\(^7 C_1 =7\\). Since the events are independent, the total number of favorable outcomes is \\(^6 C_1 \\times ^7 C_1 = 6 \\times 7 = 42\\). Thus, the required probability is:\\(p(\\text{one white and one blue}) = \\frac{^6 C_1 \\times ^7 C_1}{^{16} C_2} = \\frac{42}{120} = \\frac{7}{20}\\)\nExample 10.16: A bag contains 5 red and 4 black balls. What is the probability that both balls drawn are red?\nSolution 10.16:Total number of balls = 5 + 4 = 9. The number of ways to draw 2 balls from 9 is given by \\(^9 C_2 = \\frac{9 \\times 8}{2 \\times 1} = 36\\). The number of ways to draw 2 red balls from 5 red balls is \\(^5 C_2 = \\frac{5 \\times 4}{2 \\times 1} = 10\\). Thus, the required probability is: \\(p(\\text{both red balls}) = \\frac{^5 C_2}{^9 C_2} = \\frac{10}{36} = \\frac{5}{18}\\)",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "probability.html#bayes-theorem",
    "href": "probability.html#bayes-theorem",
    "title": "10  Probability",
    "section": "10.11 Bayes’ theorem",
    "text": "10.11 Bayes’ theorem\nBayes’ theorem gives a mathematical rule for inverting conditional probabilities, allowing one to find the probability of a cause given its effect.\nLet \\(E_1, E_2, \\dots, E_n\\) be a set of events associated with a sample space \\(S\\), where all the events \\(E_1, E_2,\\dots, E_n\\) have non-zero probability of occurrence and they form a partition of \\(S\\). Let \\(A\\) be any event associated with \\(S\\), then according to Bayes theorem,\n\\[p(E_i \\mid A) =\\frac{p(E_i) \\cdot P(A \\mid E_i)}{\\sum_{k = 1}^{n}{p(E_k)}\\cdot p(A \\mid E_k)} \\tag{10.11}\\]\nThe following terminologies are commonly used when applying Bayes’ theorem:\nHypotheses: The events \\(E_1, E_2, \\dots, E_n\\) are called the hypotheses.\nPrior probability: The probability \\(p(E_i)\\) is considered the prior probability of hypothesis \\(E_i\\).\nPosterior probability: The probability \\(p(E_i \\mid A)\\) is considered the posterior probability of hypothesis \\(E_i\\).\nFor any two events \\(A\\) and \\(B\\), the formula for the Bayes theorem is given by:\n\\[p(A \\mid B) =\\frac{p(B\\mid A)\\cdot p(A)}{p(B)}, p(B)\\neq 0 \\tag{10.12}\\]\nWhere \\(p(A)\\) and \\(p(B)\\) are the probabilities of events \\(A\\) and \\(B\\). \\(p(A \\mid B)\\) is the probability of event \\(A\\) given \\(B\\). \\(p(B \\mid A)\\) is the probability of event \\(B\\) given \\(A\\).\nExample 10.17: A bag I contains 4 white and 6 black balls while another bag II contains 4 white and 3 black balls. One ball is drawn at random from one of the bags, and it is found to be black. Find the probability that it was drawn from bag I.\nSolution 10.17: Let \\(E_1\\) be the event of choosing bag I, \\(E_2\\) the event of choosing bag II, and \\(A\\) be the event of drawing a black ball. \\(p(E_1)=p(E_2)=\\frac{1}{2}\\). Also,probability of drawing a black ball from bag I is given by \\(p(A \\mid E_1) = \\frac{6}{10}=\\frac{3}{5}\\). Probability of drawing a black ball from bag II \\(p(A \\mid E_2=)\\frac{3}{7}\\).\nBy using Bayes’ theorem, the probability of drawing a black ball from bag I out of two bags,\n\\[p(E_1 \\mid A) =\\frac{p(E_1) \\cdot P(A \\mid E_1)}{p(E_1) \\cdot P(A \\mid E_1)+p(E_2) \\cdot P(A \\mid E_2)}\\]\n\\[= \\frac{\\frac{1}{2}\\times \\frac{3}{5}}{\\frac{1}{2}\\times \\frac{3}{5}+\\frac{1}{2}\\times \\frac{3}{7}} =\\frac{7}{12}\\]\nExample 10.18: A man is known to speak the truth 2 out of 3 times. He throws a die and reports that the number obtained is a four. Find the probability that the number obtained is actually a four.\nSolution 10.18: Let \\(A\\) be the event that the man reports that number four is obtained. Let \\(E_1\\) be the event that four is obtained and \\(E_2\\) be its complementary event. Then, \\(p(E_1)\\) = probability that four occurs = \\(\\frac{1}{6}\\). \\(p(E_2)\\) = probability that four does not occur = \\(1-p(E_1) = 1-\\frac{1}{6} = \\frac{5}{6}\\). Also, \\(p(A|E_1)=\\) probability that man reports four and it is actually a four \\(= \\frac{2}{3}\\) (because man speaks truth 2 out of 3 times). \\(p(A|E_2)=\\)Probability that man reports four and it is not a four \\(= \\frac{1}{3}\\).\nBy using Bayes’ theorem, probability that number obtained is actually a four, \\(p(E_1 \\mid A) =\\frac{p(E_1) \\cdot P(A \\mid E_1)}{p(E_1) \\cdot P(A \\mid E_1)+p(E_2) \\cdot P(A \\mid E_2)}\\).\n\\[p(E_1 \\mid A)= \\frac{\\frac{1}{6}\\times \\frac{2}{3}}{\\frac{1}{6}\\times \\frac{2}{3}+\\frac{5}{6}\\times \\frac{1}{3}} =\\frac{2}{7}\\]\n\n\n\n\n\n\nTry yourself\nSix cards are drawn at random from a pack of 52 cards. What is the probability that 3 will be red and 3 black?\n\n\n\n\n\n\n\n\n\nHistorical Insights\n\n\n\nThe gambler’s fallacy\nIn 1913, a group of gamblers gathered at a casino in Monte Carlo, where they witnessed a highly unusual event at the roulette table. The ball landed on black 26 times in a row—an incredibly rare streak. As the streak continued, more and more gamblers bet heavily on red, convinced that red was “due” to appear. But to their shock, black kept winning, and many lost fortunes that night. This event is now a famous example of the gambler’s fallacy also known as the Monte Carlo fallacy or the fallacy of the maturity of chances, the mistaken belief that past events influence future outcomes in independent events, like the spin of a roulette wheel. In reality, the probability of the ball landing on black or red remains the same on every spin, regardless of previous results.\n\n\n\n\n\n\n\n\nQuotes to Inspire\n\n\n\n“The theory of probabilities is at bottom nothing but common sense reduced to calculus.” – Pierre-Simon Laplace",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "3graph.html#pie-chart",
    "href": "3graph.html#pie-chart",
    "title": "3  Graphical representation",
    "section": "3.8 Pie chart",
    "text": "3.8 Pie chart\nA pie chart is a circular graph divided into sectors, each sector representing a different value or category. The angle of each sector of a pie chart is proportional to the value of the part of the data it represents. The bar chart is more precise than the pie chart for visual comparison of categories with similar relative frequencies.\nSteps for constructing a pie chart\n\nFind the sum of the category values.\n\nCalculate the angle of the sector for each category, using the following formula. Angle of the sector for category A = \\(\\frac{\\text{value of category A}}{\\text{sum of category values}} \\times 360\\)\n\nConstruct a circle and mark the centre.\n\nUse a protractor to divide the circle into sectors, using the angles obtained in step 2.\n\nLabel each sector clearly.\n\nTable 3.7 presents hypothetical data on the production of different commodities in India during a particular year. Pie chart based on this data is shown in Figure 3.13\n\n\n\nTable 3.7: Hypothetical data on the production of different commodities\n\n\n\n\n\nCommodities\nProduction(tonnes)\nAngle\n\n\n\n\nWheat\n27000\n(27000/81000)×360= 120\n\n\nGrams\n22500\n100\n\n\nMaize\n13500\n60\n\n\nRice\n6750\n30\n\n\nSugar\n11250\n50\n\n\nTotal\n81000\n360\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.13: Pie chart",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Graphical representation</span>"
    ]
  },
  {
    "objectID": "probability.html#random-variable",
    "href": "probability.html#random-variable",
    "title": "10  Probability",
    "section": "10.2 Random variable",
    "text": "10.2 Random variable\nA random variable is a numerical value that represents the outcome of a random experiment. It is a function that assigns a real number to each possible outcome in the sample space of the experiment.\nRandom variables can be classified into two types:\n\nDiscrete random variables: These take on a countable set of values, such as integers (e.g., the number of heads in three coin tosses).\n\nContinuous random variables: These can take on any value within a given range, which is typically uncountable (e.g., the height of individuals in a population).\n\nIn mathematical terms, if \\(S\\) is the sample space of a random experiment, a random variable \\(X\\) is a function that maps sample space to real number set, which can be represented as \\(X: S \\to \\mathbb{R}\\), where \\(\\mathbb{R}\\) represents the set of real numbers.\nThe probability of a random variable can be represented by \\(p(X = x)\\) or \\(p(x)\\), where the small letter \\(x\\) denotes the value taken by the random variable \\(X\\).\nConsider the example of throwing a die once.Here, you can define a random variable of your interest. Defining a random variable involves assigning a numerical value to the outcomes of the experiment. Let \\(X\\) represent the number appearing on the die. The possible values of \\(X\\) are: \\(x = \\{1, 2, 3, 4, 5, 6\\}\\).\nConsider another example of even number appearing on the die. Let \\(X\\) represent the even number appearing on the die. The possible values of \\(X\\) are: \\(x = \\{2, 4, 6\\}\\).\nIn each of the above case, you can see that the random variable \\(X\\) assigns a value to the outcomes of the experiment.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Probability</span>"
    ]
  }
]